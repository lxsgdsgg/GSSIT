the images below show segmentation maps, with yellow regions corresponding to predicted text, and purple everything else.
when running `tf.estimator.estimator` model that registers to `evaluation_hooks` of in distributed environment, an error `attributeerror: 'perreplica ' object has no attribute 'begin '` occurs at the beggining of evaluation.
4. click on _execute_ 4. see the state reset and no request executed.
also tried forcing it by including a with tf.device("/gpu:0") call but that doesn 't seem to help at all.
however, it completes normally if `episode_length` is halved, the `simplernn` layer is commented out or the `batch_normalization` layer is commented out.
but i don 't think this is a good solution.
i can provide this code if requested, but don 't suspect it to be problem as use it for other working projects.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
label_image, benchmark_model) or use the interpreter via an import in python will generate error messages at time of loading a flatbuffers tflite model file.
i should get no exception.
switched to protocol grpc, the code works well.
@summary.py:46] [movingaveragesummary] 75 operations collection 'moving_summary_ops ' will run session hooks.
in the linked [colab] i try to train a walsmodel, with a sparse matrix with shape 44000)` when i try to run it locally on my quadro i get: phofcode which, ok, i have a 16gb memory card, that makes sense, guess.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
looks like the strongswan user does not exist in freebsd.
2.the code i modifyied to using gpu delegate is right?
but need only be done once during model build.
phofcode here results are similar, there is a small difference.
figure out cleaner solution def is_feature_layer(layer): """returns whether `layer` is featurelayer or not."""
i am trying to run an .pb file on c++.
see that there is no prompt for additional properties (only for `foo`)
in gvim/cmd terminal (windows 10), if i terminate a job with job_stop(), this calls the exit_cb callback, but with a status of 0 (in linux for the same event it is -1), so a terminated job runs the callback as a succesfully ended one.
1364 iterator 1365 1366 return iterator shared_name) 1853 return pylint: disable=protected-access 1854 except attributeerror: 1855 pylint: disable=protected-access 1856 1857 shared_name) 1495 if 1496 raise runtimeerror( 1497 is not supported when eager " 1498 "execution is enabled.")
cannot install, every time same error.
it works fine when run this in one console phofcode and then the other phofcode if you change 200 to 20000, you will see errors on load phofcode phofcode
there should be a way to copy-paste text from vim to an x11 application.
i am note sure if this is intended or not, but as this works with a custom training function i tend to think it may be bug in `fit` method.
required property should be marked by red asterix
valueerror traceback (most recent call last) in <module>() 5 import numpy as np 6 import tensorflow as tf ----> 7 8 from keras import layers 9 keras import models in device_policy, execution_mode) 5421 5422 -> 5423 server_def=none) 5424 5425 in device_policy, execution_mode, server_def) 5465 if 5466 raise valueerror( -> 5467 must be called at program startup.")
if i turn on gpu in the notebook, and rerun the same code, i get an exception.
i start another deep learning image (tf-latest-cpu , m10): running exact same code this machine with environment variable (export mkl_verbose=1): i can observe a lot of openmp thread settings , kmp_xxx settings and mkl instructions logged with some timing information.
a future version of pip will drop support for python 2.7.
1/281 - eta: 9:05 - loss: e aborting ringgather with internal: [_derived_]inconsistent output shapes, got [16], but expected is [64].
another remarkable thing is that before inference get following warning 5 times: `w allocation of exceeds 10% of system memory.`
used steps as on tensorflow lite convertion tutorial phofhyperlink , i generated both unit8 float type tflite models.
a clear and concise description of what you expected to happen.
- there seems to be no obvious `status_invalid` type option, and i'm not familiar enough with the context these values are used in or what minimal impact fix would be appropriate, so i'm filing this as an issue rather than a pr.
i am using tensorflow_probability (0.5.0) to sample points (random walk metropolis algorithm) that i want to use as inputs for my neural network.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): source tensorflow version (use command below): 2.0.0-beta1 python version: 3.6.0
second line should be indented, but it isnt.
t# simplified example timport tensorflow as tf tmnist = tf.keras.datasets.mnist t(x_train, y_train),(x_test, y_test) = mnist.load_data() tx_train, x_test = x_train / 255.0, x_test / 255.0 tw, h = 28, 28 t# reshape input data from (28, 28) to (28, 28, 1) tx_train w, h, 1) tx_test w, h, 1) t# buggy example -> what is going on with conv2d?
should not throw the above error.
exception is reported to stdout, with stack trace, when testing under `self.assertraises()`: i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 fma failed: 1] [[{{node = assert[t=[dt_float], summarize=3, phofhyperlink ]] caused by op defined at: file line 184, in _run_module_as_main "__main__", mod_spec) file line 85, in _run_code exec(code, run_globals) ... file line 600, in run testmethod() file line 19, in testraises sess.run(foo()) 7, foo max_assert = 1.01), [max_val]) 189, wrapped return
phofcode if i check the output i see: phofcode
phofcode - os: mageia 7 - terminal: konsole if you edit the line or even apply gq one more time, the formatting will be correct
the deeplablite class has two important functions:
the `quickfixline` highlighting starts from the beginning of the line, which here displays `/tmp/file|1 col 1|`.
i''ve changed the official cnn estimator ( phofurl phofhyperlink ) to work with _one_hot_ encoding on labels (only changing the _loss_ function) phofcode i've aslo create dummy labels to test it: train_labels = eval_labels =
a clear and concise description of what you expected to happen.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): y - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.13.6 - mobile device (e.g.
calling model.fit_generator on a keras model in tf 2.0 or compat.v2
cnn flatten seem to also have problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
the same problem appears when and `tf.import_graph_def()` are used.
my point that training performance on gpu and cpu significantly degraded when relatively memory costly `something` object passed instead of `none`.
large logs and files should be attached.
anyway overcome this problem without rewriting data feed?
full code to reproduce is in the .ipynb file.
it does not seem normal that quantization gives too much differences in output values.
in: phofcode returns [] to stdout
the failure happens somewhere between version 8.1 patch 978 (where everything works), and 8.1 patch 1298.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): phofcode
if you do that same sequence of steps but without the `make clean`, you will see all of the other tons of test files that `make clean` does wipe out.
i 've been running into ` 'redrawtime ' exceeded, syntax highlighting disabled` a lot working with large files with 8.1.
if including tracebacks, please include the full traceback.
however, for the same piece of code, my gcp instance is more than 3 times slower than `gcolab`.
but in the case where some outputs are not part of the loss function (i.e.
though even if mfa is activated according to instructions, the ansible cloud-ec2 tasks do not follow the proper steps to execute the ec2 modules using mfa.
here is the traceback in tf 2.0-preview: traceback (most recent call last): file "<stdin>", line 1, in <module> file line 148, plot_model dot = model_to_dot(model, show_shapes, show_layer_names, rankdir) file line 123, model_to_dot for inbound_layer node.inbound_layers: typeerror: 'inputlayer ' object is not iterable ``
is documented here phofhyperlink but absent (it 's in `tf.keras.utils`).
when `feature_columns` argument is specified, `tensorforestestimator` will throw the following error during a `fit()`: phofcode
run_metadata should be saved even (especially?)
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 64bit, aws ec2 p3.2xlarge instance - tensorflow installed from (source or binary): `pip install tensorflow-gpu` - tensorflow version (use command below): 1.13.1 python version: 3.6.7 bazel version (if compiling from source): none gcc/compiler version (if compiling from source): none cuda/cudnn version: 10.1 gpu model and memory: nvidia tesla v100 16gb include any logs or source code that would be helpful to diagnose the problem.
factorize the matrix and evaluate without the error.
--config=ngraph build intel ngraph support.
importing tensorflow prints an error: > e hadoopfilesystem load error: libhdfs.so: cannot open shared object file: no such file or directory
tf.layers.flatten uses shape, stridedslice, pack ops to build the new shape for reshape.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.6.8 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: the output is as follows (i 've added comments): phofcode as you can see the order of the data is perfectly identical during the 1st and 2nd epochs.
import numpy as np import tensorflow as tf print(tf.__version__) class def __init__(self, vocab_size, hidden_size): super(embedding, self).__init__() self.vocab_size = vocab_size self.hidden_size = hidden_size def build(self, input_shape): self.shared_weights = self.add_weight( "weights", shape=(self.vocab_size, self.hidden_size), dtype=tf.float32, mean=0.0, stddev=self.hidden_size
checked out the release 1.12 branch of the tf repo, configured bazel (configuration is below) and ran `bazel build -c opt --config=opt compilation fails at error c2338`, it seems a static assert is failing: phofcode
phofcode edit: typo in `keras.models` edit: typo in `keras.layers
provide a reproducible test case that is the bare minimum necessary to generate the problem.
alternatively, i expected that at least `k.learning_phase()` would return `true` (or `1`), but it 's always `0`.
i 'm using a cnn built with `tf.keras.layers` layers (called `model`) and my compile step, callbacks definitions, and fit step look like this: model.compile( optimizer= 'adam ', ) callbacks = [ monitor= 'val_acc ', verbose=1, save_best_only=true, mode= 'max '), tensorboard(), patience=5, min_delta=0, mode= 'max ') ] model.fit(dataset, steps_per_epoch=1000, epochs=1000, validation_steps=3, callbacks=callbacks)
auto deeplab keeps going as expected
1. run `vim --clean -t main`.
and the feed tensors are not processing in parallel.
`:resize 0` fails to squash a non-focused window to zero lines, even when ` 'winminheight '` is set to `0`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes.
the sorting order should change without corrupting the file listing.
the matmul function should return correct results, regardless of sparsity or data type.
lstm/gru supporting zero masking, and having no impact of training speed
should not have the gap
probably because the restore of the input checkpoint is done during the init of hook.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): 1. code in golang 2. used go tensorflow 3. mobilenet model - os platform and distribution (e.g., linux ubuntu 16.04): raspberry pi (1.4ghz, 1024 ram, 4 cores) - tensorflow version (use command below): v1.12.0 phofcode
the code below phofcode produces a non-zero result like phofcode
one must drop the const specifier and moreover since tflite file is memory-mapped mapping options include prot_write mode map_private in mmap_allocation.cc.
it throws the following error: phofcode which is not very helpful for figuring out what the problem is
... 1. wrap the `markdown` component: phofcode 2. render an openapi spec, e.g.
python code is single-threaded, and each process writes a separate .npz file.
if the re-compile is commented out, fit() time remains constant.
large logs and files should be attached.
when calling `screenpos()` for a position close to the right edge of the screen, the return can have `col` of `0` even though the character is visible.
large logs and files should be attached.
i got correct output of the model, and the inference time is 30ms(1 thread), 17ms(2 threads), 15ms(3 threads).
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i got values but are not the same with the same image tested with ckpt files.
when double clicking in gvim, word-wise selection begins.
i have also tested with different providers such as pia, nordvpn, etc.
1. run `vim --clean popup.vim -c "source popup.vim"` phofcode 2. mover cursor to the right side of the window and press `<space>ep`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): custom code using - os platform and distribution (e.g., linux ubuntu 16.04): privileged ubuntu 16.04 docker - mobile device (e.g.
the same model starts training in a couple of minutes without distribute strategy when using much simpler model, like resnet50 mirrored strategy also lags at startup compared to single gpu, but nevertheless training starts in a couple of minutes.
gfile should support the python io semantics that supports seeking on a write only file.
this commit phofurl was found to be causing the regression.
the file may need to truncate at some specific place.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): arch linux - mobile device (e.g.
phofcode the images were still logged every global_step % n == 0 iterations, yet as can be seen here, the code within the block still executed every global_step.
colab notebook available here phofhyperlink .
the first implementation was using the eager version to do the train, but the model collapses, nothing works.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): arch linux - mobile device (e.g.
has anyone actually managed to use `tensorflow_cc.dll` (on windows) at all?
w0519 tf_logging.py:161] entity <function tf_for_tf_break at could not be transformed and will be staged without change.
i0206 camera_input.go:69] captured image, len queue 0 i0206 runner.go:85] detection time terminate called after throwing an instance 'std::bad_alloc' what(): std::bad_alloc aborted
`hparams` suddenly `dict`, not instance `hparams`.
after restoring the input checkpoint, the train restart at the right dataset iterator step.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.13.1` python version: 3.7.2 bazel version (if compiling from source): 0.23.1 gcc/compiler version (if compiling from source): `apple llvm version 10.0.0 cuda/cudnn version: n/a gpu model and memory: n/a here's what the output looks like: peak memory = warning: the tensorflow contrib module will not be included in tensorflow 2.0. for more information, please see: * phofurl * phofurl if you depend on functionality not listed there, please file an issue.
no error, it was working in 1.13.
large logs and files should be attached.
i run the following snippet of code in eager execution: phofcode however, the call to gives me the following error: phofcode also, i am not able to find any documentation on `deferredtensor`s whatsoever.
if including tracebacks, please include the full traceback.
info:tensorflow:start train and evaluate loop.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): pip binary tensorflow version (use command below): tf 2 nightly as of 3/7/19 (comparison version is 1.13.1 python version: 3.4.2 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory:
> w0613 5776 ag_logging.py:145] entity <function train at could not be transformed and will be executed as-is.
when trying to use a custom * layer * into a new custom * model * , the model 's call method doesn 't work when i am initializing the custom layer in the model 's constructor but when instantiating this layer directly in the call method it works
i have created model from tf.keras.models for binary classification problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
1. make new project with strapi beta 14 2. create user in admin ui 3. try to update user in admin ui 4. see error
it must return the value of the collection keyed by tf.graphkeys.init_op, in this case it should be something like loss/init type noop
if including tracebacks, please include the full traceback.
passing flattened tensor into rnn results in an error: "shape (608, ?)
we just wanted to make certain this was on your radar as a use case, since we didn't see it in the rfc
the following code produces an array of zeros after running several times (for me it seems to reliably be on the fourth run): phofcode
phofcode notice that the status line shows "/tmp/
all the instances of `class` are highlighted as keywords, with the 2nd one highlighted as 'selected ' using `pmenusel`
i wanted to define a custom metric that is the combination of 2 metrics: python import tensorflow as tf class f1(tf.metrics.metric): def __init__(self,
/tmp/a.txt -s /tmp/s.vim +'b a.txt' this time, commenting out `:badd +0 /tmp/a.txt` from session not enough.
`lstm` and `gru` is dependent on the batch size when `stateful=false`.
using more than about 10 minutes, keep running demo app, but not working inference.
1. go to content type builder 2. click on an existing content type with at least one number type field 3. click the edit icon on a number field 4. go to advanced 5. check minimum value 6. try to enter a minimum value of 0 (steps 2 and 3 can be replaced with adding a new number field to an existing content type, or by creating a new content type and creating new number field).
if you create a distribution strategy like then create a tf.int32 variable var, then call tf will crash.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
0. load this file long.txt phofhyperlink 0.
ideally, would not like to see the error.
then call the ec2 module `sts_session_token` to create temporary credentials.
expected a "typeerror" or an empty list as a result.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
in order to do an experiment, i thought of starting with fine-tuning a vgg16 model and seeing the api 's action in that case.
large logs and files should be attached.
observe that it probably doesnt feel instant.
tflite was invoked through python interfaces, the model comes from tf model zoo phofhyperlink (the tflite model inside phofhyperlink ).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): colab - mobile device (e.g.
tags u-boot-1.1.6 tags if i run gvim with double click or its shortcuts in desktopgvim 8.1 it work normally why it takes so long to jump to definition with the popup menu project u-boot-1.1.6 sotre in a hard disk drive(hdd) vim was install in a solid state disk(sdd),which is faster than hdd.
(i 'm not sure if we call this minor issue "bug", though.)
it doesn 't happen when i use which is to be removed in future version.
i expect it to be quicker with optimized graph.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: dont know - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: 3.7.3 bazel version (if compiling from source): not used gcc/compiler version (if compiling from source): not used cuda/cudnn version: not related gpu model and memory: not related here is my output from running the code: phofcode notice the time used for creating a variable grows exponentially.
import numpy as np import librosa import tensorflow as tf wave, sr = mono=true, sr=none) sr = 44032 / 4 def generator(batch_size): while true: # start_positions = np.random.randint(0, len(wave) - sr, size=batch_size) # batch = [] batch [] for start_position in range(0, len(wave) - sr, 100): + sr], axis=-1), axis=-1)) # batch1 # batch2 np.expand_dims(batch1, axis=-1) if start_position % batch_size == 0: yield np.array(batch), np.array(batch) def get_model(input_n): input1 1, 1)) x_a input1 for i in x_a tf.keras.layers.conv2d(8 * (2
value is fetched without warnings.
here i created a toy example: phofcode which will create 2 batches of size 1, first batch has length 3, second batch has length 4. when embedding layer trainable set to false, model.fit runs successfully.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - tensorflow installed from (source or binary): source - tensorflow version (use command below): master from april 22nd python version: 3.6.7 bazel version (if compiling from source): 0.24 gcc/compiler version (if compiling from source): 7.4 cuda/cudnn version: 10.0 / 7.5.0 gpu model and memory: gtx 1080 ti
1. see more detailed log on what offending layers and tensors are.
then i used this option to create a session and load a model.
it 's same for verbose=2.
however when on-demand is active and i'm connected to my home wifi ssid, it immediately connects wireguard.
large logs and files should be attached.
cannot convert provided value to eagertensor when applying keras constraint on variable in tf2.0 eager mode.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): i have written custom code, albeit based off of code from tflearn.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): os x 10.14.3 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): python version: 3.7
0, vim vimrc 1, type '#whatever#' 2, the 'cursor' on the letter 'w' 3, issue 'de' 4, it would cut 'whatever#' - including the end '#'
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): gpu 1.13.0rc2 python version: 3.7.2 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: gpu model and memory: rtx2080ti gddr6 11gb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
it is a risk if just scp (save) to remote failed but no warn, somehow network was bad, the content was not really saved to remote.
i want to deploy it into a ubuntu 18.04 docker container.
all appended nodes are still supported ops by gpu delegate.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): `linux #1 smp preempt wed feb 27 utc 2019 x86_64 gnu/linux` - mobile device (e.g.
`tf.data.dataset::cache` doesn 't cleanup unused `lock` and `tmp` files if the iterator doesn 't complete a pass over the whole dataset or if the process exits before cache has been completed.
passing the a dataset object to `estimator.fit` method results in the following error: phofcode
tried visualizing with netron but
all nodes gpu utils is 100% phofcode dmesg output segment fault info phofcode
this rule is used (for example) in the linux repository: phofurl
program ends with an unclear error, while trying to retrieve the bias gradients of the two dense layers in the model.
i try to restore previously saved checkpoint variables and i get a method deprecated error.
--- in case of option, i know there are other solutions, like: set stl=%!getstl(winnr()) fu getstl(nr) return %{w:is_active ?
load failed... segmentation fault (core dumped)
source code of the program is listed in the `code` section below.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): - mobile device (e.g.
i mean it is deprecated warning should be shown to user when user write their own code using low level tensorflow function.
i attached a screenshot of the code and the system memory status along with gpu status.
we do this because we want to use dense1 and dense2 to perform different tasks but we want their weights to be shared (in a realistic example, mylayer would be split into mylayer1 and mylayer2, which would perform different operations on `call` but would both depend on `dense`).
you 'd need the whole neural network code plus the datasets and the rest...
assume that there is tf.assert inside of decorated function, which is independent of input argument (then defun get same graph function from graph cache, right?)
* create clone / copy constructor for `hparams` * use deep cloning?
large logs and files should be attached.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
this might be an issue with the keras api rather than with tf.keras.
the program should load the model.
print(tf.gradients(c, [a, b], # => [<tf.tensor 'zeros_like:0 ' shape=(2,) dtype=float32>, # <tf.tensor 'zeros_like_1:0 ' shape=() dtype=float32>] ``
was there a recent change in this area
receiving a segmentation fault when i run this code
the program should finish with no error.
... steps to reproduce the behavior: 1. open the above yml file in swagger ui 2. open the '/parameter ' request no example 3. click 'try it out ' no example
detailed steps to reproduce the behavior: 1. run `gvim (or open gvim to edit any plain file) 2. type `:!dir ` will list lost of file in 3. type ':terminal' will open a cmd shell with pwd is 4. i've install gtags plugin, `:!global -pq' cannot find the gtags of my project, also output
when using the function `tf.autograph.to_graph`, i see a memory leak which i don't see if i use the annotation `@tf.function`
the mirror strategy should work well on machine that have more than one gpu otherwise it is useless right?
it should behave like python3: reading the x characters.
a future version of will drop support for 2.7. more details about 2 support pip, can be found at phofurl requirement already up-to-date: (19.2.2) collecting using cached phofurl installing collected packages: successfully installed virtualenv-16.7.2 deprecation: reach the end its life on january 1st, 2020. please upgrade your as won 't maintained after that date.
the algo installer gets stuck on the common playbook gathering facts.
the output of `synid()` is always the id of the syntax group `helpbar`; not sometimes the id of `helpbar`, and sometimes the id of `helpexample`.
`tf.saved_model.save()` should work naturally for autographed functions even if they output a `raggedtensor`.
w0429 23940 deprecation.py:323] from > to_int32 (from is deprecated and will be removed in a future version.
if i look at the tensor before and after the depthwise_conv2d the channel number should be preserved, and look something like this: phofcode so imo depthwise_conv2d with dilations and nchw it 's loosing the information about number of channels
these overheads lead to high cpu utilization and limit the throughput our training.
hope the issue will be fixed quick so that i can use the vim completion
a possible implementation of this feature is to prompt user for mfa serial number and mfa token code.
repo: phofurl steps to reproduce the behavior with redux dev tools: 1. clone the linked repo and run the app 2. have redux dev tools installed and look at swagger ui state.
if 'evaluator ' in cluster_spec: raise job is not supported if you don 't use " here the value of cluster_spec is like 'ps ': 'worker ': if 'evaluator ' in cluster_spec.jobs which is the way it should be.
when i do this, everything trains correctly on ml engine but i don 't get logging like i normally do.
*the two systems perform as expected other tasks.
* the lines starting with `.sh` should not be considered as blank lines.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.11.0 python version: 3.5.2 bazel version (if compiling from source): 0.19.0 gcc/compiler version (if compiling from source): 5.4.0 cuda/cudnn version: n/a gpu model and memory: n/a full valgrind output: phofcode gdb falls over like this: phofcode
because when building in model where the input_dimensions and out_put_dimensions differ i need to manually transpose last 2 dimensions in order for model to be trainable.
if there are unsupported ops in the graph for which execution cannot be done, execution must fall back to cpu.
i dont have hadoop installed and am not interested in using it.
i was trying to build a word2vec model by following this tensorflow tutorial phofhyperlink .
i am interested in using the `validation_data` argument while calling `fit()` on a model.
'vim' or 'vim --clean' in linux console shows a 'c' in first position (1st char of 1st line).
1. run `vim --clean` 2.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below): master python version: 2.7 bazel version (if compiling from source): 0.22.0 gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory:
phofcode this yields the following output: phofcode notice that `test write` appears twice in the output.
try and rebuild model with new parameters.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i have the following issue when importing tensorflow in python3 since some days ago: phofcode
- have i written custom code - linux ubuntu 18.04 - tensorflow installed from source - tensorflow version: 1.12.0 python version: 3.6.7 cuda/cudnn version: gpu model and memory: rtx2080 ti i suspect that the problem is caused by tensorflow loading the first rgb channel, (so the red channel) instead of the color indexes, for palette based png images like given example.
using the dcgan tutorial ( phofurl and modifying it to include a summary_writer : phofcode and adding scalar logging to the training step function: phofcode the average time per epoch [running on google colab, and on a windows 10 pc with 1080 ti] is a lot longer when logging: original configuration:
i 'm trying to do tpustrategy for tensorflow 2.0 with disabled eager execution.
i would like to load _resampler_ops.so generated by bazel to fix the bug ( i am not sure am i doing the right thing or not, if not, what should do to fix this bug?). ")
create new content type and put console log to it's rendering three times every page loading.
i am trying to use weight normalization with data-dependent initialization as reported in salimans kingma 2016 phofurl i use two approaches: 1 sharing the variables between initialization and convolution, 2 creating initializers that will be used in convolution to create the variables accordingly.
i haven 't managed a minimal repro of this yet.
the static graph version and the eager version should have the same behavior.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.6 python version: python 2.7.12 bazel version (if compiling from source): 0.13.0 gcc/compiler version (if compiling from source): 5.4.0 2 cuda/cudnn version: 9.0, 7.0 gpu model and memory: titanxp, 12g you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
i am using the fashion_mnist dataset to train a convolutional network, `fashion_mnist = (train_images, test_labels) = when i use a model with conv2d as first layer, more specifically: `model = keras.layers.conv2d(64, (3,3), activation=tf.nn.relu, input_shape=(28,28,1)), ...` then use the fit function: `model.fit(train_images, train_labels, batch_size=1, epochs=5)` an error pops up: ` valueerror: error when checking input: expected conv2d_2_input to have 4 dimensions, but got array with shape (60000, 28, 28)` so i changed the input shape to but another error pops up: `valueerror: input 0 of layer conv2d_6 is incompatible with the layer: expected ndim=4, found ndim=5.
include any logs or source code that would be helpful to diagnose the problem.
steps to reproduce the behavior: 1. add & save connection with ssh key 2. reopen rdm 3. try to connect 4. see error
the code works only when phofcode
server python part, used to convert the exported graph into .tflite file phofcode here is the principal change in script for inference run (java code part): phofhyperlink
- have i written custom code (as opposed to using a stock example script provided in tensorflow): example code for cnn network - os platform and distribution (e.g., linux ubuntu 16.04):windows with anaconda - tensorflow installed from (source or binary):binary - tensorflow version :1.12 python version:3.7 bazel version (if compiling from source):na gcc/compiler version (if compiling from source):na cuda/cudnn version:9.0/7.5 gpu model and memory:rtx 2080
as it happens, enabling eager execution results in 13m49s runtime, while enabling it increases it to 19m17s, i.e.
tensorflow version (use command below): tensorflow version: 1.11 python version: python version: 3.6.6 bazel version (if compiling from source): not compiled source gcc/compiler version (if compiling source): not compiled source cuda/cudnn version: cuda/cudnn version 9.0 gpu model and memory: nvidia geforce mx150, approx total memory 10097 mb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
create maximal num_threads and limit context switches.
note that if ` 'formatoptions '` contains `l` but * not * `a`, long lines are not automatically reformatted (that 's correct).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes.
1. create many-to-many relation between two entities (relation table is created as expected) 2. shut down strapi 3. start up strapi again __error message from above__ if i remove the relation table,ii am able to start up strapi again.
the unit test should run successfully.
memory used by these methods should be released when done.
this is blocking us from upgrading to the latest tensorflow version in phofhyperlink
i tested all variables, all of them are tf.tensor
that is, phofcode however, when set all gpu avaiable, it fails with the following error log.
* ideally * , `tf.reduce_mean` could yield correct results for non-floating-point dtypes as well.
training the network doesn 't fail
@reedwm is this intended behavior?
the accuracy of the nn approaches 99%, yet, this metric says: binary_true_positives: (as shown on the website too).
multiplication of complex numbers typically requires 6 floating point operations (4 real multiplications and 2 real additions) and not just 1 floating point operation.
should work like with tf 1.11.0
in a situation where two variables are involved, optimization of one of the variables leads to wrong gradients for the other variable when placeholders weights are used.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): pip tensorflow version (use command below): python version: 3.6.4 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory:
if including tracebacks, please include the full traceback.
or expects generator to always return number of batches dividable by number of replicas
when i call a `sequencefeatures` layer on a dense tensor, like a tensor produced with numpy, a `typeerror` is raised because `sequencefeatures` `call` method expects `sparsetensor` as input.
if including tracebacks, please include the full traceback.
tflite converter incorrectly shapes the bias for fullyconnected operators.
it returns `none` rather than the partial derivative.
steps to reproduce the behavior: 1. install ubuntu core on raspberry pi 2. install classic mode 3. follow instructions in readme.
i also can reproduce the problem with macos without cuda.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): written custom code for face image classification.
i do not attempt to apply the gpu delegate before the attempting to load it.
it works fine when using `none` as strategy i tried to follow the instruction: phofurl
no internet access and latest handshake timer going up when using wireguard
furthermore, a silent error is being thrown when this happens, requiring me to print status of restore to see it.
phofcode should works without showing unsupported type
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:- - tensorflow installed from (source or binary): source(?)
i have also modified benchmark_cnn.py in above repo to call my custom op, instead of hvd.allreduce.
- os: ubuntu 16.04 lts 64-bit.
so i put an autocmd into my vimrc to `lcd` on bufread and bufenter and it was fine but at some point in the 8.x series it started printing directories to the output.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):yes - os platform and distribution (e.g., linux ubuntu 16.04):ubuntu 16.04 - mobile device (e.g.
t > numpy answer: > -0.25 ] > > [ ]] tnon-sparse tf: -0.25 ] [ ]] tsparse tf: sparse fp16 a, non-sparse fp32 b [[ -0.25 ] [ -0.25 ] sparse fp32 a, non-sparse fp16 b [ ]] non-sparse fp16 a, sparse fp32 b ]] non-sparse fp32 a, sparse fp16 b 0.25 sparse sparse [[ sparse sparse bfloat16(-0.25)] bfloat16(-0.25)]
the gradient becomes an indexedslices through the second gather_nd operation.
highlighting with `matchadd()` and `matchaddpos()` etc.
however, i get stuck at native methods.
in the example below the first instance of tensorflow in the virtualenv should have been able to see the gpu.
thus, this argument is actually only useful if `none` can be specified for any axis and is then expanded to the actual size of that axis in the input tensor passed to the layer.
steps to reproduce the behavior: 1. i followed the steps in the readme for setting up an aws account, to a t.. 2. installed home-brew 3. ran "brew install python3" 4. ran "python3 -m pip install --upgrade virtualenv" 5. ran "python3 -m virtualenv --python="$(command -v python3)" .env && source .env/bin/activate && python3 -m pip install -u pip virtualenv && python3 -m pip install -r requirements.txt" 6. created two users in the config.cfg file, one each for my phone and laptop.
here is my result: test image: two_human phofimage result from cpu (correct): hp_nonflatten_cpu phofimage result from opengles delegate: hp_flatten2_gpu phofimage i believed that this issue is related to the space_to_batch_nd) that opengles not supporting thus fallback to cpu.
when using model.fit it throws an error: > cast string to float is not supported [op:cast] name: cast/
there are automatically generated wrapper classes for graph operations that provide a neat interface and consistency in practice.
in a plugin, i 've encountered a `e488: trailing characters` error in a code path, but only _if an exception was thrown in previous lines_ of the function.
weights are created when model is first called on inputs or `build()` is called with an `input_shape`.
for example, have this: exe "set <m-y>= ey" cno <expr><unique> <m-y> readline#yank( 'c ', 1) before commit, `<m-y>` was replaced by `` in output of `:cno`.
3. make a request to the api with this param.
- - os: rhel 6.5 - terminal: xterm * build succeeds with `--disable-nls`.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no.
key: thal, column dtype: <dtype: 'string '>, tensor dtype: <dtype: 'int32 '>`
i run a simple code to report on the system devices in a 2-worker cluster.
i would expect to not have a runtime failure at inference time on the jvm: either `operationbyname` would recognize the `request:0` node or another java api would exist to fulfill my requirements.
it just works as in the docs
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below): python version: bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" i have trained inceptionv3 classification model with custom dataset and got retrained.pb and retrained_labels.txt.
phofcode produces: phofcode i didn't see this behavior in colab so not sure whats causing this.
otherwise k + axis <= n.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no - os platform and distribution (e.g., linux ubuntu - mobile device (e.g.
generates random jpeg quality for each image/batch of images passed through it.
the scripts used for conversion: conversion_scripts.zip phofhyperlink for inference i have used tflite 1.10 and tflite 2.0. i have used the profiling functionality of tflite and observed that: - in tf 1.10 there are 49 dequantize nodes vs 15 dequantize nodes for tf - conv_2d is ~3x slower for the converted model in tf (when running a float model conv_2d seems similar in both tf versions) - by looking at the run order of the nodes it seems that only depthwise nodes are quantized in but 1.10 expand, project and depthwise are quantized.
(experimental) [y/n]: n clang not downloaded.
when i run this command to compile using a `libtensorflowlite.so` i built: -std=c++11 minimal.cc -ltensorflowlite -lflatbuffers` i get an error `undefined reference to >*) '`
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): colab - mobile device (e.g.
maybe @yaozhang can answer why layout optimizer was disabled for some architectures until 1.8.0 f0d1abbf2 why @zhangyaobit enabled for all architectures since 1.9.0 d4976f7
detailed steps to reproduce the behavior: 1. run `vim --clean` (or `gvim --clean`, etc.)
i have done some debugging and i do not see a restart of running container.
when using tensorflow version 1.12 there isn 't an error: phofcode
the program output is attached as 1.log.
it will exit without waiting other workers.
large logs and files should be attached.
like with using an integer valued sparsetensor i expect tf.sparse.to_dense to return a dense tensor with string values.
the keras code example in the documentation for the `tf.saved_model.load()` function raises an exception: phofcode
if including tracebacks, please include the full traceback.
should work, especially since already successfully loaded into the python process, and so did cudnn64_7.dll
traceback (most recent call last): file line 91, in <module> tfmodel = converter.convert() file line 455, in convert
enabling eager mode or control flow v2 should not affect the training time (or improve it, ideally).
however, if the architecture of model is changed between runs, instead of receiving an error due to incompatible checkpoints, tensorflow is loading old checkpoint into new model even though new dimension is being considered.
- have i written custom code: yes - os platform and distribution: android 7.1 (compilesdkversion 28) - mobile device: galaxy s7 (custom rom android 7.1) - tensorflow version: tflite 1.14.0
the peculiar part is that when omit it seems run on cpu rather inefficiently, in about 980 seconds.
the consequence is that things like vim-airlines branch indicator and vim-gitgutter can cause switching buffers take one or two seconds, rather than a very small fraction of second.
this occurs only in the conv2d_transpose layers.
this issue was resolved upstream in keras in keras-team/keras#8977 if `merge_mode` is used in bidirectional, it should merge the returned state from each wrapped gru even if `return_state=true, return_sequences=false` is set.
when i set inter=1 and intra=1, i expect the usage of cpu is limited to <= 100%.
it creates a model where an input and a variable have same depth.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no - os platform and distribution (e.g., linux ubuntu 16.04):windows 7 home premium 64-bit sp1 - mobile device (e.g.
changed few lines in phofhyperlink : ` try { d.tflite = new modelfilename), d.tfliteoptions); } catch (exception e) { throw new runtimeexception(e); } `
would be happy dig deeper and contribute fix if can find one, but first it would be nice know whether solution already exist (i might just be missing something, perhaps from keras backend submodule).
if a server application is running long enough to go through a lot of examples of same rank but different shapes it would eventually run out of system memory.
1. run this shell command: vim -nu none + 'au winenter * wincmd _ ' + 'bo sp ' + 'set lines=10 ' 2. press `q:` to open the command-line window.
caused gpu hang error (ioaf code 3)
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): (this is the tf 2.0-preview) python version: 3.6.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: i ran the code in tf 1.12, no problem.
cause: converting lp.call <__main__.lp valueerror: unable locate code lp.call <__main__.lp note that functions defined in certain environments, like interactive python shell do expose their code.
large logs and files should be attached.
- it would be nice if vim provided some scrolling functions that could be used in a popup-filter.
--template vim.desktop.in -o vim.desktop msgfmt: unrecognized option '--desktop' try `msgfmt --help' for more information.
w/system.err: caused by: internal error: failed apply delegate: gpudelegate prepare: fuse_auto_input failednode number 133 (gpudelegate) failed prepare.
instructions for updating: use standard file apis to check for files with this prefix.
1. open a scheme file using vim (e.g.
>attributeerror: module has no attribute 'summary_scope '
- os platform and distribution (e.g., linux ubuntu 16.04): <s>ubuntu 16.04.5 lts</s> red hat enterprise linux 7 - mobile device (e.g.
the first choice for me was to build a custom layer using your article here phofhyperlink .
cursor should go to the signature line of `void qux()`.
import numpy as np import matplotlib.pyplot as plt import tensorflow as tf fan_in = 100 fan_out = 100 w = tf.get_variable("w", shape=(fan_in, fan_out), sigma = np.sqrt(2 / (fan_in + fan_out)) with tf.session() as sess: w_now sess.run(w) bins=100) plt.show() print(2*sigma)
a workaround for this is to replace this line phofcode with this: phofcode alternatively there is (i believe) a fix underlying cause here: phofurl
objcuhtamd64/ /zi linking: link /nologo /subsystem:console /opt:ref /ltcg:status oldnames.lib kernel32.lib advapi32.lib shell32.lib gdi32.lib comdlg32.lib ole32.lib netapi32.lib uuid.lib /machine:amd64 libcmt.lib user32.lib /nodefaultlib:lua53.lib wsock32.lib /pdb:vim.pdb -debug
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04.2 lts - mobile device (e.g.
install and configure the create a new content-type with a `media field` for images.
large logs and files should be attached.
w0327 deprecation.py:506] from calling variancescaling.__init__ (from dtype is deprecated and will be removed in a future version.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
check definition of main function.
the worker meets an assertion, and exits.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
can 't be loaded by tf.saved_model_loader - it states phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.12.0-rc2 python version: 3.5 bazel version (if compiling from source): 0.15.0 gcc/compiler version (if compiling from source): 5.4.0 cuda/cudnn version: cuda 10.0, cudnn 7.4 gpu model and memory: 8xv100 32gb
` 2. build fails with following error:
i make the tensorflow to a stastic library and then try to load the model on c++.
it echoes `[{ 'cmd ': '4 ', 'static ': 0, 'name ': 'foo ', 'kind ': ' ', 'filename ': 'xtest '}]`, like it did before
the code snippet below will hang after the last function call.
when evaluating a keras model, the progress bar randomly stops before 100% (however, the loss and metrics returned by the function are correct).
2. have the server return a 404 with an empty response body to have an empty schema appear with no errors.
using phofurl to verify external ip and dns, i expected to see n/a for my dns.
this will cause any multi-byte element of a tensor to be incorrectly interpreted since flatbuffers use strictly little-endian for all numeric values.
i can fix this behavior by manual device placement, by i think that computations should be done on gpu by default.
`swagger-config.yaml` changes the defaults but is overridden by other config mechanisms.
(followed this phofurl tensorflow version (use command below): 1.13.1 python version: 3.6.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: cuda-10.0 , cudnn7.3 gpu model and memory: tx2 (nvidia jetson) you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with: 1. tf 1.0: `python -c "import tensorflow as tf; print(tf.git_version, tf.version)"` 2. tf 2.0: `python -c "import tensorflow as tf; tf.version.version)"` include any logs or source code that would be helpful to diagnose the problem.
the motion should only bring the cursor onto the `{` on lines 1, 3, and 6 as start of class, start of method, end of class, respectively.
`:e` 13. the buffer is now populated with `b` again
`decode_predictions` seems to be from here phofhyperlink instead.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): tf 1.13.1 python version: 3.6.7 cuda/cudnn version: gpu model and memory:
when loading a custom model using the tf lite api for android, the output tensor dimensions become malformed.
the returned object should be a legal tensorarray instance, as is the case when you use only eager mode functions.
short repro: prepare files: $ cat > std_function_fw.h #include <functional> int f); d $ cat > std_function_fw.cc #include "std_function_fw.h" int f) { treturn f(42); } d $ cat > std_function_client.cc #include <iostream> #include "std_function_fw.h" int main(int argc, char
i your cpu supports instructions that this binary was not compiled to use: sse4.1 sse4.2 avx avx2 fma i cpu frequency: hz i xla service executing computations on platform host.
from tensorflow 1.13, there is integration between autograph and tf.eager.function.defun (alias of tf.function in tf2.0 maybe?)
on intel: phofcode on s390x: phofcode
instructions updating: use standard file utilities to get mtimes.
i expect the code to return a single number.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): docker pull tensorflow version (use command below): 1.12.0 python version: 3.5 bazel version (if compiling from source): not source gcc/compiler version (if compiling from source): not source cuda/cudnn version: 7.1.4 gpu model and memory: p80 12gb include any logs or source code that would be helpful to diagnose the problem.
failed to deploy to do using image "freebsd-11-2-x64-zfs".
the statusline should not flicker while scrolling.
if including tracebacks, please include the full traceback.
the following code adds a trainable variable y to my model in tf 1.12. it does not work in tf 1.13 currently the operation y = k.variable([0.0], dtype=tf.float32, name= 'y ') add_y = lambda(lambda x: tf.math.add(x,y))
you can manually explore an api doc by putting in the url and it will use settings from config.json file as defined.
my codebase works correctly on tensroflow r1.12 given the error and my reading of the tensorflow source code throwing the error, my best guess is some op is missing/has a malformed device_spec, or a deivce_spec object is being passed into the function.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): '2.0.0-preview ' / python version: python 3.6.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: no gpu model and memory: no you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
the tf.tuple can 't be indexed by tensor in graph mode.
can 't build all tflite micro 's samples on windows or some embedded systems (requires visual c++).
we should be able to train the model when restored from savedmodel.
the screen is restored without colors changing.
2. type `make test` or `make test_quickfix`
here is log: w/system.err: internal error: failed to apply delegate: gpudelegate prepare: fuse_auto_input failednode number 133 (gpudelegate) failed to prepare.
using :set relativenumber and :set norealtivenumber lets the whole screen flash.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): red hat 4.8.5-16 - mobile device (e.g.
it seems setting `allow_growth` to true takes much more vram than needed.
using index 1 or 2 gives same error as not giving an index.
").format( --> 146 name])) 147 exported_function = value 148 previous_attribute_name = name valueerror: exporting an object with no tf.saved_model.save(..., signatures=...) argument specified, and with more than one @tf.function-decorated method attached to it: 'serve '].
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04.2 lts (gnu/linux s390x) - mobile device (e.g.
it should use value that is set
the tensorflow lite model should run without errors in gpu and cpu with element-wise operators.
large logs and files should be attached.
compile the without bitcode support: bazel build -c opt then for comparison compile it with bitcode support: bazel build -c fastbuild --apple_bitcode=embedded --copt=-fembed-bitcode use these with the experimental tensorflowliteswift cocoapod (not yet publicly available, see #25800 for reference) to perform a model inference on a real device or a simulator.
in the code of [found here phofhyperlink , first line `y_true = axis=-1)` shrinks one dimension of `y_true` for apparently no reason.
1. query a content types relationship with variable in a nested filter 2. compare the results to the same query entering the string directly in the filter
using oauth2 with authorization code flow should work while using phofurl
gpu memory is very high when using init_from_checkpoint and reuse=true with resource variables, in certain cases.
well, i admit that the _correct_ behavior of such a menu is not obvious (which window should be closed?
i think the changed result should be store in the chekpoint.
-0.5 0.25] stateful: [ 1.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): macos x 10.14 - mobile device (e.g.
if including tracebacks, please include the full traceback.
i merely want to clarify the autograph behavior and document this for other tensorflow users.
the model loaded from savedmodel should keep the named inputs.
i implemented a custom op with zero_out_op_kernel_1.cc phofhyperlink in tensorflow repository.
the test should execute normally.
however, uncomment the line and you will get a `valueerror`.
while trying to remove a fold, the error `e315` is raised and some text is wrongly removed in addition to the fold marker.
... this is not necessarily a bug, but maybe a react best practice.
... steps to reproduce the behavior: 1. use content type application/octet-stream for a response 2. open swagger-ui page and look at response example value
(float32_ref 256) [256, bytes: 1024] (float32_ref 1) [1, bytes: 4] (float32_ref bytes: (float32_ref 128) [128, bytes: 512] 128) [128, 512] 128) [128, 512] [73728, 64) [64, 256] 64) [64, 256] 64) [64, 256] 3x3x64x3) [1728, 6912] 3) [3, 12] total size of variables: total bytes of variables: [*] reading checkpoints... [*] failed to find a checkpoint [!]
- vim version [gvim - os: windows 7
my goal is to run model on google cloud platform on a couple of gpus at once, but first i need to get code running locally.
i am trying to migrate 1.12 code using estimator to tf 2.0 using everything from keras as it is suggested.
pasted yaml data should still be present.
for some reason, this bug crops up when using reshape in a skip connection as shown in the sample code below.
does anyone know how to debug the tensorflow source code with the test module; i have tried to use bazel build -c dbg while_loop_test, but it seems that it will compile all the project, and it will take much time.
here 's how i set up input/output tensors: // setup output tensors int outputtensorindex = 0; int[] outputlocationshape = datatype outputlocationtype = outputtensorindex++; int[] outputclassshape = datatype outputclasstype outputtensorindex++; int[] outputscoreshape datatype outputscoretype outputtensorindex++; int[] numdetectionshape datatype numdetectiontype= // setup input tensor int imagetensorindex 0; imageshape // {1, height, width, 3} imagesizey imageshape[1]; imagesizex imageshape[2]; imagedatatype // creates the input tensor.
when i set a zset member 's score, i usually got "syntax error"
last line should extend beyond the length of screen.
here is my code: phofurl phofhyperlink run on colab: 1. tpu: phofurl phofhyperlink 2. gpu phofurl phofhyperlink
since pb file size big, do not upload it at this point thanks.
`_make_train_function` keras_team keras fit(): 1.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.6.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: here is the stacktrace: python >>> traceback (most recent call last): file "<stdin>", line 1, in <module> file line 541, in __call__ file line 1572, in _maybe_build self.build(input_shapes) file line 949, build trainable=true) 355, add_weight aggregation=aggregation) 612,
phofcode this will immediately output phofcode and then output every 10 seconds `tf.tensor(2, shape=(), dtype=int64)`
i want to show some status text before it 's done.
i build a model with this code: phofcode then i froze the model.
open admin interface and see how they appear differently.
rotation phofimage when angle is randomly chosen with `angle = tf.random_uniform([])`, there is an offset between rotated and rotated points.
if including tracebacks, please include the full traceback.
the reproduce code as below: phofcode the photo below is the behavior in my computer.
- have i written custom code: yes - os platform and distribution: linux ubuntu 16.04 - mobile device if the issue happens on mobile device: dell precision tower 7910 - tensorflow installed from (source or binary): tensorflow-gpu 1.12.0 tensorflow version (use command below): tensorflow-gpu 1.12.0 installed from: conda python version: python 3.6 gcc/compiler version (if compiling from source): 6.5.0 cuda/cudnn version: 9.2 but nvcc 7.5 gpu model and memory:quadro m4000
however, running it the second time results in this warning message: phofcode
when passing them through the model api, causes them to have an unexpected behavior.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): red hat enterprise 7.6 - mobile device (e.g.
(x-www-form-urlencoded) example swagger/openapi definition: phofcode curl1 phofimage
devices: i streamexecutor device (0): <undefined>, <undefined> w (one-time warning): not using xla:cpu for cluster because envvar was not set.
warning:tensorflow:from initialize_all_variables (from is deprecated and will be removed after instructions for updating: use instead.
this happens because vim uses screen scraping to save the screen and those legacy console apis do not support 24-bit color.
with `windo` this is annyoing as i'm using it to automatically do some things in various windows but i don't want the layout to change.
large logs and files should be attached.
(env) ./algo play [ask user for the input]
consistent behaviors for both resnet-50 and vgg16.
bug tf 2.0 ----> 2 score = 3 4 # print test accuracy 5 print( 'loss: ') in test_on_batch(self, x, y, sample_weight, reset_metrics) 1310 # validate and standardize user data.
i also, just to check, ran pip install --upgrade tb-nightly i believe on the 25th.
the only difference in the code is the following lines: phofcode
when closing popup window with `<c-w>z` no messages appear ever.
4. then call the problematic command phofcode and you should get error: phofcode
here is a minimal example, also runable in colab phofhyperlink : phofcode
same code was working few days ago, also the example codes in seedbank are using same code and they were working fine.
all models work as good as your hosted model
large logs and files should be attached
test on mobile safari and you should see a black rectangle instead of the gradient: phofurl - here's a trimmed down one without the svg animation just in case it get's distracting and to show that it's not part of the issue: phofurl - here's another that i tried to create svg outside of svelte's element creation as test: phofurl - last, here's it working (just using vanilla js and svg el, same link in expected behavior) phofurl
running the `train()` procedure provided below breaks while using the `@tf.function` decorator.
to use an object in pyspark it must be serializable, but i am getting a pickle issue when trying use a `savedmodel` 's concrete function.
it either just happens after a long enough period of time, or if an error is produced and left for a long enough period of time.
so if a script creates multiple such graphs in a row, it will use more and more memory unnecessarily.
if you are using pure `keras` with no `eager` mode, then you can change learning rate of keras optimizer in following way value)` but as `keras` optimizers aren't supported in `eager` mode, i don't have any control over learning rate of my optimizer.
written to: * data input_fn: input file(s): batch size: 1024 epoch 1000 mode: train thread 32 shuffle: true ( 'target_dtype ', <tf.tensor 'decodecsv:9 ' shape=(?,) dtype=int32>) info:tensorflow:create checkpointsaverhook.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): n/a tensorflow version (use command below): python version: 3.6.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: gpu model and memory: f0 f1 f2 f3 f4 f5 f6 f7 f8 f9 f0 f1 f2 f3 f4 f5 f6 f7 f8 f9 ``
make a file called `zero_out.cc` with the following contents (as per the example from phofurl phofcode make another file called `test_zero_out.py` with the following contents: phofcode ensure that gcc 5 is installed, along with python 3.6 (on ubuntu 18.04), and execute the following: phofcode this compiles and runs custom op.
2-4x performance degradation intel mkl.
the load does not get shared by all the available cores(mostly one core at a time), as a result the performance is poor.
i get neumf model source code from official models repo, and original code only support local training with estimator, and i add below code to support distributed training: phofcode the `cross device ops` implement of `mirrorstrategy` is `multiworkerallreduce` which is defined in i change value of `all_reduce_alg` parameter of `__init__` of class `multiworkerallreduce` from `pscpu/pscpu` to `nccl/xring`.
in the first try i converted it to tflite directly, in a second try i set the = true` option in the conversion process to quantize my model and in a second try optimized my model for inference before converting and quantizing.
i found a similar bug in the js project but not sure if it's related: phofurl
run this shell command: $ vim -nu none + 'sil ", 3)|setl list|wincmd w|b x ' -o x y in the right window: - the ` 'list '` option is not set (`:echo &l:list` is 0) - the end of lines are not displayed with a dollar character - the tab characters are not displayed with `i`
-d_glibcxx_debug compiled example code gives: invalid argument: must specify at least one target to fetch or execute.
the sign column also appears suddenly.
error arises during concatenate when i run the following code: phofcode `valueerror: graph disconnected: cannot obtain value for tensor shape=(none, 256, 256, 16), dtype=float32) at layer "concatenate_8".
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macosx 10.13.6 - mobile device (e.g.
the output minimum value of `max_pool_2d` operator is 0 instead -19.3, which would throw said warning.
the same problem is same with each.
i suppose the tf_loadlibrary should work and fix the non-registration error.
- have i written custom code: yes - os platform and distribution: red hat enterprise linux server release 7.4 - tensorflow installed from (source or binary): source - tensorflow version: 1.10.0 python version: 3.6.6 cuda/cudnn version: v9.2.88 gpu model and memory: intel(r) xeon(r) cpu e5-2690 0 @ 2.90ghz e not found: tensorflow device gpu:0 was not registere
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): no - mobile device (e.g.
importing tensorflow causes cpus allowed list to single core on main thread.
when writing a heredoc to assign a list of strings to a variable, sometimes, a line beginning with backslash is unexpectedly joined with the previous one.
the code is exactly the same, only change is the model path and data path.
i wish i had a portable example for you but i 'm not quite sure why my code triggering this path and i don 't have much time to investigate the root cause, but i can confirm that is an error that have encountered naturally.
it is required that the penn treebank dataset is downloaded/available (directions in source).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): osx 10.14.3 - mobile device (e.g.
steps to reproduce the behavior: 1. go to '...' 2. click on '....' 3. scroll down to '....' 4. see error
i expect to be able to iterate over the rows of a `raggedtensor` without any error.
instructions for updating: colocations handled automatically by placer.
- os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 / wsl - mobile device (e.g.
math ops perform similarly to when they have dense tensor inputs but returning a sparse format, e.g.
`base_data_path = ` `model_file = 'my_model.h5 ')` `model = `model.summary()` `print( ' # generate predictions ')` `predictions = verbose=1 )` `print( 'predictions shape: ', predictions.shape)` `print(predictions)` `saved_model_dir `!mkdir -#p saved_model_dir` ` pythonlstmcode.txt phofhyperlink
that means if the src file is shorter than the dest file, the resulting dest file contains mix of two.
sigabrt (double free or corruption) occurs when changing mintty screen size after calling popup_create() with 'pos ': 'botright '.
the page on tensor/models claims 569mil macs, but number of macs this modified 1.0 has is unclear.
python !pip install import math import numpy as np import tensorflow as tf from tensorflow.keras import backend as k class def __init__(self, optimizer, cool_period = 10,
turning off `syntax on` or `mouse=a` prevents the cpu usage spike
profile data after optimization: node name | requested bytes | total execution time | accelerator execution time | cpu execution time matmul (100.00%, 88.05%), 1.11sec (100.00%, 46.86%), 0us (0.00%, 0.00%), 1.11sec (100.00%, 46.86%)
initialize grpcchannelcache for job worker -> {0 -> localhost:3002} started server target: grpc://localhost:3002 server already started (target: grpc://localhost:3002) w0327 `eval_fn` not passed in.
you can use any below by adding "--config=<>" your command.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the training commences and finishes normally.
don 't know what would be helpful, but returns true.
in addition, the check failure message should provide more information about the location of the error, such as originating layer names (which are visible in graphviz outputs).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): manjaro 18.0 - mobile device (e.g.
given the following code: phofcode i get the following traceback: phofcode
i should see phofcode displayed in example box
20. names both tables "parameters" "responses", respectively.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04.5 lts - mobile device (e.g.
`info:tensorflow:running training and evaluation locally (non-distributed).
place the cursor at the asterisk in the following code sample: namespace foo { class bar { void qux() { // press [m here -> * } } }
model trains good for the first 3-5 folds but it eventually returns a out of memory error.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): osx mojave 10.14.5 - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): anaconda tensorflow version (use command below): 1.12.0 python version: 3.5.0 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: na gpu model and memory: na you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
when we use despite the number of workers or `max_queue_size` memory keeps growing linearly up to point when process gets killed by system.
but lately there is new test debris, specifically from the testing of indent files.
whenever it is called by tf is gets `nullptr` instead valid value of the `inferencecontext` argument.
<class typeerror traceback (most recent call last) in <module>() 32 model = sequential() 33 model.add(dense(8, input_shape=(20, 20))) ---> 34 model.add(mylayer()) in _method_wrapper(self, *args,
using original keras, fitting is good in any case.
please find the source code @ phofurl [update] my latest test case @ phofurl stackoverflow rereference : phofurl
_the problem occurred only once so far._
when using the coc.nvim plugin and toggling its extensions, i get a sigabrt from a double free.
managed to get it all working, and installed tensorflow-gpu in anaconda (i am working within an anaconda environment).
error message: in converted code: typeerror: tf__call() takes from 2 to 3 positional arguments but 4 were given this is how the call method is defined in my class.
python import tensorflow as tf model = tf.keras.sequential( [ ] ) strategy = with strategy.scope(): out = model(tf.zeros((1, 10)), training=true) ``
correct behavior is seen with tf.image.resize(o, size=size, keras upsampling should use this as the default instead of the current defective behavior.
run this shell command: vim -nu none +'syn on|h :au' the text `note:` is correctly highligted by the syntax group `helpnote`.
here is a minimal example to reproduce the issue (the case of `indirect = true` can be used as a workaround and shows the expected output in contrast of `indirect = false`): phofcode example output: phofcode
when using `x.to_tensor(...)` on the elements of the dataset, the shape should be evaluated to the shape of tensor.
can use the google research bert and profile for memory using profilerhooks.
loading annotations into memory... done (t=0.47s) creating index... index created!
if rnn layer is not present, and data 2d (like (none, 10) ), model can train normally.
for every kernel that is queued on a gpu i expect it to be executed at some point and logged in the timeline
the below code has been taken from the tensorflow without phd series - rnn time series prediction.
- vim version - os: windows 7 if signcolumn is no, this does not appear wrong
vim then hangs here when reading it: phofurl `getc` returns `eof` but vim doesn't check for that.
screen shot at 20 26 47 phofimage - vim version phofcode - os: ubuntu 19.04 - terminal: xterm(330) add any other context about the problem here.
if including tracebacks, please include the full traceback.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n * tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.5.2 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: 10 gpu model and memory: gtx 1080 8g you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
* note that you have just "derived" your new modified value from "first".
that is, when i import tensorflow (even not using it).
exclude the lstm `c` state from the list of dropout candidates.
editing out the skip connection makes the code run fine.
even these apis are called in different threads, interpreter.run() should return correctly with blocking issue.
large logs and files should be attached.
copied from my repository phofurl phofcode
`tf.nn.conv2d` should be evaluated, which is done iff the convolution is explicitly placed on the cpu (e.g.
to iterate over this tensor use tf.map_fn.` also decorating the method `_inner` eliminate the error.
- os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 on wsl, also colab - tensorflow installed from (source or binary): binary (pip, no gpu) - tensorflow version (use command below): 1.13.1 python version: 3.6.8 (anaconda)
phofcode only accepts phofcode unless explicitly passing phofcode arg to phofcode .
* placing the cursor at line 4 and using the `ip` operator should select the lines 4, 5 and 6.
sudo python setup.py install ... file line 31, in runtimeerror: python version >= 3.5 required.
in mock example, model consists layer 100 lstm cells with inputs masking (due to length variability) and default tanh activation topped with single feed-forward unit with sigmoid activation.
the only related (closed) issue i could find: #19497 "nhwc convolution sometimes incorrectly considered nchw" ` invalidargumenterror (see above for traceback): default maxpoolingop only supports nhwc on device type cpu t [{{node label_image_dilated}} = maxpool[t=dt_int32, data_format="nchw", ksize=[1, 1, 3, 3], padding="same", strides=[1, 1, 1, 1] phofhyperlink ]] t [[{{node oneshotiterator_2}} = output_shapes=[[?,?
when `f` flag is present in `shortmes`, some autocmd does not work properly (the `echom` command does not display messages)
see below, the working runconfig is commented out, the non-working one not so.
it should train fluently, and i try on google cloab, it try fluently, but it train error on my computer.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
not able to call more than once to create tf-trt nodes for disjointed sub-graphs.
install python-prctl so processes can be cleaned guarantee.
i have a working proof of concept, and can submit the feature request
it comes with 1 gb of ram, and i'm not expecting it to run anything really large, but it's an ideal machine from a cost perspective to try out tensorflow basics, and it works just fine with tensorflow 1.14 and earlier.
to "fit" the model without error
101.54mib 58.83mib 53.54mib 10, 9.
image phofimage - os & version: microsoft windows [version
the model is the exact implementation of the cvae tutorial from tensorflow website: phofcode because layer size is increased, i suspect it has something to do with memory, because earlier issues refer 'dst tensor ' issue to a memory problem with gpu.
the output during prediction from rnn layers, i.e.
- here's the original version.
not sure what to make of either of these errors.
w cannot find shardable dataset, adding a shard node at the end of dataset instead.
this seems to be a problem with any multi-input rnn with stateful=true.
large logs and files should be attached.
phofurl please see the readme
i'm an identical conversion process for both models.
the training crashes with an error( valueerror: you must specify an aggregation method to update a mirroredvariable in replica context.)
it's very unnatural that we need to call or to save records in eager execution mode.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
import numpy as np import tensorflow as tf x = 100), tf.float32) y = 100, 100), tf.float32) z = 100), tf.float32) class model(tf.keras.model): def __init__(self): super(model, self).__init__() self.layer = tf.keras.layers.gru(100) @tf.function # remove this and it works fine def call(self, x, y): z self.layer(y, initial_state=x) return z model model() with as tape: # if persistent=false it works fine loss tf.norm(model(x, y) - z) grads tape.gradient(loss, ``
* the code is modified from the stock example for quick experiment.
given task id 472 --> 473 return executor.run() 474 475 run(self) 611 config.task_type != 612 logging.info( 'running training and evaluation locally (non-distributed). ')
python import numpy as np import tensorflow as tf import os import matplotlib.pyplot as plt psutil def memory(): pid = os.getpid() py = psutil.process(pid) memory_use = py.memory_info()[0] / 2.
during handling of the above exception, another exception occurred: traceback (most recent call last): "issue2.py", 19, <module> output = distributed_run(x) 432, __call__ *args,
but when i converted it to estimator as shown in migration guide phofurl it fails with error python valueerror traceback (most recent call last) in <module>() 1 estimator = ----> 2 keras_model = model 3 ) 9 frames in wrapper(*args,
i expect `relu` 's input minimum value (for quantization) to match the input tensor 's actual minimum value (a negative real number).
2. open swagger ui with the browser console open.
if including tracebacks, please include the full traceback.
include any logs or source code that would be helpful to diagnose problem.
i am trying to add a trainable variable y to my model.
memory usage keep increasing and never down after gc collection.
i except the window to close and be back authenticated on swagger ui interface (like in chrome).
the canberra library is linked in when making a tiny build (also normal, and probably small).
w the tensorflow library wasn 't compiled to use sse4.1 instructions, but these are available on your machine could speed up cpu computations.
running this code gives a directory called logs.
code is in description above.
furthermore, it has worked fine with the same version of keras-retinanet when using older versions of tf and cuda (see above).
a clear and concise description of what you expected to happen.
i expect correct model loading behavior from both checkpoint and hdf5.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): via anaconda tensorflow version (use command below): 1.12 python version: 3.6.8 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): 4.8.5 cuda/cudnn version: n/a gpu model and memory: n/a
phofcode the complete code can be found at: phofurl
detailed steps to reproduce the behavior: 1. use `set shell=c:/program 2. test that `:terminal` works.
so, making vim in a terminal preserve a multi-line output when focusing a different window, would make the latter more consistent with gvim.
run these shell commands: $ cat <<'eof' >/tmp/a.txt fold one title {{{1 some text fold two title {{{1 some other text eof $ vim -nu none +'e /tmp/a.txt|setl fdm=marker|sp b.txt|mksession!
detailed steps to reproduce the behavior: 1. run `vim --clean` in cmd 2.
fatal python error: aborted current thread (most recent call first): file line 33 in execute file line 251 in _run_main file line 300 in run file line 40 in run 59 main 9 <module> unpy.py", 85 _run_code unpy.py", 193 _run_module_as_main
i have a graphdef and checkpoint i am trying to freeze.
variable is expected to be iterable, just as eagertensor, but right now it raises typeerrors
`z = tf.stack([x, y], axis=3), where x and y both are 4d tensor.` what can i do to get the interpreter run?
: i am trying the newly released api.
with any model with the loggingtensorhook provided as the hooks parameter, it throws an exception when trained with tpus.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:qualcom 801 platform - tensorflow installed from (source or binary):source tensorflow version (use command below):1.11 python version: bazel version (if compiling from source):1.18 gcc/compiler version (if compiling from source):gcc 5.4 cuda/cudnn version:na gpu model and memory:na
then i tried to test the file with interpreter.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04.5 lts (xenial xerus) - mobile device (e.g.
add any other context about the problem here.
running rm command with job_start behaves differently than running it in the command line.
the `xvfb-run gvim` command to write a file `test` with the contents `1`.
import tensorflow as tf import numpy as np np_arr = np.array([[1.,2.,3.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux mint 19.3 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0.0b1 python version: 3.6 cuda/cudnn version: / gpu model and memory: gtx 940mx, 430.26 phofcode
phofcode i think this is essentially the same as #8905, except that here i am using single precision.
a simple example: phofcode however currently the models persist (can be seen in %tensorboard) and continue to fill up gpu memory.
dose this mean i have to slow down my multi-gpu quantization-aware training in future version of tensorflow?
however unlike every other error in tensorflow, it is written to stdout instead of stderr.
2. mv .vim abc 3.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
change the two instances of `let t_f2` to `let &t_f2`.
in the example below, an effort has been made to add simple navigation to a popup buffer.
tf.nn.top_k is throwing a `valueerror` despite clearly using the `topkv2` op under the hood.
setting 'use_eager_mode' to true/false switches between the two cases.
we are developing based on tf keras, then this behavior is not backward compatible .
3. click 'toggle swagger ui' button a couple of times 4. look at swagger ui state in redux dev tools.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): there doesn 't seem to be any example of using subclass api with model.fit with embedding layer - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
it has excess output that looks like it 's coming from immutablejs.
i would expect that `model.fit()` restarts from the beginning in the validation dataset after every epoch of training.
zip file also contains two requirements files: `requirements_1.10.txt` and `requirements_1.13.txt` streamline setting up virtual environments test issue.
one another log phofhyperlink shows influence of disabling layout-optimizer in commit d4976f7 (that introducing drop performance) some release versions of tensorflow.
phofurl the first line `a a` shows that the initial value of the array is `['a']`.
phofcode if the input batch is 2: phofcode however, the test case passes if the batch dimensions are identical.
vim - vi improved 8.1 (2018 may 18, compiled apr 29 2019 included patches: 1-1234 compiled by <alexpux@gmail.com> huge version without gui.
a clear and concise description of what the bug is.
tf.matmul is failing to multiply matrices above a certain size, with error: `failed to run cublas routine cublasgemmbatchedex: i have confirmed using `nvidia-smi` that the gpu is nowhere close to running out of memory.
should not return this error
i expected the generated spring server would contain two controllers, one per tag
however, in rnn models such as gru and lstm, some variables are attached to enter op which are incompatible with the generated constants.
is there a modification needed for custom layer?
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
1. create a file `text.txt` which is not encoded in utf-8 format.
this code is taken from the cnn tutorial phofhyperlink with the exception of a line added to try limiting the gpu memory usage.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes, provided below - os platform and distribution (e.g., linux ubuntu 16.04): various - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac osx 10.14 - tensorflow installed from (source or binary): pip install - tensorflow version (use command below): 1.13 python version: 3.6 epoch 1/3 traceback (most recent call last): file "sparse.py", line 24, in <module> epochs=3, steps_per_epoch=1) file line 880, in fit file line 266, in model_iteration batch_outs = f(actual_inputs) file line 3046, __call__ sparse_coo = value.tocoo() attributeerror: 'sparsetensor ' object has no attribute 'tocoo ' ``
train_and_evaluate should run evaluation after model save checkpoint
python import tensorflow as tf import tensorflow.keras.layers as layers class variable(layers.layer): def __init__(self, initial_value,
so what with rampant memory usage?
`[on]` is displayed in the status line, and the cursor stays where it is in the buffer.
it also applies to tf.gather_nd.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
5.0.1] add any other context about the problem here.
when i run the model, the program used almost all the cpu cores on my server, and thread spawned to several hundreds.
!pip install -q tensorflow==2.0.0-beta1 from google.colab import auth auth.authenticate_user()
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): source tensorflow version (use command below): 2.0.0-beta0 python version: 3.6.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): 7.2.1 cuda/cudnn version: v9.0.176 gpu model and memory:
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13 python version: 3.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: n/a gpu model and memory: n/a error thrown: > error:tensorflow:model diverged with loss = nan.
i used this phofhyperlink nvidia-docker image.
note that using the .tflite model given by the tflite object detection example script produces no error with the same picture (although it wouldn 't yield any pertinent result).
note also that some transposes are implicit (performed, say, by `tf.tensordot()`).
the author used tf feature column and tf data alongside a sequential model, which worked out just fine.
im assuming it tests the `switch` ead branch, which should be non-used branch for inference.
like in `vim i expect that only background color will be changed, but foreground color is should stay the same.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
my custom app is based on the tensorflow lite android image classification example.
- have i written custom code: yes - os platform and distribution: ubuntu 16.04 - tensorflow installed from: binary - tensorflow version: python version: 2.7.12 text traceback (most recent call last): file "tf2/class.py", line 10, in <module> print(layer.variables) file line 1330, in variables return self.weights file line 708, in weights return self.trainable_weights + file line 687, in trainable_weights nested = 1850, getattr(layer, attribute) for layer nested_layers)) typeerror: 'property ' object is not iterable ``
large logs and files should be attached.
3. observe that "first" is the active example.
instructions for updating: use standard file apis to check for files with this prefix.
when the first epoch of training ends i get following error: resource does not exist.
expected: trainable variables are updated by optimizer, but untrainable variables are not.
f some of the operators in the model are not supported by the standard tensorflow lite runtime.
compute_output_shape should always return correct results
to reproduce, run the following on a multi-gpu system and check nvidia-smi during the 20 second sleep.
1. run `vim --clean popup.vim -c "source %"` phofcode 2. result: `e475: invalid argument: testprop` 3. when `"bufnr"` isn 't specified, everything works fine: phofcode
1.1. make sure `vim` was compiled with either python2 support, python3 support or both.
ubuntu18.04lts local installation unknown error
- have i written custom code (as opposed to using a stock example script provided in tensorflow): below - os platform and distribution (e.g., linux ubuntu 16.04): mac 10.13 - tensorflow installed from (source or binary): `pip` - tensorflow version (use command below): 1.9.0` python version: `3.6.5` you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" phofcode
this happens for example when `{cmd}` outputs an error because of wrong usage, or when `--help` is passed to it.
i think both windows and macos should echo `4`, which mean `term_getline` should contain the last several spaces.
`cat label.pbtxt` > some txt in the file 'label.pbtxt ', overwrite=true)` `cat label.pbtxt` > some txt in the file
with 20 inception graphs loaded and unloaded, the ram usage goes up to around 2gb.
using `xla.compile` on a gradient coming from a while loop within `tf.custom_gradient` seems to generate an inverse permutation op with undefined shape.
then run the following command: phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
the error only is showed when an embedding layer with trainable=true, is followed by an lstm/gru layer
the problem might not be connected to convolution operation but it fails after increase size this tensor.
in the predict fuel efficiency: regression lesson and use ml->regression), the normalize method is called to normalize the entire dataset, including the one-hot columns.
... visit phofurl and execute a request after having selected a file with either an invalid extension (e.g., foo.abc) or potentially a good extension that is unrecognized (such as nuget 's 'foo.nupkg ').
same output as making predictions immediately after training.
say you train an estimator using phofhyperlink .
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): nightly python version: 3.5.2 cuda/cudnn version: 9.0 gpu model and memory: nvidia titan xp (12196mb) i think this issue occurs because inside `unwrap` function in it does nothing if `innermost_decorator` is `none` thus, if i change line in as phofcode it works.
for example, i specified it to use no more than 50% of gpu memory.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
this function just generates a unique identifier for a tuple by taking the object ids of all the components, converting them to strings and concatenating.
if including tracebacks, please include the full traceback.
conversion to fp32 using tensorrt fails on with: phofcode with conversion to fp32 fails with: phofcode for conversion to fp32 succeed but with int8, the error is similar to `vgg19` with fp32: phofcode
i expect the relative urls in the oauth definition to be resolved to the current api server.
observe that a single line of border of _first_ items preview popup is still visible in when second item is selected.
loading saved functional model raises exception.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
large logs and files should be attached.
in const&, const&) () from (gdb) bt #0 in const&, const&) () #1 () #2 const&, tensorflow::graphdef*) () #3 tensorflow::graphdef*, #4 const&, tensorflow::graphdef*) #5 tensorflow::graphdef*) #6 tensorflow::configproto tensorflow::metagraphdef bool, std::string tf_status*) #7 _wrap_tf_optimizegraph #8 ??
... steps to reproduce the behavior: 1. open swaggerui 2. extend address with "?urls=[]" or change inital swaggerui configuration inside index.html to something like: phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: none - tensorflow installed from (source or binary): source tensorflow version (use command below):1.12-gpu python version:3.6 bazel version (if compiling from source):0.19 gcc/compiler version (if compiling from source):5.4 cuda/cudnn version:9.0/7.3.1 gpu model and memory:gtx 1080 ti
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): pip install tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: cuda 10.0, gpu model and memory: geforce gtx 1070 8gb
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the tensor with tensor 's shape (2,) is the correct output when apllying flat_map to a dataset containing x sample, each consting of two values.
regular keras printed progress bars correctly, but switching to tensorflow.keras now spams my screen with control characters when trying to print progress bars.
the model can be trained regardless of because i used model.train_on_batch().
the model can be successfully frozen and loaded in cnn, however, failed to work in rnn such as lstm and gru
the use of `@tf.function` should correctly create autograph
google 's re2 library seems to be unable to handle rewrite patterns with characters like ` `, ` t`, etc.
ive used ` 'autochdir '`, so that working directory matches files directory.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 2.0.0-beta1 python version: 3.6.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: attributeerror traceback (most recent call last) in <module>() 25 26 from google.colab import auth ---> 27 auth.authenticate_user() 28 29 prepared_record_paths = in 154 with as sess: 155 with open(_get_adc_path()) as auth_info: --> 156 157 sess, 158 if _check_adc(): attributeerror: module 'tensorflow' has no attribute 'contrib
`e154: duplicate tag "test-functions"` occurs.
i get a valueerror: must be called at program startup.
the behavior goes away and works without problem if i comment out the loggingtensorhook parts, yet i have no idea what 's the relevant issue.
python import numpy as np import tensorflow as tf import os print(tf.__version__) class def __init__(self, c_out, kernel_size=3): super().__init__() self.conv = kernel_size=kernel_size, strides=1, padding="same", use_bias=false) self.bn = epsilon=1e-7) def call(self, inputs): res = return res class fnet(tf.keras.model): def __init__(self, start_kernels=64, weight=0.125,
i did try using but its a bit wonky in a few ways (and didnt demonstrate the issue).
the time taken per epoch hugely varies from 10 min to 2 hours from 1st epoch to 2nd.
`:tabedit test.txt` to created another tab 5.
code to report the mean absolute discrepancy between the empirical and theoretical means: phofcode code to visualise distribution: phofcode
below is the traceback from this internal error.
some pixels are present twice and this gives wrong pixel values in scatter.
i have a model that accepts two string features, say 'subject ' and 'body ', and predict if the string content is spam.
running `:pedit file` always opens an empty popup window with the message: phofcode closing the window with the mouse * sometimes * echos the message: phofcode (one time vim even crashed).
ideally, it would be nice if the `cursorline` option could be extended so that users can select the highlighting behavior.
un_webcam.py --camera 0` thx :-)
the status line should show "/tmp/
expected tensorflow match nvidia-smi device indexes
if you try set_weights inside tf.function, your program crashes because keras attempts to convert tensors into numpy arrays phofcode
if there's any reason why instantiating a dynamic_rnn with tf.int32 type in its input and state should not be allowed, a custom error should be raised.
this results in `check failed: dim_x == dim_y` error.
it is strange that i already use 2.0.0-beta1 in other virtual environnements and never saw these warnings before.
running `toco` with or without arguments throws this traceback: phofcode
unknown error and failure initialize gpu.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:na - tensorflow installed from (source or binary): pip tensorflow version (use command below):1.12 python version:3.6 bazel version (if compiling from source):no gcc/compiler version (if compiling from source): cuda/cudnn version no gpu model and memory: not using gpu you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" additional information: i am trying the process as part of initial data preparation for training a convolutional network.
- have i written custom code: yes - os platform and distribution: ubuntu 16.04 - tensorflow installed from: binary - tensorflow version: python version: 3.6.6 cuda/cudnn version: 10.0
the `input_shape` should be set to the shape passed to `compute_output_shape`.
keras.model.fit passes sparsetensors as is.
#include #include #include <iostream> int main(int argc, char
i also see the same error when creating a keras model from scratch.
"required by {ip address} ikev2 profile."
phofcode - function phofcode : phofcode function phofcode : - error of response display on network: i try to send directly the image buffer, but this error was displayed: i try to send directly the image binary string, but this error was displayed:
phofcode model + loss python from __future__ import absolute_import from __future__ import division from __future__ import print_function import numpy as np tensorflow as tf from tensorflow.keras model tensorflow.keras.layers conv2d, conv2dtranspose, dense tensorflow.keras.layers flatten, reshape, input adam def conv(n_filters, filter_width, strides=2, activation="relu", name=none): return conv2d(n_filters, filter_width, strides=strides, padding="same", activation=activation, name=name) def deconv(n_filters, filter_width, strides=2, activation="relu", name=none): return filter_width, strides=strides, padding="same", activation=activation, name=name) class """ custom layer.
the tflite model should perhaps be considerably smaller.
incorrect gradient computation when placeholder weights are involved.
large logs and files should be attached.
devices: streamexecutor device (0): <undefined>, <undefined> [ ] [ warning:tensorflow:from (from mpl) is deprecated and will be removed in a future version.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
no matter the tensorflow version, note that the training does not crash and the accuracy is properly computed when
python-dev is already newest version python-setuptools (39.0.1-2).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
but this transform leads much more computations that i cannot afford.
phofcode now loading the model: phofcode this raises the following error: `valueerror: '_restoredoptimizer ' is not a valid scope name`
cursorline in popup window makes the signcolumn show up
provide a reproducible test case that is the bare minimum necessary to generate the problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary):source tensorflow version (use command below):1.12 python version:2.7.5 bazel version (if compiling from source):no bazel.i use the build_all_linux.sh.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): `ubuntu 14.04.5 lts`, `debian `archlinux` - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.4.1`, 1.10.1`, `1.10.0`, python version: `3.6.0`, `3.4.3`, `3.7.0` cuda/cudnn version: `9.0/7.1.2`, `cpu only` gpu model and memory: titan 12gb the output is not deterministic and sometimes produces correct results so the code might have to be run multiple times.
apparently algo has a bug when deploying to a win10 client.
if including tracebacks, please include the full traceback.
python import tensorflow as tf from import autocast_variable var = tf.variable(0., dtype=tf.float32) var = with assert var.dtype == tf.float16 # assign should return an autocastvariable but returns tf.variable var_assign = var.assign(5.)
compile the model without raising any exceptions.
it should be possible to check whether tensorflow is executing eagerly before programs enable eager execution.
i converted my tflite model with f16 quantilization, then i create gpu delegate to run my model, the source code version of tensorflow is r1.14.0.
this happens when `number` is set (and possibly when `signcolumn` etc.
vim reports an error when a large buffer is passed to `job_start({cmd})` and `{cmd}` is outputting some text before the input is processed.
at method) at at at
here i just load in the batches without performing any action on them.
the second call to `test` not to be made because of the unhandled exception in the first.
storing coordinate form on disk is <2 gb.
also, this only happens for the first `import`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no ( phofurl + model.save + load_model) - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
phofcode the above code does not fail and produces empty output mask.
when using the same gs:// path with
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu - mobile device (e.g.
this is relevant for forward compatibility with *tensorflow 2.0.0-alpha0* which does not implement enable_eager_execution() but instead reports: attributeerror: 'module ' object has no attribute
now, after the last line isn 't read anymore if there 's no newline.
- running with non-zero l2 regularization works.
phofcode this outputs: phofcode so `importer2` is only freed after the python application finishes.
phofcode produces no output in the file `output`.
i will leave this branch untouched so you can test the bug report.
attempts to restore a frozen keras model including an "embedding" layer fail.
- os platform and distribution: ubuntu 18.04 / linux mint 19.1 - tensorflow installed from (source or binary): `pip install tensorflow-gpu` (example not using gpu, - tensorflow version (use command below): 1.14.0` python version: `3.7.3`
when filing bug, set verbosity 10 (on linux, `export autograph_verbosity=10`) and attach full output.
this could mean that the variable was uninitialized.
seamless connectivity with only some hit on latency
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): tensorflow==2.0.0b0 python version: 3.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: tpu: v2.8, tf nightly.
in between cursor holds, i am simply moving up and down document with `j` and `k`: phofurl as you can see "active" window changes to quickfix window and doesn 't return where cursor is.
if including tracebacks, please include the full traceback.
cause: (unicode error) 'utf-8' codec can't decode byte 0x93 in position 1: invalid start byte (tmp9j9bce4r.py, line 6) temporary file is created by shift jis (japanese windows default) character code.
however, we found that for certain operations that required multiple inputs (i.e.
the log function is fundamental and used all the time, it should be available in `tf.log()` (and also in `tf.math.log()`.
i believe ideally vim would be able to parse urxvt's `rgba:` response (ignoring alpha should be reasonable?
when debugging with i can see why the ops are not optimized: `i skipping readvariableop node because it must be preserved` i can 't figure out why those items must be preserved.
import tensorflow as tf def load_image(x): image = tf.io.read_file(x) image = tf.io.decode_png(image) image = tf.image.resize(image, size=( 128, 128), return image with tf.device('/cpu:0'): image_2 = load_image('test.jpg') ``
i haven 't proposed a fix because it requires a bit more close study of the wrapping logic
i am using golang enviroment to load tensorflow model and run.
import tensorflow as tf from absl import flags './log', 'log directory')
import tensorflow as tf import as optimizers import tensorflow.keras.losses as losses from import mnist (train_x, train_y), _ = mnist.load_data() model = sequential() model.add(conv2d(32, kernel_size=(3, 3), activation= 'relu ', input_shape=(28,28,1), model.add(conv2d(64, (3, 3), activation= 'relu ', 2))) model.add(flatten()) model.add(dense(128, activation= 'relu ', model.add(dense(10, model.fit(train_x, train_y, batch_size=64, epochs=2, verbose=1)
`attributeerror: 'tpucontext' object has no attribute if passing a user-provided instance to the `tpuestimator`.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
we run data through our model pipeline compute loss, gathering gradients by using either gradienttape or keras optimizers.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i would expect only as many iterations as specified by `steps_per_epoch` in each iteration.
popup window should have sign placed
the error does not occur in jupyter or ipython, and it does not occur when i revert back to tf 1.12.0.
this happens in gvim and vim on windows 10.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): master python version: 3.6.5 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory:
- have i written custom code: no - os platform and distribution: n/a - mobile device: n/a - tensorflow installed from: n/a tensorflow version : master python version: n/a bazel version: gcc/compiler version: cuda/cudnn version: gpu model and memory:
calling model.predict() includes calls to various non-prediction related things, in order of decreasing severity: * reset_metrics * get_progbar * standardize_user_data * in our project, this results in a 2x prediction speed regression: 35 ms per call for community keras 70 ms per call for tensorflow keras a snakeviz flamegraph for prediction of our network in tensorflow keras, showing unnecessary overhead: tensorflow keras phofimage
however, the non-sequential repeated concatenation in the loop leaves a graph disconnected error.
it should not fail to create a concat primitive descriptor
large logs and files should be attached.
thank you in advance, ref) phofurl
custom ops built with gcc5 should continue working with tf built with gcc4.
attempting to use progbar as a callback to model.fit() fails.
... steps to reproduce the behavior: 1. set a securityscheme like in the standard example 2. click on authorize 3. the description is not visible and this text is always displayed: scopes are used to grant an application different levels of access to data on behalf of the end user.
even 1,000 steps_per_epoch causes this issue though.
that doesn't work at all; nothing is returned (supposedly the `&optionname` syntax isn't supported there (yet; i think it should)).
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no, i was trying the - os platform and distribution (e.g., linux ubuntu 16.04): macos high sierra 10.13.5 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.13.1 python version: 3.6.2 numpy version: 1.16.3 when i downgrade numpy from 1.16.3 to 1.16.1 (whose default value was still allow_pickle=true), i could finish the example successfully.
phofcode phofcode phofcode phofcode the ragged tensor documentation hints that this might be intended or necessary, is it?
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os - tensorflow installed from (source or binary): binary with pip - tensorflow version (use command below): 2.0 alpha0 python version: 3.6 typeerror traceback (most recent call last) <timed exec> in <module> in use_keras) 587 #exporters=exporter) 588 --> 589 train_spec, eval_spec) 590 591 def use_keras): in train_spec, eval_spec) 471 '(with task id 0).
execute result and all possible results should be clearly separated.
when i split window, i can scrolling only right window by mi finger on touch screen.
in phofhyperlink hann window is calculated as blow: `(*window)[i] = 0.5 - 0.5 * cos((2 * pi * i) / window_length);`
tflite_convert --output_file=test.lite --inference_type=float --mean_values=128 --std_dev_values=128
python -c "import tensorflow as tf; tf.version.version)" nvcc --version phofcode
the error message get is: phofcode there seems be an error with dimensions input data but `y_true` should be shape `[64,4]`, `y_pred`should `[64,4,12]` before applying ` y_pred = math_ops.argmax(y_pred, axis=-1)` `[64,4]` after.
large logs and files should be attached.
windows 10 1806] - redis-server version [e.g.
it doesn't work with phofurl repro attached phofhyperlink , just call `run.sh`.
i want to be able to see the results of each step of the variable at runtime through
info: created tensorflow lite delegate for gpu.
if i call `model(model.input)` before custom loss function is called, it surprisingly works.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
using tf.saved_model to save session does not save the state of the random number generator associated with the graph that the session executes.
4. open the document with gvim.
8.1.950] (or paste the result of `vim --version`.)
the 'cmdheight' option effectively has tab-local values (the `:help` says _global_, but then clarifies _the value of this option is stored with the tab page, so that each tab page can have a different value._) as such, it should be possible to query the option's current value with `:echo gettabvar(2, '&cmdheight')`.
both options `popup_filter_menu()` and `#{close: 'button '}` should work together.
when i try to use `model.fit_generator` or `model.fit_on_batch`, i get the following error: phofcode here 's the details about the model: it 's a custom classifier which uses xception phofhyperlink as the feature detector and a custom feedforward network for a classifier.
large logs and files should be attached.
phofcode the issue is at line 2079 of `keras_support.py` where should be used instead.
1410 % (optimizer,)) -> 1411 session=session) 1412 # record this checkpoint so it 's visible 1413 in save(self, file_prefix, checkpoint_number, session) 976 save_path, new_feed_additions = 977 --> 978 979 if new_feed_additions: 980 file_prefix, object_graph_tensor) 916 (named_saveable_objects, graph_proto, 917 feed_additions) = self._gather_saveables( --> 918 919 if != graph_proto 920 # when executing eagerly, we need to re-create saveableobjects each time _gather_saveables(self, object_graph_tensor) 882 """wraps _serialize_object_graph to include the object graph proto."""
if someone knows how also, when i do the markup for a link on one of my other pages, it always adds target=_blank which i do not want because i am linking around the same site, how can get rid of that?
when executing steps below however, commands are found but terminal behaves in a very buggy manner constantly overwriting previous text with new text etc.
i want to implement a keras custom layer without any input, just trainable weights.
large logs and files should be attached.
that is when monitoring the gpu usage only one gpu shows substantial dedicated gpu memory usage and gpu utility.
... steps to reproduce the behavior: use definitions added tho this issue
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the preview popup is cleared in the left-hand window.
if user error or a new limitation/feature is causing the problem, it should be warned about in update notes/quick start.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 14.04.5 lts - mobile device (e.g.
and one of the nodes processes info are phofcode ps-ef-info.txt phofhyperlink and i print `1965` process python call stack it shows thread file line 884, in _bootstrap self._bootstrap_inner() file line 916, in _bootstrap_inner self.run() file line 864, run
=> {"msg": "an unhandled exception occurred while running the lookup plugin 'file '.
please see attached jupyter notebook phofhyperlink .
the cpu memory usage also keeps growing during this "warmup" phase.
large logs and files should be attached.
- os platform and distribution: google colab - tensorflow installed from: binary via `pip install - tensorflow version: 2.0.0-beta0 python version: 3.6.7 cuda/cudnn version: n/a (google colab as of gpu model and memory: n/a (google colab as of my best guess is that changes have been made to the pretrained mobilenetv2 or to the adam optimizer, otherwise the drastic loss in accuracy is hard to explin.
when using `tf.while_loop` (v1), we are able to call gradients in the body of the while loop.
it seems like a bug, but perhaps it 's just a bit too complicated for me.
memory usage of tf 2.0 should be same or similar to other libraries, not double.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.13.6, tensorflow r1.12 - mobile device (e.g.
we excluded warmup time from the profile, and average over number of samples.
to ensure that the images and the labels (labels are images, too) are transformed in the same way (e.g.
see your tutorial on audio recognition.
the contents not to be clipped.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the training fails with a very vague exception, which only says that the last op(tf.reduce_mean() in my case) has been marked as not fetchable, and no other useful information provided.
when a line start with `.sh`, and whatever characters are after, the line is considered as a blank line.
the only way to see the correct keys again is to restart rdm
same behavior if you don't set any scopes.
- have i written custom code: yes - os platform and distribution: ubuntu 16.04 - tensorflow installed: binary - tensorflow version: python version: 3.6.6 phofcode @qlzh727
if including tracebacks, please include the full traceback.
mpi mpi specify optimization flags use during compilation when bazel option "--config=opt" specified [default -march=native]: would like interactively configure ./workspace android builds?
before installation of tf-serving-api: phofcode after installation of tf-serving-api: phofcode
i have already set the (or a paraphrase thereof, i don 't remember the exact flag) to true.
the docker `latest` family should contain tensorflow version 1.13.*.
- os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
according to pr for label_image #27464, i modified the label_image code to support the gpu delegate and removed the code from 205 phofhyperlink to 251 phofhyperlink lines .
2. do custom layers use the cpu by default?
phofurl phofcode the output from the above code is: features 0: f: (8, 8, 11) <dtype: 'float32 '> labels 0: a: (8,) <dtype: 'int32 '> labels 1: b: (8,) <dtype: 'int32 '> prediction: <class 'list '> pred out 0: (8, 8, 3) float32 pred out 1: (8, 8, 4) float32 valueerror traceback (most recent call last) in <module>() 49 50 loss={"a": ---> 51 "b": 52 53 model.fit(ds.batch(8)) 3 frames in input_dict, expected_values) 589 raise valueerror( 'unknown entries in {} dictionary: {}.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - mobile device (e.g.
however, iterating infinitely over the dataset in autograph is weird and shouldn 't happen.
phofcode output: > 10/10 - 0s 6ms/sample - loss: omitting `validation_data` stops the output from appearing link to an example notebook: phofurl
i execute the code twice to account for `tf.function`s tracing which has to be one once for `_df`.
error occurred when try to run the colab notebook shown in tf 2.0 alpha: distributed training in tensorflow phofhyperlink for tpustrategy: phofcode i had enabled colab runtime to `tpu`, and even checked there indeed is a tpu available: phofcode
import tensorflow as tf img = dtype=tf.float32, shape=(1, 64, 64, 3)) var = dtype=tf.float32, shape=(1, 64, 64, 3)) val = img + var out = tf.identity(val, name="out") with tf.session() as sess: converter [img], [out]) tflite_model converter.convert()
i have noticed this with vim 8.1 in debian unstable and reproduced the issue with current git master.
file line 1322, _do_call return fn(*args) file line 1307, _run_fn options, feed_dict, fetch_list, target_list, run_metadata) file line 1409, _call_tf_sessionrun run_metadata) indices[40] = -1 is not [0, 517) [[node: = gatherv2[taxis=dt_int32, tindices=dt_int32, tparams=dt_float, phofhyperlink ]] [node: = tensor_type=dt_float, phofhyperlink ]] during handling above exception, another exception occurred: traceback (most recent call last): file line 620, trainer) 94, launch_train_with_config 343, train_with_defaults steps_per_epoch, starting_epoch, max_epoch) 315, train starting_epoch, max_epoch) 176, wrapper return func(*args,
should work properly with input of type dict.
- os platform and distribution (e.g., linux ubuntu 16.04): win10 - tensorflow installed from (source or binary): pip install --ignore-installed --upgrade tensorflow-gpu - python version: python 3.5.3 - cuda/cudnn version: cuda 9.0.176, gpu model and memory: nvidia geforce gtx 1050, 4gb
model loading is fine and runs perfectly for 15-20 min and after that, it throws phofcode error and exits.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): (this is the tf 2.0-preview) python version: 3.6.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: i also checked the changes listed in the api names rfc, and i noticed a few discrepancies (i can file a separate issue if necessary): phofcode and `tf.spectral` is called `tf.signal`
a .tflite file is converted.
phofcode after training, i am freezing the model using phofcode after is being generated, i 'd like to convert model to a model format that is compatible with iphones.
from ._conv import register_converters as _register_converters traceback (most recent call last): file line 463, in <module> tf.app.run(main=main, argv=[sys.argv[0]] + unparsed) file line 1 25, in run _sys.exit(main(argv)) file line 107, in main model_settings, flags.summaries_dir) typeerror: __init__() missing 1 required positional argument: 'summaries_dir ' ' ' ' where i was wrong and what i needs to do
i added monitor= 'val_loss ', verbose=1)" as part of tf.keras.model.fit callbacks, but it never gets triggered.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: android emulator - tensorflow installed from (source or binary): binary tensorflow version (use command below): tf-nightly python version:3.7.1 bazel version (if compiling from source):na gcc/compiler version (if compiling from source):na cuda/cudnn version:na gpu model and memory:na
`[on]` is displayed in the status line, and the cursor is drawn right afterward.
the initializable iterator is saved and restored properly, behaving the same with the one shot iterator.
device assignments active during creation: tf.device(/gpu:1): warning:tensorflow:tried to colocate (defined at test.py:102) having device '/device:gpu:0 ' (defined at test.py:93) which had an incompatible '/device:gpu:1 '.
any distributed training program should be okay.
im happy to debug further, and am capable of compiling vim.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - modified from the stock example - phofurl - working example is attached to this bug report os platform and distribution linux ubuntu 16.04 mobile device (e.g.
previously i haven 't seen these errors with older versions of keras/tensorflow.
t additional grpc error information: received from [64].
the problem is in the function: assign_dyn_vals() once i instantiate a variable and i try to assign to this variable some computations.
to solve this problem, there 's a need to either pass undelying optimizer variables() method (together with passing lossscalemanager variables too) or fill slot- and non-slot variables some other way.
save this script as and run on each node with fully qualified domain names `master` and `slave1`: phofcode or set different index of worker in tf_config in on different nodes.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.13.1 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): vs2015 cuda/cudnn version: 10.0 / 7.5 gpu model and memory: rtx 2060 6gb include any logs or source code that would be helpful to diagnose the problem.
i have a simple test to serialized and deserialize a model which has no compiled loss, just one added with add_loss.
the benchmark script finishes normally and prints performance numbers, as only a single-day of commits before on with (we run into this with our in-house untested nightly build at
according to the documentation, the bookmark phofcode is expected to point to
this is a continuation of phofurl
it will fail to load.
... try the below snippet on phofurl phofcode i took this example from the swagger documentation: describing request body form data phofhyperlink .
a super simple work-around is to call but the real issue is the unexpected behavior: "why would i ever think that would return duplicates of the same variable ?!"
t [[{{node e aborting ringreduce with internal: [_derived_]inconsistent [64].
when `completeopt` contains `popup` and the following are set: `set the preview popup can leave drawing artefacts on a non-current window.
hello, i have trained my custom dataset using the github project link phofhyperlink to create a model that detects eye region with landmarks.
an error occurs when training an lstm with a custom loss function, using `sample_weight` and `batch_size > 1`.
root@:~# python -m virtualenv --python=`which python2` env && source env/bin/activate && python -m pip install -u pip && python -m pip install -r requirements.txt running virtualenv with interpreter /usr/bin/python2 new python executable in /root/env/bin/python2 also creating executable in /root/env/bin/python please make sure you remove any previous custom paths from your /root/.pydistutils.cfg file.
if including tracebacks, please include the full traceback.
start container: `docker run --rm -it bash` start python3 in the container without site initialization: `python3 -s` add package locations to the sys.path and import tensorflow: phofcode
successfully saving a savedmodel with the layer.
calling `tf.linalg.solve(a, b)` with `b.shape == (n,)` causes a segfault.
in the output of `:cno`, `:ino`, `:nno`, ... all mappings using the control or alt modifier are printed twice.
we 've created a cae.pb file and it seems to run properly, and the time that the session.run() call is made in rapid succession (using the same session each time), the time it takes decreases (up to a certain point).
this is a mild annoyance when am running models with soft-warp enabled on the command line.
1. image phofimage 2. image phofimage 3. image phofimage
we are using tensorflow java api to loaded a tensorflow model and a program exception is thrown.
if including tracebacks, please include the full traceback.
i can work around this issue by calling before creating the model, but oddly this does not work if i call _after_ creating and compiling the model, and just before calling `fit()` method.
call --arch arm --api 21 --stl=libc++ --install-dir android-toolchain` 2. build `libtensorflowlite.so`, i added this to the build file: phofcode and then called phofcode 3. clone and compile the flatbuffer repo.
to make sure this not specific to my personal machine, i used google colab with gpu enabled under notebook settings.
after connecting to the redis cache server.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04.1 lts - tensorflow installed from (source or binary): 1.11 - tensorflow version (use command below): pip python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: cuda release 9.0, v9.0.176 gpu model and memory: titan xp / 12gb include any logs or source code that would be helpful to diagnose the problem.
large logs and files should be attached.
the vm still has lots of disk memory so no problem on this part.
tensorflow version (use command below): 1.14.0 python version: 3.6.5 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with: 1. tf 1.0: `python -c "import tensorflow as tf; print(tf.git_version, tf.version)"` 2. tf 2.0: `python -c "import tensorflow as tf; tf.version.version)"` include any logs or source code that would be helpful to diagnose the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 x64 - mobile device (e.g.
this happens when typing '* ' over identifiers that are 24 characters or longer (e.g.
train on 50 steps, validate on 157 steps epoch 1/2 50/50 4s 73ms/step loss: 1.8029 accuracy: 0.5256 val_loss: 0.5635 val_accuracy: 0.8345 epoch 2/2 50/50 3s 58ms/step loss: 0.6426 accuracy: 0.7937 val_loss: 0.4020 val_accuracy: 0.8787 history dict: { 'loss ': 'accuracy ': 'val_loss ': 'val_accuracy ': [0.8345, 0.8787]} w sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
our text recognizer is based on crnn model and ctc.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
in order to generate the memory occupancy, the `memory_profiler` package can be used.
the training is expected to work correctly even with distributed scope turned off.
t [[{{node during handling of the above exception, another exception occurred: traceback (most recent call last): file line 27, <module> np.array([[1.]])})
input shape = [?, 128, 128, 8] output = depthwise_conv2d(input, filters=8, kernel_size=3, strides=(2, 2), dilation_rate=(3, 3), padding= 'same ') output shape = [?, 65, 65, 8]
possible clues: * this appear be sensitive the size of the file in question.
the same custom operators rebuilt for tf 1.12 are not found by tf.load_op_library, although the paths of the dll given to tf.load_op_library are correct.
i used lot of low-level operations like tf.ones, tf.concat, tf.tile, tf.reduce_sum and matrix slicing.
can't convert the dataset/iterator to its graph representation: i'm forced to loop in eager mode, but i want to have a graph representation of the loop too (since i need to export a savedmodel that contains the loop itself).
the output sequence should match the input, even when the batch dimensions are greater than 1. why do batch dimensions need to be 1?
why does the numpy array/float "remember its origin"?
variation 1: vim gets stuck upon hitting enter: phofcode variation 2: abrt instead of segv:
large logs and files should be attached.
should not throw the above error.
the model is a modified version of the deeplab gpu delegate model provided by google.
lear_rate = 0.0 image phofimage learn_rate = 0.001 image phofimage learn_rate = 0.1 image phofimage
random graph operations added during mapping will be reproducible.
developers needn't care about which threads are used for calling these apis.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): compiled form source with flags phofcode tensorflow version (use command below): 1.11.0 python version: python 3.6.6 bazel version (if compiling from source): 0.18.0 gcc/compiler version (if compiling from source): 4.8.5 cuda/cudnn version: 9.2, 7 gpu model and memory: tesla v100-pcie 16gb and tesla v100-pcie 16gb
it 's failing for while loops without `lambda` wrapping, giving a very unhelpful error message.
... steps to reproduce the behavior: 1. try to expand
1. write this in `/tmp/vimrc`: if 1 let list = ['not from heredoc'] else let list =<< trim end from heredoc end endif echom list 2. start vim like this: $ vim -nu none -s /tmp/vimrc vim outputs `['from heredoc']`.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): pip install tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: cuda 10.0, gpu model and memory: geforce gtx 1070 8gb
but please understand that svelte is run by unpaid volunteers in their free time, and issues that follow these instructions will get fixed faster.
instead of pressing the up key while on command-line, vim simply wrote `<plug>(up)`.
should work as before and without any error messages
expected install to work smoothly.
besides, if you replace `:bo sp` with `:sp`: - in context of command-line window, previous window number changes from `2` to `1` - issue is not triggered so, if autocmd succeeds in maximizing window which is entered after leaving command-line while previous top one, i would expect it to also succeed when previous bottom one.
and log.txt is not created.
when vim is invoked with the flag `-d`, `-o` or `-o`, during the startup, none of these events seem to be triggered: - `bufleave` - `bufwinleave` - `winleave` this may give unexpected results.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a tensorflow installed from (source or binary): pip package `tensorflow-gpu` tensorflow version (use command below): 1.8.0 python version: 3.6.3 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: v9.0.176 gpu model and memory: geforce gtx 1060 6gb
- vim version - os: macos 10.14 - terminal: terminal.app
no node-device colocations were creation.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): windows and linux - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12.0 python version: python 3.6.5 cuda/cudnn version: v9.0 / 7.2.1 gpu model and memory: gtx 1080 and also on gtx 1080 ti
provide a reproducible test case that is the bare minimum necessary to generate the problem.
python raises when doing a first read (no writes before it) on a nonexistent `tf.gfile.gfile` in `w+` mode.
the request should be made with http, not https
i am doing data augmentations using dataset map method.
2 1 phofimage and gvim81.exe win10 and win 10 - terminal: use gui i could reproduce 2 sometimes with blow min .vimrc: phofcode please fix this issue which makes ppl out of maid.
but when i run `interpreter = i get the following runtime error: numdimensions(input0) < 4 was not true.node number 14 (pack) failed to prepare.`
it only appears to happen on windows.
it works for a batch size of 8 and less.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0.0 python version: 3.7 this bug is also reported on upstream keras phofurl here is a detailed analysis on why this is happening phofurl full exception file "test.py", line 27, in <module> # exception: axes don 't match array !!!
if including tracebacks, please include the full traceback.
if endpoints didn 't have the same prefix (`/v1/`), spring server exportation would generate two controllers.
i reinstalled tensorflow and the issue persists.
i0603 saver.py:1280] restoring parameters from i0603 session_manager.py:500] running local_init_op.
run this shell command: $ printf 'a b c' >/tmp/file && vim -nu none --cmd 'syntax on' +'sil vim /./ % | copen' /tmp/file the `quickfixline` highlighting starts after the text `a`.
if including tracebacks, please include the full traceback.
n/a, this is a failure of libtensorflow to return an error for system configuration issue.
2. name should be provided as "schemes".
it works well with keras `model.fit()`.
gif demonstrating issue phofhyperlink .
provide a reproducible test case that is the bare minimum necessary to generate the problem.
examples for the `test` parameter should be shown somewhere in the ui
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): arch linux - tensorflow installed from (source or binary): binary - tensorflow version (use command below): python version: 3.6.8 cuda/cudnn version: cuda version gpu model and memory: nvidia geforce gtx 1080 ti
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
steps to reproduce the behavior: 1. list ten users in config.cfg 2. deploy algo 3. add one user to config.cfg & run algo update-users with success 4. add another user to config.cfg & run algo update-users with error
... steps to reproduce the behavior: 1. create an openapi spec with some errors in it.
... steps to reproduce the behavior: 1. open swagger-editor site 2. paste in provided openapi spec from this ticket 3. click to unfold the `get /pet` endpoint in live preview 4. see error in editor and `wrong` example in live preview.
before adding the wrapper, the program is completely running normally.
should run normally, giving the `[2, 10, 100, 32, 32, 3]` tensor.
upsampling2d outputs size times the size of the new input
see this colab notebook phofhyperlink
provide a reproducible test case that is the bare minimum necessary to generate the problem.
1. go to phofurl 2. click on connection name and write something with spaces 3. click on save button 4. see error in a console
to confirm that xla is active, pass (as a proper command-line flag, not via tf_xla_flags) or set the envvar w0624 deprecation.py:323] from (from is deprecated and will be removed in a future version.
recently we tried to upgrade the tensorflow to tf 1.9 and find the recognizer inference time increases a lot.
now, issue here is that it doesn't matter much how many parallel sessions i run in multiple threads, gpu utilization still low,
use `if t not none:` instead of `if t:` to test if tensor defined, and use tensorflow ops such as tf.cond to execute subgraphs conditioned on the value of tensor.
1. write some custom graphql schema 2. start the project 3. use the custom query/mutation 4. see error
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: im using raspberry pi for android things implementation - tensorflow installed from (source or binary): tensorflow version (use command below): python version: bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: `imageutils:native library not found,native rgb -> yuv conversion may be unavailable.
warning:tensorflow:no new checkpoint ready evaluation.
large logs and files should be attached.
the sum should be 3.
hi, i am using this script resnet_cifar_main.py in phofurl to test parameter distributed strategy.
if applicable, copy/paste the text or add screenshots to help explain your problem.
i am trying to benchmark a custom tf lite model on android and ios devices with the gpu delegate option.
info:tensorflow:error reported to coordinator: traceback (most recent call last): file line 297, in stop_on_exception yield file line 795, in run self.main_result =
here is my failing part: phofcode
but it did not work.
the result should the the sum of the elements on the diagonal (compare with `np.einsum('ii', matrix)`)
attached nhw3 model (and script to generate pb and tflite with optimization options, which do not have any effect), this is basically nhw3 input, single tf.layers.conv2d() constant-initialized layer and reduce_sum() for output.
large logs and files should be attached.
expected behavior is that multiple tf.keras models can be instantiated on multiple threads without conflict.
the -h, -help, -helpshort, and -helpfull arguments may cause exceptions to be triggered as opposed to displaying flags, default values, and programmer-provided information.
one is expecting a batch_dims argument for gather_v2 while other believes this is an invalid argument.
the higher the is set to the higher chance that `iterations` will start at `0` instead of at `1`.
... steps to reproduce the behavior: 1. enter a description with a linebreak.
run command: python --data_dir=/data --model_dir=/data --resnet_version=2 --clean --save_summary_steps=10 --train_epochs=1820 --batch_size=512 --num_gpus=2 ``
1.13 and 1.14 behavior phofcode phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow): phofcode phofcode - os platform and distribution (e.g., linux ubuntu 16.04): win7 cpu - mobile device (e.g.
another side effect of current code is that is called from `model.train_on_batch` so in fit_generator path this is now being called on every iteration.
deprecation: python 2.7 will reach the end of its life on january 1st, 2020. please upgrade your python as python 2.7 won't be maintained after that date.
1. put `set formatoptions=rql` your vimrc file.
- os & version: macos app store builds - redis-server version any
provide a reproducible test case that is the bare minimum necessary to generate the problem.
6. see that the wysiwyg editor remains, when it should just be an input
i 'm using tensorflow hub to restore a model from a savedmodel, i do expect the restore to work even if it executed inside a distribution strategy scope, but instead it raises an exception.
devices: i streamexecutor device (0): <undefined>, <undefined> i xla service executing computations on platform cuda.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04.2 lts - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
but the code as simple as above using high level function thrown 3 warnings?
2. for each sub square, split it into 6 * (2 * 12) horizontal & 6 * (12 * 2) vertical rectangles.
detailed steps to reproduce the behavior: 1. open any file using vim 2.
* did some very elementary benchmarking of both systems: - calculating number of primes, spawning a number of threads using as expected, performance between p3.8 and p3.16 appears to be identical when n_threads < 32, 3.16 performs better when n_threads = 128.
i input the below string into my vimrc py3 print( 'hello world \')" when i opened vim and delete "py3 import os;import it should work but it does not work sometimes , it show below error fatal python error: initfsencoding: unable to load the file system codec modulenotfounderror: no module named 'encodings '
steps to reproduce the behavior: 1. clone current repo.
otherwise, the advantages of eager mode, like the ability to use native control flow, are lost.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary):binary(pip) tensorflow version (use command below):1.13.1 python version:3.6 bazel version (if compiling from source):n/a gcc/compiler version (if compiling from source):n/a cuda/cudnn gpu model and memory:nvidia geforce rtx2080ti 11gb include any logs or source code that would be helpful to diagnose the problem.
error should not be raised, code should work.
this causes an exception: phofcode it is a custom model, see attachment.
nothing is selected when on the first ` '` and on the `b`.
when i open a zset key twice at the same time, it shows me two blank windows.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
add any other context about the problem here.
- os: macos 10.14 - terminal: apple terminal/macvim gui phofcode original issue: phofurl
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
2. click "try it out".
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution: 19.04 - tensorflow installed from (source or binary): pip install tensorflow==2.0.0-alpha0 - tensorflow version (use command below): 2.0.0-alpha python version: 3.7.3 e0504 ultratb.py:149] internal python error in the inspect module.
the gradients in are not none, and seem ok. that is, a_values_grad and b_grad are not none (also none in _gradient_function) `(none, a_values_grad, none, b_grad)` but output of `imperative_grad` has none for weights sparsetensor.
), i set random seeds at same value.
i creating new thread pool with default inter op setting: 2. tune using for best performance.
since tensorflow does not support value assignment/update to large matrices , i have to use a lot of tf.stack()/tf.concat() functions.
if you click on the component a/b/c tabs, you'll see that when navigating from comonent c to components a or b, the tab selection does not always update, even though the tab content does.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i've been using vim for years and it felt unexpected when i noticed it a few days ago, so i don't know if it's always been like that or if something's changed.
if including tracebacks, please include the full traceback.
if `repeat()` is being invoked, repeat the dataset.
- os platform and distribution: linux 18.04 - tensorflow version 2.0-alpha - python 3.6.8
setting `data_format="nhwc"` or `use_cudnn_on_gpu=false` procudes same error.
i implemented the same model (2-layer fully-connected neural network + batch norm to reduce initialization effects) using 1. low level apis (graphs and sessions), and 2. high level apis (estimators) i tested the two implementation and tested it on the mnist dataset.
detailed steps to reproduce the behavior: 1. run `cmd.exe` and input `gvim --clean --cmd ":set dir=$temp//" 2. modify some lines and use windows task manager to kill gvim.exe 3. then back to step 1, and choose `recover` to restore last modified state.
when called by the java program, the code will crash at this line phofhyperlink
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): windows10 - mobile device (e.g.
i opened a file containing some zlg tex, and vim crashed.
in the example code below, `model.summary()` outputs `[(none, (2,))]` for the output shape.
i know it has been used for the mobilenet variants before.
tensorboard cannot be used properly, since the new training will write its values to the same steps as the first training, instead of appending them.
concretely, when `a` is a tf.variable phofhyperlink and `a_sqrt` is run together with `a_scaled`, the output `a_sqrt_1` is `a_sqrt_2
void model_path, unsigned inter, unsigned intra){ sess_; sess_options_; tensorflow::graphdef graph_def; auto default_env = model_path.string(), &graph_def); sess_->close(); } // this situation the inference time is 90ms.
print( '###predict images### ') i 0 test_images np.empty((0, 299, 299, 3)) test_labels np.empty(0) for x, y in test_generator: i i + 1 if i > len(test_generator): break test_images x), axis=0) test_labels y), axis=0) error_count 0 predictions for in predicted_label if not predicted_label == test_labels[i]: error_count error_count + 1 print("label for image %d: %d" % (i, test_labels[i])) print("predict label for image %d: %d" % (i, predicted_label)) decimals=3)) print("total error count is %d, total predict count is %d" % (error_count, test_images.shape[0]))
large logs and files should be attached.
a trained win10 runs fine win10, but fails to load rpi.
phofurl provide a reproducible test case that is the bare minimum necessary to generate the problem.
interpret primitive type as tensor at the right time.
why is it using `output_node_names` to find a file?
specifically, the line `while cond(*loop_vars):` ( here phofhyperlink as of this date) is only valid when `loop_vars` is a list of `eagertensor` instances, thus enabling the `while` check on the `numpy` attribute of results.
actions with `y` or `>`, when used with forward motions (like `e`, `)`, or `}`) don't move the cursor, but backward motions (like `b`, `(`, or `{`) do.
it completes training, but then it fails to load with phofcode no, that 's wrong.
type in input, error message shows up
provide a reproducible test case that is the bare minimum necessary to generate the problem.
when using or on a layer with a `build` function, the `build(input_shape=...)` argument is always set to none.
1. create a model with a boolean value 2. attempt to create an entry 3. see error in strapi terminal console * note * : it appears this does not affect data sent over rest or graphql as you can see in the example provided from postman
large logs and files should be attached.
* fix problem in `tpuestimator` by always using instances of `hparams` and setting `context` before, so we add `context` by calling `add_hparam`, which does not use type casting, instead `set_hparam`.
gives a phofcode when called on a foldl operation.
tf.manip.roll executes on cpu and is very slow
i 'm able to reproduce this consistently, so let me know if there 's more information i can post that would be of use
this should happen even when using profilecontext.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
n/a - vim version phofcode - os: ubuntu 18.04.2 lts - terminal: xterm(330) n/a any guidance on how to investigate this further would be greatly appreciated.
now for production requirements, i need to build a standalone application or library which can perform the inference in c++.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.11.0 python version: 3.6.5 cuda/cudnn version: n/a gpu model and memory: n/a include any logs or source code that would be helpful to diagnose the problem.
the following code snippet reproduces this behavior.
- os platform and distribution (e.g., linux ubuntu 16.04): colaboratory (ubuntu 18.04.2 lts) - mobile device (e.g.
does not do weight decay when using `mirroredstrategy` along with `tf.get_variable`
if including tracebacks, please include the full traceback.
it should be able to convert the model to tflite format.
1. able to connect on port `22` steps to reproduce
as mentioned, yesterday the models were running perfectly fine and i expected the same to happen today as i have made no changes to my software/hardware in between.
i 'm using the example of object detection given by phofurl so the tensorflow model loads perfect but when i try to optimize it a segmentation fault raise.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): win 10 pro - mobile device (e.g.
only once `regtilde()` is called).
3. using the mouse select the text to the end of line.
however, after the `session->run` is performed, i try to make an flat of the output: when i try to do the flat, system execution is interrupted, error are:` check failed: dtype() == expected_dtype (9 vs. 1).` to understand result of output, i tried print content of my output, but i cannot.
`tf.nn.conv2d_transpose` with the same inputs produces different outputs on different calls on gpu.
happens in both vim and gvim, but here 's the vim version: vim - vi improved 8.1 (2018 may 17, compiled jun 13 2018 ms-windows 64-bit console version included patches: 1-55 compiled by appveyor@appvyr-win huge version without gui.
download the data as follows: cd ~ mkdir datasets cd ~/datasets mkdir jena_climate cd jena_climate wget phofurl unzip run jupyter notebook and load the notebook that is attached to this issue in a python 3.7 environment with tensorflow 2.0.0. in notebook replace /home/daniel with /home/(your username).
(can not convert a perdevice into a tensor or operation.)
i input my home wifi ssid to exclude from on-demand when building my config.
the keras/engine/training.py `_make_train_function` adds optimizer update ops / aka, backward prop phase, and fills out model 's graph.
using version 1.14 of tensorflow-gpu, it worked, however none of the submodules were available; i checked using `dir(keras)` and the module was completely empty.
i would expect custom loss functions to work irrespective of batch size and sample weights.
ideally a stacked tensor returned (i was adapting code i developed interactively in eager execution mode, where it worked, in a jupyter notebook for addition to a keras based model), or an error that the argument cannot be a tensor (this is the tf1 behavior): phofcode
if using `map_and_batch`, one get changing results like: phofcode there should be always only one and non-duplicate non-zeros per-line.
phofcode for the models generated without input shape, here is result: phofcode and here is snapshot for models generated with input shape.
because the file a is write protected, `rm` prompts `rm: remove write-protected regular empty file 'a'?` in the command line, while it didn't trigger the `on_stdout` with this prompt.
- have i written custom code: no - os platform and distribution (e.g., linux ubuntu 16.04): windows server 2016 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.13.1 python version: 3.7.3 cuda/cudnn version: 10/7.1 gpu model and memory: tesla v100 16 gb
if i want to specify, for example, a reduction method of the loss function, i will need to explicitly create an instance of and pass it to `model.compile()` instead of passing the keyword.
the element shape of the `tensorarray` should be partially known, consistent with the behavior of an equivalent `tf.while_loop`.
everything is ok here when testing with ckpt files and pb frozen graph in prediction it gives 75 %, 80 % predictions : that a result on ibug dataset resulttaa phofimage i have used ml kit firebase sdk to integrate tflite model and i have followed instruction for how use custom model.
however on galaxy j5 (odler device), it crashes with: e/androidruntime: fatal exception: inference process: bazinac.aplikacenahouby, pid: 12367 internal error: failed to apply delegate: warning: op code #40 cannot be handled by this delegate.
if eager execution is disabled it works fine.
i 've previously installed algo w/ zero issues on this machine.
`y_true` and `y_pred` should both be of shape `[64,4]` so i quickly figured out where the problem comes from.
the error does not occur if `batch_size = 1`, or if `sample_weight = none`.
file folded in both windows c because `:badd` run in `a.txt` buffer: $ vim -nu none +'e /tmp/a.txt|setl fdm=marker|badd /tmp/a.txt|new|b a.txt' but this does not work as expected c i.e.
large logs and files should be attached.
run this shell command: vim -nu none +"call matchadd( 'conceal ', '(.
string graph_name = argv[1];// name.pb tsession *session; tstatus status = &session); tif (!status.ok()) t{ t tcout << "create session wrong: "<< status.tostring() << endl; t treturn -1; t} tgraphdef graph_def; t//read model: tif graph_name, &graph_def).ok()) t{ t tcout << "read model(pb) failed!"
when slow worker still stays in iteration 1, the other workers cannot go to iteration 3 or further.
phofcode my configuration: windows 10 home tensorflow 1.13.1 python 3.5 gtx 1060 mobile max-q it doesn 't happen every time i run my program.
a list of my eips for the region should show
if that's the case, perhaps this is more appropriately categorized as a feature request.
but when i ran the model after 8-10 min, tensorflow is throwing oom allocation error.
this is not an issue in this toy example, but `importer2` could have a reference to a large number of other objects that take considerable space in memory in reality.
if you want xla:cpu, either set that envvar, or use experimental_jit_scope enable xla:cpu.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i expect `model_to_dot()` to be available in or at least in `tf.keras.utils`.
i build the tensorflow1.4 with gpu and -c dbg version, but when run test code or some example, meet cuda memcpy failed, but if you build with -c opt and work normally and no issue found.
i just compiled the new master branch, and it was working before.
add a new field to it.
the output is correct so the `__call__` method is correctly called.
cells = x = none, 5)) layer = stateful=true) y = layer(x) ``
2. try figure out what exactly blocks vim in first place, make it try continue, even if support isn 't going work properly.
we took care make sure this not profiling glitch.
phofurl python mnasnet_main.py --model_name=mnasnet-a1 --use_bfloat16=true --use_keras=false --mode=train --train_batch_size=96 --num_gpus=1 --steps_per_eval=33363 (temporary version.the code does not really use bfloat16, use float16 instead.)
training a mobilenetv2 model , i see the loss is descending and becomes very small after a few epochs while the val_loss is not.
here 's the full code (i tried to add as little as i could): phofcode
with tf 2.0 preview, the converter is not able to convert a tf.keras model to tflite either through python or command line
densnet169 crashes with memory error even with datagenerator and a batchsize of 8.
2. deploy 3. stops at strongswan register p12 step due to error about mismatched cpus.
lstm) does not produce output as expected for "zero_output_for_mask" when the layer is created with "return_sequences=true".
a dataset has 101 examples, with batch-size=10), the calculation is incorrect because it will mistakenly give higher weight to the last batch 's loss.
second gradient with input is none
`swaggeruibundle` should be successfully imported (along with but the problem isn 't with that)
- os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
<script> let toggle = false; function _toggleme() { if (toggle = !toggle) { console.log(toggle); } else { console.log(toggle); } } </script> <button <span class="text {toggle ?
large logs and files should be attached.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a python version: python3.6.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: cuda9.0 gpu model and memory: nvidia geforce gtx 1080 ti 11264mib
all workers progress thru the benchmark and finish successfully.
large logs and files should be attached.
the simplest way to tackle this issue first seems include again pip package build custom operators on windows
traceback (most recent call last): file line 1741, in <module> main() file line 1735, in main globals = none, none, is_module) file line 1135, in run globals, locals) # execute the script file line 86, <module> gru4rec = model.gru4rec(sess, args) 72, __init__ self.build_model() 142, build_model output, state = multi_cell(inputs, tuple(self.state)) 233, __call__ return super(rnncell, self).__call__(inputs, state) 374, __call__ outputs = super(layer, self).__call__(inputs, *args,
make sure that your dataset can generate at least `steps` batches (in this case, 100 batches).
the repl: phofurl the local version: phofurl app.svelte phofcode componenta.svelte phofcode componentb.svelte phofcode componentc.svelte (just for a sanity check) phofcode when running the app in a repl, the output is: when running the app locally, the output is:
i would expect it to allocate no more than 50% memory.
if including tracebacks, please include the full traceback.
the binary that used the .so works without segfault whether or not the .so is compiled in debug mode or not.
tf.signal.stft does not work in eager execution mode, producing the error: traceback (most recent call last): file "<stdin>", line 1, in <module> file line 83, in stft signals, frame_length, frame_step, pad_end=pad_end) file line 120, in frame num_outer_dimensions = file line 180, wrapper return target(*args,
model does not converge and training is extremely slow with distribute strategy, if tf.py_function called in dataset.map.
when running a model that previously yielded no errors, i 'm getting errors of the form phofcode when using autograph with the tf.function decorator.
the url origin ( phofurl does not match page ( check server returns correct headers.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): centos linux release - mobile device (e.g.
'./algo ' 2. picking option 9 2. following instructions, including public ip, etc.
it produces the following error: > traceback (most recent call last): > file "./src/freeze_graph.py", line 100, in <module> > tflite_quant_model = converter.convert() > file line 908, in convert inference_output_type) file line 200, in inference_output_type, allow_float) file line 76, in calibrate_and_quantize 112, feedtensor return input_value) valueerror: cannot set tensor: got tensor of type string but expected type float32 for input 98, name: image_input i tried to only quantize the weights with tf.lite.optimized set to optimize_for_size and without setting representative_dataset option.
different escaping methods methods do not resolve the problem.
i am able to successfully build a model train, evaluate and run prediticions on a sequence model using nasnetmobile.
input-data-types are preserved when converting keras-model to estimator.
by removing tf.function decorator or enable run_functions_eagerly, program runs as expected.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
when gets a dtype of uint32 or uint64, it fails with error: `unimplemented: copyslicetoelement unhandled data type: 22` (this is uint32) this is an issue for using tensor_slice_dataset_op, which can calls this function and should also support these dtypes.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution: linux ubuntu 16.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12 python version: 3.5 cuda/cudnn version: v8.0.61 gpu model and memory: tesla p100-pcie, 12193mib
i would have expected the output to be same in eager mode as well, just like in graph mode.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): python version: 3.7 cuda/cudnn version: 10/7.4 gpu model and memory: tesla m60 on aws g3.8xlarge python w0301 tf_logging.py:161] object at note that this layer is not optimized for performance.
--- contrary `winleave`, `tableave` triggered when invoking vim with `-p` flag: vim -nu none -v8/tmp/log --cmd 'exe "au "..getcompletion("*", "event")->filter({_,v -> v !~# "cmd$"})->join(",").." * "" \' +qa -p /tmp/file{1..2} + 'v/executing/d_ ' + 'sil %s/executing ( s ).
the training should run smoothly.
if including tracebacks, please include the full traceback.
the output of `:syn list` not `no syntax items defined for this buffer`)
cursorline should highlight either the entire row (just like insert-mode completion 's popup menu), or the current item only.
my code with more results can be seen here: phofurl the key code is simply: phofcode the c++ op is compiled with -o2 optimizations.
phofurl there is a comment in the fieldset.svelte component indicating how to remove the {...props} from being adding props to the component.
steps to reproduce: - `git clone phofurl - `git checkout tensorflow_2.0` - (if needed) `pip3 install -r requirements.txt` - `export flask_app=flaskapp.py` start the app with `flask run` using postman or curl send any image of a dog or cat to the app screenshot at 16 10 57 phofimage or phofcode
this is inconsistent with the output obtained from calling directly on the original tensor (or slice of the original), and also inconsistent with format expected by `tf.raggedtensor.
- os platform and distribution (e.g., linux ubuntu 16.04):archlinux - tensorflow installed from (source or binary): binary - tensorflow version (use command below): - python version: 3.6
mixed precision training in tf.keras 2.0.zip phofhyperlink
after each run with the estimator, checkpoint is stored, saving the weights of that run.
if applicable, add screenshots to help explain your problem.
when collecting batch-level metrics, epochs are incorrectly numbered with the current `_samples_seen`
1. set terminal column size to 160 or more.
5. focus back the xterm window.
... steps to reproduce the behavior: 1. go to phofurl 2. click on `/api/bar` 3. see ui jump to a new category
when i call the model.summary() after define and the build model that is defined by the error occurs.
there should be no error initializing the gpu when using `predict_on_batch`.
- vim version: vim - vi improved 8.1 (2018 may 18, compiled jul 28 2019 included patches: 1-1722 - os: macos 10.14.6 - terminal: iterm2
provide a reproducible test case that is the bare minimum necessary to generate the problem.
calling `seek()` on a `tf.gfile.gfile` opened in write only mode raises
in tensorflow 2, this parameter has been removed the correct behavior (align_corners=true behavior) is now default.
traceback (most recent call last): file line 1, in <module> from model import siamesedream file line 1, in <module> import keras.backend as k file line 3, in <module> from .
as my code understanding with `concat` is different, am curious to understand if we really need to process int64 type of axis (is it a real time use case?)
as an example tf-addons uses this to link phofhyperlink with tensorflow core.
throwing the error same as the header of this issue phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): macos mojave version 10.14.5 - mobile device (e.g.
if applicable, add screenshots to help explain your problem redisuibeforecrash phofimage .
(= shows deterministic result) custom dense layer was phofcode and net with phofcode (+) when 'use_bias=false ' option applied on hidden layers, is was ok. (= shows deterministic result)
currently this raise an exception at runtime (not during graph construction).
a possible conversion from the `deferredtensor` to a `tensor` or `eagertensor`, or alternatively to perform group normalization in another way.
more code and examples of surprising behavior in this colab phofhyperlink .
<em>include any logs or source code that would be helpful to diagnose the problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): source tensorflow version (use command below): master python version: 3.6 bazel version (if compiling from source): 0.25.1 gcc/compiler version (if compiling from source): 5.4.0 cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with: 1. tf 1.0: `python -c "import tensorflow as tf; print(tf.git_version, tf.version)"` 2. tf 2.0: `python -c "import tensorflow as tf; tf.version.version)"` na
i have tried a few alternatives such as the `@tf.function` function decorator and `tf.scan` function, which have both been unsuccessful.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
first time stack: phofcode second time call stack: phofcode the only difference is before `_maybe_define_function, function.py:1596`.
these multiple callbacks also appear be concurrent, i.e.
batch normalization ops should have been folded into weights and biases of previous layers in the tflite graph in order to optimize inference latency.
phofcode this issue is critical for us.
code for deep and wide linear model.
traceback (most recent call last): 2375, <module> main() 724, main symlink=options.symlink) 992, create_environment download=download, 922, install_wheel call_subprocess(cmd, show_stdout=false, extra_env=env, stdin=script) 817, call_subprocess % (cmd_desc, proc.returncode)) oserror: command /root/env/bin/python2 - setuptools pkg_resources wheel failed with error code 2 phofcode
since commit a `tf.keras.layers.conv2d` in my training code fails with the following error: traceback (most recent call last): file "./train.py", line 351, in <module> total_loss = train_step() file line 417, in __call__ self._initialize(args, kwds, file line 360, in _initialize *args,
(or is there any other api that could help resolve this issue?)
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): arm 6 (raspberry pi zero w) - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macosx 10.13.6 - mobile device (e.g.
from abc import abstractmethod import tensorflow as tf import logging class function(object): def __init__(self, name=none): self._name = name def call(self, params, inputs,
link phofhyperlink to my python3 colab noteboo
large logs and files should be attached.
have already tried lots of different suggestions on how to release gpu memory phofurl phofurl phofurl several stackoverflow suggestions to no effect.
[[{{node e component function execution failed: failed precondition: error while reading resource _anonymousvar4 container: localhost.
')[-1] == 'jpg'] image_list_ds = image_list_ds image_list_ds image_list_ds.repeat() # read in and preprocess the images def preprocess_image(image): image channels=3) image tf.float32) [tgt_h, tgt_w, 3]) return def tf.read_file(path) return preprocess_image(image) image_ds image_ds image_ds.batch(1) # push path and into a list data 'paths_lr, inputs') return data( paths_lr=image_list_ds, inputs=image_ds ) def memory(): from somewhere online import os import psutil pid os.getpid() py psutil.process(pid) memoryuse py.memory_info()[0]/2.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
running the single-operation model given below using the metal gpu delegate causes the following error: > execution of the command buffer was aborted due to an error during execution.
a `tf.data.dataset` created using `from_generator()` is consumed and not recreated/repeated even when explicitly invoking the `repeat()` method.
search for either '' or '' in a file containing both of these characters, for example : `.
minimal.cc phofcode compiling commands: phofcode python script: phofcode
large logs and files should be attached.
tracked it down to `winbar_click()` where `wp` 's `w_buffer` is set some invalid address
i expect the dataset to be reshuffled after each epoch.
the compilation functions want to replace the `~` with `reg_prev_sub`, but this pointer is `null`.
in example value tab should be combined schema with all resolved references
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12 python version: 3.6 cuda/cudnn version: 9.0/7.3 gpu model and memory: 1080ti
'p ' not save file
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the only changes i made to the config.cfg are mtu to 1420 and i set wireguard to false user@user:~/algo$ ./algo play [localhost]
), but am not sure whether a patch introducing this behavior would welcome?
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 x64 1809 - mobile device (e.g.
is this a normal behavior, if it is, then which version of the matrix product should be taken as correct output?
- vim version - os: arch linux, up to date - terminal: console (no x server) * `strace` shows repeated: phofcode ` * i don't see 100% cpu usage when running an x server, and a terminal with vim inside it, using the x cursor!
on a second attempt to add a user and run algo update-users, the script errors when trying to create a new private key for ipsec configuration.
just show the data correctly.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary/from official docker container tensorflow version (use command below); python version:2.0.0-alpha0 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: using tf gpu docker/official gpu model and memory:nvidia v100 ,32 gb,nvidia-smi reinstalling tf in docker container works pip install --upgrade --force-reinstall tensorflow-gpu (this installs tf 1.13 version and unistalls the tf 2.0)
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macosx 10.13.6 - mobile device (e.g.
- vim version - os: up to date arch linux - terminal: konsole prime.c: phofcode
during handling the above exception, another exception occurred: valueerror traceback (most recent call last) _apply_op_helper(self, op_type_name, name,
i can work around this by using predict() method.
also note, on the code example below, that a third warning about a deprecated method, arises.
i use the benchmark_model to run the inference of models with tflite, but the speed of mobilenet is rather slow on snapdragon 845, which is nearly
horizontal scrollbar working on console window copy very long text to clipboard, and working drag.
the code works fine with cpu.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): arch linux smp preempt) - mobile device (e.g.
`ping www.google.com` <img width="1045" alt="screen shot at 4 18 17 pm" src=" phofurl
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no, beyond the trivial fix needed for phofurl and wrapping the tf.session in a debug session - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
i have this bug but i don 't know how to fix this
in most cases, program will just terminate but even more rarely following message is printed: > f check failed: buf_ null buf_ non-zero shape size 15 we discussed issue in our repository: phofurl
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): google colaboratory - mobile device (e.g.
as of custom plugins fail to build with the following error: phofcode this seems to be related to phofurl it appears that not all the files are placed in the proper locations after the installation.
when execute external command, include `:!cmd`, `:call system('cmd')` and `:terminal`, the current directory is always jump to
5. click "authorize" and enter dummy data in dialog.
all the changes i made compared to the normal code were to add the attentioncellwrapper.
or its just added fill gap between `concatv2` supported types vs its kernel implementation?
input tensor at index i will be upsampled by factor 2(n_inputs - (i+1)) # eg: if input tensors are [t1, t2, t3], t1 will be upsampled x 4, t2 upsampled x 2, t3 not upsampled.
see the code in my snippet above.
i am windowing the time-series to produce two arrays, x and y, and it looks something like this, phofcode (yes, realize that could just as easily only include one of the features.
it always shows the error above.
note that there are now phofcode instances of swagger ui redux state, where phofcode is number of times 'toggle ui' button was clicked.
however, if is applied to replicate the model on several gpus for multi gpu training, warnings like the followings are raised.
valid gradient for the weights in the sparsetensor whatever the number of calls done previously to the model.
i 've exhausted all options and followed the documentation but am unable to get `:!
if including tracebacks, please include the full traceback.
indices[66312] = is out of bounds: need 0 <= index < t [[node (defined at ]] caused by op defined at: file line 174, in _run_module_as_main "__main__", fname, loader, pkg_name) file line 72, in _run_code exec code in run_globals file line 86, in <module> main() file line 82, main 840, run 642, do_work work_executor.execute() 172, execute op.start() 436, process lambda: 221, acquire return constructor_fn) 183, acquire result = 85, acquire result = constructor_fn() 436, <lambda> lambda: 412, _make_graph_state self._exclude_outputs, tf_config) 316, __init__ saved_model_dir, {})) 370, saved_model_dir, logical_input_map, tensor_replacement_map) 227, input_map=input_map) 1435, import_meta_graph meta_graph_or_file, clear_devices, import_scope,
link to tensorflow model: phofurl link to the generated tflite model: phofurl code used to generate tflite file from tensorflow model: phofcode i used netron visualise the .pb and .tflite graph, link: phofurl
here, r=1 and axis=0, so the preconditions should be fulfilled.
... 1. go to phofurl .
it gives me the same inaccurate result or shows message like: `info: initialized tensorflow lite runtime.
if including tracebacks, please include the full traceback.
of course, might missing an existing feature allowing do so, in which case would most glad pointed way optimize run times when execution is enabled (maybe by enforcing the isolation of the operations in compiled graph?
without it, the elements are aligned but they 're not indented relative to their scope: (`setlocal cinoptions=#1,#0 | normal vip5<=ip>ip`) :verbose setlocal cindent?
svelte phofcode vue (for comparison) phofcode
this issue first appeared when i switched to tf commit toco worked fine when i was on tf commit
warning:tensorflow:from colocate_with (from is deprecated and will be removed in a future version.
and if i then focus back right window, text line of cursor in left window remains highlighted.
after much googling, found this on stackoverflow phofhyperlink so, is it an issue with the output layers in my model?
the `predict` method is supposed to not require compiling before use, but the method `_standardize_user_data` ( called at phofurl raises the following exception if model is not compiled, and a (x,y) tuple is returned from the dataset: > runtimeerror: you must compile a model before training/testing.
the logic flow is if programmer enables `autograph` option as `true` in `defun` decorator, function in will be called with `autograph=true`.
for example, if your data is graphs, even if you fix the same number of graphs per batch, you can still get a varying number of nodes or edges.
1. according to the documentation phofcode 2. run failed and shows cannot find module 'lodash'
1. paste the openapi config from above.
the framework should be named
when i run the model, the program used almost all the cpus in my server(the cpu usage is more than 5000%).
to be specific, inference takes * 0.53 seconds for original inference graph * 0.56 seconds for optimized graph why is the case?
provide a reproducible test case that is the bare minimum necessary to generate the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
to confirm that xla is active, pass (as a proper command-line flag, not via tf_xla_flags) or set the envvar w0624 deprecation.py:323] from (from is deprecated and will be removed in a future version.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): colab - tensorflow installed from (source or binary): pip install -q "tensorflow==1.13.1" - tensorflow version (use command below): 1.13.1 python version: 2.7.15rc1 include any logs or source code that would be helpful to diagnose the problem.
i seem to get reasonably good results on aws p3.8 (32 vcpu, 4 nvidia tesla v100 gpus), although it's still a bit below the ~3000 images/second that i've seen mentioned elsewhere.
when i define the following: phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux 18.04 - mobile device (e.g.
any hint on debugging this issue?
inference performance is very bad.
i suppose can be used in exactly the same way as
i used 1.12.0, 1.13.1, 1.14.0, r1.14 source build tensorflow/tflite with different optimization options with no success.
the crossshardoptimizer shouldn 't break.
why there is so huge difference?
a simple classification inception_v3 network reports on nvidia-smi using 3705mib whereas if i set a to for example 0.02, there is not performance impact when serving and it takes only 600mib of vram.
tf team should decide weather to support dict type inputs.
wireguard request timeout on macosx using client
2. edit `filename` 3. type '.... ' 4. describe the error
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: 3.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: n/a this inference network runs fine on tensorflow cpu and tensorflow gpu.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
this code is active for all calls to predict().
2. try a get with a header value for `x-irest-conn` 3. remove the value for `x-irest-conn` the request now is executed with empty `x-irest-conn` 4.
it appears keras is also having this issue: phofurl
`:call appendbufline( ' ', '$ ', [1, 2])` <img width="1680" alt=" 58 30" src=" phofurl line 1 is the unwanted blank line.
large logs and files should be attached.
rediscrashlog phofimage - os & version: microsoft windows 10 enterprise build 17134 - redis-server version 4.0.9 i'm trial user.
[y/n]: n no apache ignite support will be enabled for tensorflow.
i have sufficient ram (384 gb) to allocate the tensors because i can run 2 such processes on the machine without problem.
i 'm trying to optimize a tensorflow model to tensort optimization.
- cannot load lcoal files when cifar-10-batches-py mannully put to ~/.keras/dataset/.
there are couple of issues with the existing behavior -: a) ideally one should not be required to change the code in the callbacks.
(like it was for swagger 2.0)
padded_shape[0]=68 is not divisible by block_shape[0]=6 [op:spacetobatchnd]
just enable and --use_nnapi=true flags when running the benchmark on any model.
the gradient is calculated and returned after calling tf.session.run,
steps to reproduce the behavior: 1. go to '...' 2. click on '....' 3. scroll down to '....' 4. see error
there is no warranty; not even for merchantability or fitness for a particular purpose.
checkpoints should be restored properly
info:tensorflow:using the keras model provided.
now all are not working.
imagine you extend the tf.keras.layers.layer with the class mylayer, which is a wrapper for the tf.keras.layers.dense layer class (this class extension could also be a model, but that 's not important).
timedistributed wrapper around depthwiseconv2d fails with attributeerror: 'tuple ' object has no attribute 'dims '
with local variables, it works as expected: phofurl phofcode with a component prop, it does not: phofurl phofcode if `<h1>outer $s = {$s}</h1>` is commented out, no subscribe/unsubscribe occurs when the button is clicked.
error message: > i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 fma > w unable to create streamexecutor for cuda:0: failed initializing streamexecutor for cuda device ordinal 0: internal: failed call out of memory; total memory reported: > w unable create streamexecutor for cuda:1: failed initializing streamexecutor for cuda device ordinal 1: internal: failed call out memory; total memory reported: > f attempting fetch value instead handling error internal: no supported devices found platform cuda
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux - mobile device (e.g.
i have tried probably about 20 different servers.
already had to use tf.float32)` to prevent some errors.
so i believe intended behavior is that all created models are loadable.
when evaluating many tensor, this substantially spams console.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.13.6 - tensorflow installed from (source or binary): from pip install - tensorflow version (use command below): 2.0.0-beta0 python version: oct 20 2018,
assert not isinstance(var_assign, assert var_assign.dtype == tf.float32 ``
>>> import tensorflow as tf >>> @tf.function ... def foo(): ... return 42 ... >>> foo() traceback (most recent call last): file "<stdin>", line 1, in <module> file line 336, in __call__ self._initialize(args, kwds) file line 309, _initialize *args,
when trying to test the inference with `saved_model_cli --input_examples`, it produced error message.
the custom ops run correctly in inferring mode, however, it throws bug during `model.fit` or below: phofcode
24. the property a focusable element not null.
removing that will make reactivity work again.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): tensorflow 2.0.0-alpha0 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.0, 7.5 gpu model and memory: 1080ti 11gb
i've convert my keras network to tensorflow eager and this network dosn't learn anything.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
* bash phofcode removing -lprotobuf from the above command will get rid of the segfault.
- have i written custom code: yes using c++ api - os platform and distribution: ubuntu 16.04 - mobile device (e.g.
1. generate the test data phofcode 2. run the test code.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.14.0rc1 python version: 2.7 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): gcc 7.4.0 cuda/cudnn version: nr gpu model and memory: nr n
1) version 1 : single placeholder which takes in tfexample and rune the parseexample operator and does inference 2)version 2 : instead of single placeholder , i have about ~300 placeholders .
if you have tiny vertically split windows and use `windo`, then the tiny windows get expanded.
i expect the progress bar to go up to 100% and display a newline.
i would imagine it to be as parallel as the model itself is (i.e.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): (this is the tf 2.0-preview) python version: 3.6.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: here is the output of the above commands when i run them in a python shell: pycon (tf2) $ python python 3.6.6 (default, jun 28 2018, [gcc 4.2.1 compatible apple llvm 9.1.0 on darwin type "help", "copyright", "credits" or "license" for more information.
after some digging, i found that it may be caused by the algorithm used in the cycle detection code: phofurl
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): docker image tensorflow version (use command below): tensorflow-gpu 1.12.0 python version: 3.5.2 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 9.0 / 7 gpu model and memory: gtx 1080
if i use the custom training loop, which manually computes gradients and applies them via an optimizer, the model does not converge.
that the python kernel operate normally.
should only open the clicked item
session run cost 4s on gpu: phofcode session run cost 4s on cpu: phofcode
traceback (most recent call last): file line 426, in import_graph_def graph._c_graph, serialized, options) # pylint: disable=protected-access nodedef expected inputs ' ' do not match 1 inputs specified; op<name=const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; nodedef: {{node during handling of the above exception, another exception occurred: traceback (most recent call last): file line 100, in <module> export() file line 95, in export file line 146, in _ = name="") 507, new_func return func(*args,
if using a dictionary to pass input to a `keras.model` with named inputs, the `steps_per_epoch` argument appears to have no effect.
run this script phofhyperlink passing as an argument this file phofhyperlink to create an h5 keras model.
in a normal, non-popup window syntax highlighting following text property is as expected.
that is true for graph mode, but not for eager one.
phofcode a stack overflow post phofhyperlink reported the same issue.
phofcode with this setup, i observe phofcode phofcode but on the other hand, if i pass the input as a dictionary i observe phofcode strangely, if i then run the first `.fit` command again, it also performs a full pass over the data, rather than doing specified number of steps as originally.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: samsung galaxy a5 - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: 3.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: na gpu model and memory: na you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
it seems there are substantial differences in tf0.12 and tf1.12.0 in terms of initialize a variable throught the value of another tensor.
in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype) 870 871 --> 872 swap_memory=swap_memory) 873 874 # unpack final output if not using output tuples.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.6.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.0 gpu model and memory: geforce 940m 2gb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with: 1. tf 1.0: `python -c "import tensorflow as tf; print(tf.git_version, tf.version)"` 2. tf 2.0: `python -c "import tensorflow as tf; tf.version.version)"`
doing `make clean` after a `make test` should (and historically has, afaict) cleaned up all the test-file debris.
- vim version: - os: arch linux x86_64 - terminal: st add any other context about the problem here.
parameters from tmp/model.ckpt-1 info:tensorflow:running local_init_op.
i am working with gan.
the same code worked in tensorflow 1.13. in tensorflow 1.13, `k.__module__` does not exist, while in tensorflow 1.14, `k.__module__` is ~~~ ~~~ it makes the `tensorflow.keras` module not loaded during unpickling.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): sorta - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i have an image and 3 points.
got error log: f unimplemented: this graph contains an operator of type div for which the quantized form is not yet implemented.
all outputs should be the same my model looks like this: phofcode if i run the exact same thing with `def y_pred): return k.mean(k.square(y_pred - y_true))` and loss=my_loss, and batch_size=256)))` it works fine.
this commit enabled layout optimizer by default for all gpu cluster.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the converter should not reorder operations across reshape if it would cause the dimensions to no longer match.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): both tensorflow version (use command below): 1.10+ python version: n/a bazel version (if compiling from source): 17.2 gcc/compiler version (if compiling from source): 7.3.0 cuda/cudnn version: n/a gpu model and memory: n/a also the second parameter to settotalbyteslimit has been deprecated and is no longer enforced.
phofcode but this code will be failed.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no - os platform and distribution (e.g., linux ubuntu 16.04): centos linux release 7.0 (final) - tensorflow installed from (source or binary):binary - tensorflow version (use command below): '1.11.0 ') python version: python 2.7.15 include any logs or source code that would be helpful to diagnose the problem.
at the same time, i provide a colab notebook phofhyperlink for reproducing this issue.
if including tracebacks, please include the full traceback.
works fine in tf 1.12, but the same code now fails in (`total_loss` attribute not found).
failed to clear function handle cache: invalid argument: failed to find functionlibraryruntime for device when releasing multi-device function handle 1
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): 18.04 lts - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: samsung galaxy s8 - tensorflow installed from (source or binary): binary tensorflow version (use command below): on pc: current tf-nightly on phone current python version: 3.6.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: cpu only output behavior is as expected for all models.
from ._conv import register_converters as _register_converters { 'batch_size ': <absl.flags._flag.flag object at 'c_dim ': <absl.flags._flag.flag object at 'checkpoint_dir ': <absl.flags._flag.flag object at 'epoch ': <absl.flags._flag.flag object at 'h ': 'help ': 'helpfull ': 'helpshort ': 'image_size ': 'is_train ': 'label_size ': 'learning_rate ': 'sample_dir ': 'scale ': 'stride ': warning:tensorflow:from colocate_with (from is deprecated and will be removed in a future version.
checkpoint frequency is determined based on runconfig arguments: save_checkpoints_steps 2000 or save_checkpoints_secs none.
... steps to reproduce the behavior: 1. add an oauth definition to your swagger.yml file with a relative url as the autorizationurl value 2. try to authenticate with oauth 3. observes that you are redirected to the relative url you specified but on the swagger ui host, instead of api server.
- os: [windows 10 1903] - terminal: [cmd.exe and gui] i have tried neovim and it works there (with the recent version) everything works in too.
i can't delete old algo ikev2 profile since there is no delete(remove) button in settings.
and pickle uses `__name__` get global names: phofurl cc: @alsrgv @hanyucui @annarev
when it does not hang, it prints: phofcode when the process is hang, seemingly in deadlock, the only change is main thread 's stack, with top of stack being `join`.
either the tensor 's .device is wrong, or tf.cast() is wrong.
it should be `1 + 0j` as is returned by `np.complex(0., 0.)
if including tracebacks, please include the full traceback.
fft2d produces non-sensical results on complex64 datatype.
relate to an earlier (possibly always the first) image in sequence, not current image.
i try to import graph from a metagraphdef proto that contains a cudnnrnn cell, the error below raises: traceback (most recent call last): file "test.py", line 21, in <module> saver = file line 1674, in import_meta_graph meta_graph_or_file, clear_devices, import_scope,
file does not exist: savedmodel load for tags { serve }; status: success.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04.6 lts - tensorflow version (use command below): 1.13.1 - python version: 3.6.8 cuda/cudnn version: cuda 10.0/cudnn 7.6 gpu model and memory: t4 gpu with 15079mb memory #### my gcp instance info screen shot at 2 53 17 pm phofimage screen shot at 2 58 08 pm phofimage screen shot at 3 00 14 pm phofimage screen shot at 2 58 28 pm phofimage #### gcolab info 9 54 30 am
similarly, using an external browser such as firefox 68.0 (64-bit), results in a similar behavior in that `delete does cause `add button to be displayed, but `delete last does not.
' : 'c ') .
i would see how the -19 would be correct if i had a stride of 1 along y-axis, however, then it would not match up again with third number being 1 instead of -4. so it is not only that either dilation or stride is ignored, but actually providing for both non-trivial values results in some strange behavior, which i think is actually a bug.
/ 1/ ' +x /tmp/log grep -i 'tableave ' /tmp/log --- as a workaround, i 'm experimenting with this autocmd which visits all windows after `vimenter` has been and was invoked with `-d`, `-o` or `-o`: augroup au!
ideally such a `graph` would not be referenced forever, allowing it to be garbage-collected.
similarly, using 128 parallel calls in the mapping function gives better results than 32 or 64, but changing that or making it dependent on the actual number of vcpus doesn't solve the observed behavior.)
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no.
sometimes, i have a buffer which is folded thanks to fold options (`'fdm'`, `'fde'`, `'fdt'`) set from a filetype plugin.
the two models use the same architecture (keras' densenet169 implementation), however have a different number of target classes (80 in one case, 200 in the other case).
i've run into one issue - every time i update list of users, all android clients stop working and they need to upload new wireguard configs which is really hideous process.
does not broadcast the same value when using to the description of the method, it is supposed to mirror a tensor on one device to all worker devices
if it is local to buffer, the doc should be updated and `py should be the correct usage, `py not.
when writing to a zipfile where `file=gfile()` the resulting zip file is corrupt.
i created a keras model that takes as input 2 feature columns named 'store ' and 'loc '.
conv_2d: expected 1 input tensor(s), but node has 3 runtime input(s).
it takes a lot of time to train a robot.
the code should also execute normally when using `@tf.function`.
in fact, this is nearly all the operators, save for `placeholder`, `transpose`, `leakyrelu`, `resizenearestneighbor`, and `splitv`.
after running the constfold optimizer, these operations on the weights are still present, even though they should be folded.
2 means `all_gather` works if only collects tensor within one graph.
the weights are loaded successfully.
vimfile_highlight_error phofimage phofimage (also happens in gvim.)
there are 2.7 hundred million variables in my embedding files.and instread of adamoptimer,i used lazyadam.but it cost the same time as adamoptimizer
- os platform and distribution : linux ubuntu 16.04 lts - tensorflow installed from binary : pip - tensorflow version : 1.13.0-rc1 - python version: python 3.5.2
`docker run --cap-drop=all -it -v -v ~/path/to/config:/data trailofbits/algo:latest`
input_data) interpreter.invoke() output_data_tflite output_data_tflite)) </pre>
python import tensorflow as tf import numpy as np tensor_1 = 20))) print() # set breakpoint here ``
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: 3.6.8 bazel version (if compiling from source): none gcc/compiler version (if compiling from source): none cuda/cudnn version: none gpu model and memory: none tf.version: 1.13.1 console (tf-1.13-py3) [huwh1@huwh1-centos worksync]$ python tf_dataset_saveable.py warning:tensorflow:from colocate_with (from is deprecated and will be removed in a future version.
however this is not possible because only 58% utilised and training runs fine, therefore it seems there an issue with saving checkpoint.
it is returning a value error: dimension -3 must be >= 0. i believe all the information in fit method is correct...
ensure that python 3.7 is installed and try same thing: phofcode this should run successfully and print "success".
i don't see this happening in real life.
not found: resource does not exist.
inside a distribution strategy scope, restoring a keras model (that has been trained at all) with raises the exception shown below (while handling the optimizer in particular, it seems).
1. write this in `/tmp/vimrc`: fu func() let [a, b, c] =<< trim end xx x xx end echo [a, b, c] endfu call func() 2. start vim like this: $ vim -nu /tmp/vimrc the command raises the error `e688`: e688: more targets than list items the first `xx` has been joined with ` x` (the backslash is removed in the process).
1 edit `vimrc` and add `set wildignore=*.pyc` 2. create a file `file.pyc`.
i convert model to .tflite, i know the model 's training input size is i want to use different input tensor size, such as so i use the resizeinputtensor() function,
having looked at the source code, i would expect `on_epoch_end` to be triggered at the end of each epoch
traceback created after appending to the issue phofurl shows that assign_sub is failing as tensor object have no assign_sub.
a simple toy example using gradient tape and a subclass-model cannot be transformed into a graph with autograph.
in other words, i computed a second order derivative.
- os platform and distribution linux ubuntu 18.04 - tensorflow installed from source: v1.13.1 - bazel version 0.21 - gcc/compiler version: gcc version 7.3.0 (ubuntu the simple graph file attached supply its path as a command line argument [graph.pb.gz]( phofurl
the tutorial *tf.distribute.strategy with training loops* fails on the call when i run it on colab, as does any other attempt on my part to use checkpoints with mirrored strategy in colab with custom loop.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary):source tensorflow version (use command below):1.12.0 python version:3.6.0 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version:9.0 gpu model and memory:nvidida1070/ 8gb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
in eager mode, first epoch was really slow, while following ones proved increasingly fast, stabilizing at 11 / 12 seconds (19 ms/step).
in tf 1.8 the import library is present, but not in tf 1.12. this library is required to build custom operators.
i set up the aws account on my girlfriend 's laptop.
it seems some operators are not supported currently by nnapi.
ubuntu 16, armeabi-v7a, bazel 0.18, tried out with ndk 16b and 15c
it uses the default "/admin/..." path when it should be "/panel/..."
being able to train keras model in distributed mode.
phofcode phofcode phofcode $ ldconfig -p | grep libcublas tlibcublaslt.so.10 (libc6,x86-64) => tlibcublaslt.so (libc6,x86-64) => tlibcublas.so.10 (libc6,x86-64) => tlibcublas.so (libc6,x86-64) => ``
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): google colab - mobile device (e.g.
however, the same code works without a problem if code is executed on cpu.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
error: pack node (stack) axis attribute is out of bounds: -2 -1 0 1 w failed to run optimizer arithmeticoptimizer, stage strided_slice_3.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the node giving this error: {{node is an internal bug, please file a bug report with instructions on how to reproduce the error.
just run something written with svelte on a ios 9 device.
repeatedly allocating a graph and making a summary writer leaks memory.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below): python version: bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): n/a tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: na gpu model and memory: na you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
import tensorflow as tf x = softmax_keras = softmax_tf = tf.nn.softmax(x) y_true = loss_keras softmax_keras) loss_tf softmax_tf) def return set([tensor.op.type] + [inp for tensor in tensor.op.inputs for inp in print( 'softmax ' in # prints "true" print( 'softmax ' in # prints "false" ``
python #!/usr/bin/env python import numpy as np import tensorflow as tf from tensorflow.python.keras import model from import layer class mylayer(layer): def __init__(self,
however, the call to seems to end the scope of the `summary_writer`.
notebook 6.3, (under the heading "1.6 using recurrent dropout to fight overfitting") has a model with a dropout=0.2, recurrent_dropout=0.2, input_shape=(none, float_data.shape -1])).
when training a model which contains conv2d_transpose layers (using the nchw format), the last 2 dimensions of the layers weight tensor swap when reloading the model.
the problem also goes away if i replace the `randomdataset`-derived dataset with one built with `from_tensor_slices` from constant data.
the adam optimizer does not seem to keep calling the supplied learning_rate callable.
large logs and files should be attached.
the problem is that feeding model a tensor in custom loss function leads to a `typeerror: argument of type 'nodependency ' is not iterable`.
error message: > i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 fma cpu frequency: hz xla service executing computations on platform host.
can still reproduce issue, so don 't think it 's due to some other terminal option being incorrectly set.
... 1. use my yaml 2. go to /meta 3. check "example value / model" tabs
if this is actually the intended behavior, that should be made more clear in the documentation for .map()
steps to reproduce the behavior: 1. go to '...' 2. click on '....' 3. scroll down to '....' 4. see error
`dict` objects wrapped around `_dictwrapper` by `autotrackable` are serializable by pickle.
when trying to train a model that has multiple input layers using keras (eg.
similar outputs in both cases
the tflite converter converts the tf.keras model to tflite model.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
import tensorflow as tf import numpy as np x = = 0.0, name = "x", dtype = tf.float32) y 0.0, name "y", dtype tf.float32) zero tf.constant(value 0.0, dtype np.float32, name "zero") op1 tf.assign(x, zero) with op2 tf.assign(y, x) with tf.assign_add(y, 1, name "assign_add") sess tf.session() sess.run("assign_add") print("y", sess.run("y:0"))
the path to cuda should be dynamically obtained (by configuration or environment variables).
default do you wish build tensorflow with apache ignite support?
when im trying to update-users - ansible is getting stuck after display the invocation environment.
the generated curl command includes a blank type (-f "file=...;type=") which causes the curl to be invalid.
typeerror: ( 'not json serializable: ', b '
i use input_image: grace_hoper.bmp phofhyperlink labels.txt: from phofhyperlink
i expect to be able to define metrics based on any layer 's output, when using the functional api.
the model will attempt to load the previously trained weights and resume training from the last completed epoch, leading to error.
... steps to reproduce the behavior: 1. acquire a huge model - can be provided by mail if necessary 2. click on some operation 3. browser will hang / get stuck huge model causes browser to crash / hang for a long time , while this model is also circular it is not the issue ( as for one instance when opened the specific operation it did not crash but took a long time - if it was a circular handing issue i would expect it to hang forever).
phofcode the code above leaks cpu memory quite rapidly ~3 mb/s.
`toco --graph_def_file frozen_graph.pb --output_file quantized_graph.tflite --inference_type float --inference_input_type float --output_format tflite --input_arrays input_1 --output_arrays output_node0 --quantize true`
only when two windows in a tab horizontally and the upper one maximized, the line change can be observed.
we connect to each cluster for local testing via a nat server using different ports to route traffic.
make sure that the latest nvidia driver is installed and running.
when passing model_dir with a local file system, it saved successfully.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 x64 1809 - mobile device (e.g.
everytime i attempted to use it, i would get `shape must be rank 2 but is rank 1 for (op: 'matmul ') with input shapes: [1024], but as soon as i swap to dynamic_rnn, it executes perfectly.
users need to reselect ssh private keys all the time due to app store sandboxing
the model loads and runs fine on cpu.
i would expect the mask to be propagated as if the inner layers were not in a `sequential` layer.
alternatively, i 'm also expecting to be able save the model with the `model.save(filepath)` function.
raising as a bug because i couldn 't find anything that described this behavior.
the betainc_op test should pass, where none of the values are nan
finally, it defeats the purpose of the function.
after step 3 in steps to reproduce, d phofimage * arch linux + vim * debian jessie + vim 7.4.576 both machines are ssh'ed from qterminal git-master ( phofurl `term=xterm-256color` on all local and remote machines.
node number 73 (tflitegpudelegate) failed to prepare.
not found: container localhost does not exist.
you also need comment out `$argadd /tmp/a.txt`.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
it also runs about 10x slower than its supposed to.
segmentation fault (core dumped) and this is the callstack from gdb .
import tensorflow as tf test_path = "test" # create a file at <test_path> with open(test_path, "w") as f: f.write("hello, world!")
repl repro phofhyperlink open that repl link, switch to the "js output" tab, and switch from dom to ssr mode to see the issue.
with `mirroredstrategy`, the model is expected to be run in parallel.
i 've attached corresponding timeline, meshgrid op is called timeline.trace.tar.gz phofhyperlink
even if the main function is executed in eager mode, the map function is still executed in graph mode.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i want to do validation after some training steps, so i choose `feedable iterator api`.
i am using tfrecorddataset(), and use both prefetch() and num_parallel_calls for mappings.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12 python version: 3.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: 9.0 / 7.1.4 gpu model and memory: nvidia tesla k80, quadro m1200 i traced back the error message to when it was first introduced phofhyperlink in the code base to revise the logic.
python import tensorflow as tf from tensorflow import keras import numpy as np training_set_size = 32 * 10 # <= works only if self.add_metric() is added and this is a multiple of 32 x = 8) y = 1) class def __init__(self, output_dim,
clearly, an efficient model would not perform embedding and loss computation in every step but that 's not relevant here.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.11.0 python version: python 3.6.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: cuda toolkit 9.0 (sept 2017) cudnn v7.0.5 (dec 5, 2017) gpu model and memory: device: 0, name: quadro m4000, compute capability: 5.2 memory_limit: device: 1, name: quadro m4000, compute capability: 5.2 memory_limit: keras version: 2.2.4 i am trying to load a model saved with keras (not tf.keras) to create an estimator.
keras .fit is just a wrapper why would it be way faster???
m_def = { "postprocessing.editor", "recorder", }; (but you can 't remove new when passing to a function.)
after a certain number of calls, the custom training loop with gradient tape is not able to compute gradient for the sparsematrices used in the model (weights inside the sparse matrice), gradient for biases is computed without any issue.
i'm using the model for inference from c++, my batch size is known as soon as the program starts but it can vary each time it's started so the graph doesn't have a fixed input size.
error message is as follows: linking of rule failed (exit 1) undefined symbols for architecture x86_64: referenced from: const in tfcompile_test.o referenced from: const in tfcompile_test.o ld: symbol(s) not found for architecture x86_64 clang: error: linker command failed with exit code 1 (use -v to see invocation) target failed to build` the problem seems to stem from constructing or matmulandaddcomp objects in `tfcompile_test.cc`.
i create a tflite interpreter and allocate a direct bytebuffer in java to process a sequence of images.
the cursorline is not drawn correctly in a `popup_menu()` window.
2. point ui to a http url with no `schemes` value defined.
i expect to do a forward pass with a model with a bachnormalization layer in training mode, when using the but i can't, because it reises the following exception: > runtimeerror: `add_update` was called in a cross-replica context.
if u look at the timeline, operations named
from profiling, find in practice, best-case 20ms call often expands several hundred milliseconds first time its called very short whileso practice its even less smooth than you might expect.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12 python version: 3.6.0 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 9.0 gpu model and memory: 7.4
from_config(cls, config) 414 a layer instance.
the following code shows this by printing the used memory at each iteration.
the popup window cannot be closed with the mouse when `popup_filter_menu()` is used.
7. the apikey is authorized even though there is nothing in textbox
first example: phofcode second example, with a rank even larger than 3: phofcode which outputs is: phofcode we see, in this example, that `dense` and `timedistributed(dense)` behave the same in that they only touch to the last dimension of the input.
the netrw file listing gets corrupted when cycling sort order with the `s` key, in the long listing and with `let g:netrw_sizestyle = 'h'` for human-readable file sizes.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
it is enough to execute the code snippets included in the issue.
aborted (core dumped) none what should do solve problem ??
the bug seems to rely on a combination of: 1.
slightly different than the discrepency displayed in the code snippet above)
post /auth/local/register http/1.1 host: localhost:1337 content-type: application/json { "email": "foo@bar", "password": "blablalbal231sad" }
this function should handle these primitive types
1. copy this snippet into `./test.c`: phofcode 1. create a tags file with ctags: `ctags -r .`.
the code runs without a failure.
we use `fit_generator` with a custom build on but we also have seen similar behavior with namely, when we have the default the training process is slow but the memory usage saturates at some point and everything works fine.
i built the code successfully with ndk standalone toolchain r17c.
i am trying to parallelize an estimator model built from keras model via which works fine unless i pass the train_distribute parameter (of runconfig) a mirroredstrategy when creating the model.
class mylayer(layers.layer): def __init__(self, output_dim,
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no, i have used xlnet training - os platform and distribution (e.g., linux ubuntu 16.04): 18.04 - mobile device (e.g.
in my c program i can load/save/restore my model with no problem, i am expecting to be able to load trained model and make predictions in python as well from the model trained in my c program.
after being released, vim keeps scrolling the menu for about 6 seconds.
or any hints how to debug this gpudelegate issue.
here the link to the colab code : phofurl
in other words, i'm expecting the network's weights to be updated after a training step.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): 18.04 - mobile device (e.g.
chrome tracing results: with parseexample, no cpu idle during inference image phofimage phofhyperlink without parseexample, cpu idle during inference with red marks image phofimage phofhyperlink any advices will be appreciated, thank
running estimator training as follows, should only keep 1 checkpoint file version (latest) instead all checkpoints are kept.
seems fine on cpu and bizarrely if any ops are put as a control_dependency to the apply_grads call then everything seems fine.
it should run, as it did before.
then, want to understand what's wrong with my usage of tf.nn.conv2d, but no clue.
when i am using any distributed strategy, and i fetch a `tf.variable`, i get a result that looks like: `[array([( 10,), ( 44,), ( 47,), ...], dtype=[( 'resource ', 'u1 ')])]`.
i 've had it happen twice since yesterday, but unfortunately, i can 't reproduce it yet.
running the docker version ended with `rsync` permission error and did not sync any config files.
0. yolo output [[[ ]] [[ ]] [[ ]] [[ [[ after optimization
i'm creating several threads with one session on each.
train_images /= 255. test_images /= 255.
in 1.13, `__dict__` had a mapping of every tensorflow functions and `__doc__` had printed out ` 'bring in all of the public tensorflow interface into this module.
conv_2d: expected 1 input tensor(s), but node has 0 runtime input(s).
import numpy as np import tensorflow as tf kernel = np.array([5, 5], 2, 1, 1]) bias = np.array([-4], dtype=np.float32) img = np.array([-2, -1, -3, -2, 3, -1, -2, -1, 2], 1, 9, 1]) strides = (2, 2) dilation (1, 2) depthconv2d_layer depth_multiplier=1, kernel_size=(1, 2), strides=strides, dilation_rate=dilation, padding= 'valid ', *args,
tf.scatter_nd sums update values if indices are present multiple times.
the tf.keras.model should return expected value rather than raising a error.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
i am using the functional api of keras models, and tpu does not yet support multiple inputs layers.
i would not expect upgrading the library to slow down data infeed.
15.2kib 14.5kib client-requested bin (2048): ttotal chunks: 3, chunks 3.
the expected behavior would be the model compiling and fitting as it does with the `model(model.input)` line uncommented.
the observed problem at this point is: phofcode 6. hit enter.
no message on status line.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - mobile device (e.g.
if model is trained and evaluated using * 1.10 * , results (given all constants as set in code) are following: phofcode now, results above are pretty bad, but it 's due to really small dataset, used train model, but it 's not significant from standpoint of showcasing wrong behavior: our models, predictions, done with 1.10-trained model are pretty on-point.
> ds_train = tfds.load(..) > ds_val = tfds.load(..) > > strategy = num_gpu = ds_train ds_train.batch(32 * num_gpu) ds_val ds_val.batch(32 * num_gpu) with strategy.scope(): input_image input(...) .... output .... model outputs=[output]) model.compile(...) model.fit(ds_train, validation_data ds_val) model.predict(ds_val, use_multiprocessing true)
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
please refer to this gist phofhyperlink .
-block of information displayed during algo not available
the `call` method of the model and a custom training function work well even if the `input` layer is unnamed, but the `fit` method fails.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): tensorflow==1.12.0 python version: 3.6.8 you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
also included custom layer as part tf.keras.model and simply printed model summary.
4. see how 'results ' is written in small font just after the executed result.
is there something that we are missing
when saving a checkpoint using restoring the checkpoint should also recover the optimizer variables.
when text properties are used to provide further highlighting on top of standard `syn match` and `syn region` highlighting, the first character that should be text-property highlighted actually has the highlight of the preceding character.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): (this is the tf 2.0-preview) python version: 3.6.8 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: here is the stacktrace: traceback (most recent call last): file "<stdin>", line 2, in <module> file line 455, in _method_wrapper method(self, *args,
the installer runs and completes (or throws an error)
- i have written custom code - linux ubuntu 16.04 - tensorflow installed via pip from binary - tensorflow version: 1.7.0 python version: 3.6.3 bazel: 0.14.1 gcc: 5.4.0 cuda 9.1.0, cudnn: 7.1.2 gpu model and memory: nvidia quadro p6000/pcie/sse2, 24gb memory larger context about this and the pure keras script that converges can be found here: phofurl
> traceback (most recent call last): > file line 193, in _run_module_as_main > "__main__", mod_spec) file line 85, in _run_code exec(code, run_globals) file line 140, in <module> main() file line 132, main 166, run_train_and_evaluate 81, train_and_evaluate train_spec, eval_spec) 471, train_and_evaluate return executor.run() 610, run return self.run_local() 711, run_local 354, train loss = hooks, saving_listeners) 1207, _train_model return hooks, saving_listeners) 1241, _train_model_default saving_listeners) 1471, _, loss = estimator_spec.loss]) 671, run 1156, run 1255, run raise 693, reraise raise value 1240, return self._sess.run(*args,
"); } and here 's how i 'm loading images and attempting to get a labeled probability map long starttimeforloadimage inputimagebuffer loadimage(bitmap, sensororientation); long endtimeforloadimage trace.endsection(); logger.v("timecost to load image: " + (endtimeforloadimage - starttimeforloadimage)); runs inference call.
if this is the case, the tutorial colab has to be adapted: phofurl
detailed steps to reproduce the behavior: 1. run `gvim --clean` 2. enter some text.
any network, just feed dynamic batch sizes.
python import tensorflow as tf def gen(): yield 0 def model_fn(features, labels, mode): shape = [1, 100, 10] x = tf.random_normal(shape) y = tf.zeros(shape) cell = shape[-1]) prediction, _ cell(x) loss y)) train_op return prediction, loss, train_op) config tf.estimator.runconfig() # training begins successfully with the following line commented out.
the model in question is a `tf.keras` model compiled with as optimizer, with momentum.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): nope, this only concerns performance of model.fit for a tf.keras.sequential model, comparing versions 2.0.0-alpha to 1.13.1.
detailed steps to reproduce the behavior: 1. copy text in the notepad or somewhere.
complete code snippet, you can run it directely without any modification.
note that this happens * at * program startup (after runtime restart on colab).
i have a test (for phofurl that feeds `<cursorhold>` via `feedkeys` over a channel in order to test and trigger some behavior.
assuming my previous issue is solved, i think that the result of `plot_model` should simply look like this phofimage .
the global steps should be 1625 but it 's 1. the acc and loss shouldn 't be equal to 0 as both of them are contradicting each other.
vim shows empyt lines only
this issue is compounding over a spectrogram preprocessing pipeline making tensorflow unusable for pushing my signal preprocessing from numpy into tensorflow.
it outputs: c <c-a> * xxx c <c-a> * xxx
i 've got a constant `window_size` defined of which dtype defaults to `int32`.
no error; the iterator produces a single batch.
passing python boolean values instead of `tf.constant(true)` works fine, with and without `@tf.function` decorator.
we excluded our custom written code as the source of the memory leaks and made sure that the model actually fits into memory with enough headroom.
phofcode phofcode i think here is not being able to properly assign the convolution ops to other devices which they are initially created for "model_creation_device" (see line 97 in predrnn.py phofhyperlink ).
please report this to the autograph team.
maybe this is a bug in google colab, as the code works fine with eager execution enabled on my machine with 8 gb of ram and a geforce 1060.
i have standard ae network with pixel shuffler layer.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): windows - tensorflow installed from (source or binary): pip install - tensorflow version (use command below): 1.14.0 python version: 3.6.8 include any logs or source code that would be helpful to diagnose the problem.
delegate setting done node number 70 (tflitegpudelegate) failed prepare.
detailed steps to reproduce the behavior: 1. set up gvim to use any nerd font ( phofurl 2. set listchars=tab:
include any logs or source code that would be helpful to diagnose the problem.
and the way to achieve that is to wrap a list of feature columns with and parse it to the input layer as a tensor like so: `feature_layer = `inputs = name= 'features ')`
hyperparameters do update on the first run, ie they are initialized to tf.variables at constructor of the optimizer, not on first call.
the model has no specific meaning, and i just want to test lstm network.
while trying to serve the model an error is thrown.
it works until few days ago only i do recently is update nvidia driver to 418.9
but since there no public change log for tf-nightly.
`python min.py bad` for expected behavior: `python min.py good` note: your running the bad code may or may not crash depending on your gpu memory.
large logs and files should be attached.
large logs and files should be attached.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): not sure tensorflow version (use command below): python version: 3.7.3 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: cuda 10.0 gpu model and memory: tesla v100, 16130mib logs with run commands for 3 situation above: profile.txt phofhyperlink
1. the content must have json fields.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
when the freebsd instance is rebooted the ipv4 address for lo100 is not setup and dnscrypt-proxy fails to start.
* the `{` and `}` operators should jump only to lines 3, 7 and 11.
steps to reproduce the behavior: 1. be on wifi 2. turn off wifi 3. no traffic is passed
execute the application under proxy
right now it seems a a minimum of one and a max of 4 fail with the error above.
a = input(shape=(none, 10)) attn = attention()([a,a]) model = model(a, attn) model(np.zeros((50,10), dtype=np.float32))
when i run a simple code snippet i get the following error `attributeerror: 'module' object has no attribute 'session' phofcode * output * attributeerror traceback (most recent call last) in <module>() 5 c = tf.matmul(a, b) 6 # creates a session with log_device_placement set to true.
-> 1853 return # pylint: disable=protected-access 1854 except attributeerror: attributeerror: 'prefetchdataset ' object has no attribute during handling of the above exception, another exception occurred: runtimeerror traceback (most recent call last) in <module> 1 #!!
data = data.window(3) #each entry is now a window dataset #map fails, because the dataset is nested #data.map(lambda x: print(x)) #flat_map fails because x is a tuple, not a dataset data = data.flat_map(lambda x: x.batch(3))
if including tracebacks, please include the full traceback.
if applicable, copy/paste the text or add screenshots to help explain your problem.
models of the same version should be identical.
- have i written custom code:
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): public colab instance - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.14.0-rc1 python version: 3.6
--- i can also reproduce issue when ` 'cursorline '` is not set, and ` 'cursorlineopt '` is set to `screenline`: vim -nu none + 'setl culopt=screenline ' + 'set ft=vim|syn enable|/readline#yank ' + 'setl <(curl -s phofurl so, even if cursor line is not visible, latency can increase a lot.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
because is not implemented, i implemented this: phofcode this works: phofcode but this does not: (the difference is dilations or specifically dilations + nchw data format) phofcode is gives following error on the batch_norm which follows the depthwise_conv2d x = self.caxis, training=self.training) file line 324, in new_func return func(*args,
my wifi is a openwrt router with a network wide wireguard vpn installed.
you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with: 1. tf 1.0: `python -c "import tensorflow as tf; print(tf.git_version, tf.version)"` 2. tf 2.0: `python -c "import tensorflow as tf; tf.version.version)"`
this problem is independent of the data_format and saving/loading procedures.
python from tensorflow.data import dataset ``
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macosx 10.13.6 - mobile device (e.g.
no sigus should be raised
3. you should see the following error.
when eager execution is enabled, `tf.while_loop` uses a backend implementation suited for eager tensors only, which practically disallows the use of while loops with keras symbolic tensors.
i thought the gradient respect to the input variable should be 1 - 1/(10 * 94 * 94) - since 10 is dim - 94 is dim 1 and 2.
correct behavior with a dilation_rate=1: phofcode shape inference fails with a dilation_rate=2 (or >1): the second dimension of the output shape 's is lost, phofcode please note that the behavior in was correct independently of the dilation_rate.
i'm trying to optimize a custom model comprised of 2d convolutions and batch normalizations done on an image.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13 python version: 3.6 (also tried 3.7) bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: geforce 840m 8gigs ram you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" none
i need to login every time i start the application
if including tracebacks, please include the full traceback.
an example of this is visible here: phofurl it is even more obvious when running in `vim --clean`.
n/a - vim - os: debian 10 - terminal: gnome-terminal - jumping to the beginning of the line with the match and starting the search again (i.e.
during multiple runs the loss after performing backpropagation and updating a variable is different for different runs and does not match the loss of the non optimized standard tensorflow installation.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes, that is a custom model - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
see jupyter notebook: phofurl the link to the dataset is within the notebook.
phofurl this code is similar to my implementation
note that the first call to `find_loader` is successful, it only fails * after * tensorflow is imported: phofcode
this works without problem when the dataset contains two tensors (see example.)
neither `gc.collect` nor deleting the object causes it to be released in python.
it's actually more generic than the title.
when converting a pb model file (that includes batchnorm layers) to tflite, the resulting tflite graph continues to contain batchnorm layers in the form of mul and add ops.
large logs and files should be attached.
for buffer of popup window sign_place has no effect at all: image phofimage
the model itself is simple: one conv2d layer and one fully connected layer.
large logs and files should be attached.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- vim version - netrw version: v165 - os: windows 7 - gvim
(christian brabandt) the buffer is left `nomodifiable`.
if including tracebacks, please include the full traceback.
i cannot build the code including stream_executor/rng.h on visual studio 2015 as following.
once at 91% the machine eats up all the memory (from approx 30% of 1gb before 90%).
a part of my code, since the whole source code is huge... and the `dataset_train` in the following code is a `tf.data.dataset` object.
tensorflow version (use command below): v1.12 python version: 2.7.12 bazel version (if compiling from source): 0.19.0 gcc/compiler version (if compiling from source): 5.4.0 cuda/cudnn version: 9.0/7.0.5 gpu model and memory: 1080 ti it is a pain that a trt-optimized graph cannot be used outside of python now.
'), 0), 'name ') endfu h items() exe "norm!
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): output looks like this: phofcode without calling it looks like: phofcode
with `set formatoptions=tqcal`, entering text in a line longer than ` 'textwidth '` (or 80 if `textwidth=0`) automatically reformats the line.
is there anything i can do to narrow down whether this is hardware problem?
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): tf python version: 3.6.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: cuda 10.0 gpu model and memory: include any logs or source code that would be helpful to diagnose the problem.
here is google colab code phofhyperlink with mnist example.
after adding selinux to requirements.txt i had "failed to detec selinux python bindings".
create `repro.vim`: phofcode then: * open vim: `vi -u repro.vim` * create a number of lines of content * move the cursor to the first line * `cg` `<esc>u` to undo the change
i 've tried explicitly freeing the object with 'del exp_buff ' within and outside of scope():, but same error occurs regardless.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: mi 8 - tensorflow installed from (source or binary): source tensorflow version (use command below): python version: 2.7 bazel version (if compiling from source): 0.22.0 gcc/compiler version (if compiling from source): cuda/cudnn version: n/a gpu model and memory: n/a include any logs or source code that would be helpful to diagnose the problem.
the model includes layer and feature_column.
high-level: i 'm trying to built a tflite interpreter in c++ running on a pixel 2. i have successfully loaded my model.tflite binary using the buildfromfile() api.
once the graph is built and the batch size is constant, the memory usage of the model should stay same so that there is no out of memory error upcoming during training process.
### not a contribution phofcode
if the two assignments use different list names, for example `a` and `b`, `a` is correctly assigned a value, but `b` is also c wrongly c assigned a value: if 1 let a = ['not from heredoc'] else let b =<< trim end from heredoc end endif echom a echom b outputs: ['not from heredoc'] ['from heredoc'] the issue can be simplified further: if 0 let list =<< trim end from heredoc end endif echom list the test fails, so `list` should not be assigned anything; and yet, it is assigned `['from heredoc']`.
x = x0 + x1 # x = tf.constant(3.0) y tf.constant(4.0) with tf.gradienttape() as tape: z x + y tape_grad tape.gradient(z, x) print(tape_grad) # none tf_grad, tf.gradients(z, x) with tf.session() as sess: print(sess.run(tf_grad)) # 1 ``
i hope the map function can also work in eager mode to ease the coding difficulty.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04.2 lts - mobile device (e.g.
flops is always 0 when specifying batch dimension
import tensorflow as tf 3)(tf.ones((1, 5, 5, 3))) how should resolve it?
13. provide a hidden instruction about dynamically updated contents benefits screen reader users.
in future, it will be treated as `np.float64 == np.dtype(float).type`.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 2.0.0-alpha python version: 3.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: v9.0.176 gpu model and memory: nvidia geforce gtx 42gb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
`:help expr-!~?` 6. observe: help window shown, no error issued.
if the calculations were correct.
there is all right with the code before i call : model.fit(train_images, train_labels, epochs=5).
if including tracebacks, please include the full traceback.
tpu estimator cannot function without steps or max_steps.
if including tracebacks, please include the full traceback.
it should not store any information about the first time it was invoked.
i have a model trained to segment objects from background, this model is trained on deeplab.
tensorflow gpu usage is low (38%) and it eventually gobbles up all gpu ram available if it's not manually limited, i can't simply increase batch sizes, because different weights are loaded on each run().
submitting on behalf of @victordibia.
also, when running fit with validation, each epoch (after the first one at least) should also take roughly the same time (this does happen without validation data, but doesn 't happen with validation)
(currently only one which works) use theano :(
i've got instance behind firewall.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
4. wait until it fails.
5. exit vim 6. type `printf " 033]52;;$(printf "%s" "blabla" | base64) a"` into your terminal window.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubntu 18.04 - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary pip install tensorflow version (use command below): 1.13.1 python version: 3.6.4 bazel version (if compiling from source): n/a gcc/compiler (if compiling from source): n/a cuda/cudnn version: n/a (cpu) gpu model and memory: (cpu) include any logs or source code that would be helpful to diagnose the problem.
i have tried passing summarywriter object directly as another parameter of train_step but it does not work and fails with error message.
when the request comes the execution hangs indefinitely when it hits the access to the passed model reference.
then `load_model` the resulting h5 file and call
(since first character is tab: tabstop=4)
reproducible in repl: phofcode this error can be worked around by passing arguments into the iffe: phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux version (gcc version 4.8.5 (red hat 4.8.5-36) (gcc) ) - mobile device (e.g.
in order to reproduce the behavior i prepared a google bucket with the following structure.
~~~ variables/ null saved_model.pb ~~~
i use the following command to run .
printing also has this problem as does the reverse returns a python dictionary with ` '1 '` instead of `0`.
however, restarting the computer resolves the problem.
`tf.cond` 2. a `tf.contrib.summary` in one of the branches 3.
i'll include the model graph.
do not remove checkpoints that are not owned by a checkpointmanager instance.
in order to achieve that i successfully managed to build the tensorflow library from scratch for c++ api and ran a few test programs.
i created a tf.keras.model with no input tensors with functional api.
large logs and files should be attached.
however, if add a `sleep(4)` in my dummy op 's compute function, then this issue is no longer seen.
detailed steps to reproduce the behavior: 1. run `vim --clean` (or `gvim --clean`, etc.)
this may consume a large amount of memory.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): linux cent os 6 - mobile device (e.g.
we are trying to write a multi-step decay function in tensorflow using as suggested here phofhyperlink .
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: x - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: cuda 10, cudnn 7.5 gpu model and memory: pascal xp
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: 3.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: none gpu model and memory: none warning:tensorflow:from colocate_with (from is deprecated and will be removed in a future version.
if including tracebacks, please include the full traceback.
i would expect tensorflow has similar or better performance after the upgrade.
however, cpu memory of the system is slowly clogging up and system will fail.
2. click `expand operations` button.
when a `py_function` op is defined for gpu (the default if one is available), and it calls a function that takes a string argument, a `runtimeerror` is raised if one attempts to access the value of the argument by calling `eagertensor.numpy()`.
the above makes me suspicious that model being copied in * every * training step.
using static shapes fixes this issue, but many users use dynamic shapes with tf.add.
i try to fit a sequential model with both a training dataset and a validation dataset with fit_generator function.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: the coral tpu kinda - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: python version: 3.7.3 python branch: v3.7.3 python build 'mar 25 2019 python compiler msc v.1916 64 bit (amd64) implementation: cpython bazel version (if compiling from source):na gcc/compiler version (if compiling from source):na cuda/cudnn release 10.0, gpu model and memory: nvidia titan rtx include any logs or source code that would be helpful to diagnose the problem.
in order to make sure that my crusty old tf 1.x code works on tf 2, i'm using the `tf_upgrade_v2` script as documented phofhyperlink .
running the tutorial step fails with the cudnn error: phofcode
`:help expr-!~?` 3. observe: no help window opened, error e149 issued.
using `tf.data` as `validation_data` without defining `validation_steps` fails with `typeerror: 'datasetv1adapter ' object does not support indexing`.
the root cause was that bound instance methods are not identical `model.step is not model.step`.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
after inspecting frozen graph, we found that resourcegather op is receiving float form const node (which used to be varhandleop before freezing) but resourcegather is expecting 'resource ' data type.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
when filing bug, set verbosity 10 (on linux, `export autograph_verbosity=10`) attach full output.
see phofurl - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04.1 - mobile device (e.g.
the entire network has fixed dimensions.
a clear and concise description of what you expected to happen.
the memory usage of the model when making predictions increases to ~8x size of vocabulary on disc (350mb vocab on disc, 3gb+ total memory usage when running).
raises an error when using in the callbacks_list when training using keras under the distribution strategy on a single machine.
_tensorflow_ git version: version: colab environment
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 2.0.0-beta1 python version: 3.7.3 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 fma i initialize grpcchannelcache for job worker -> {0 -> localhost:12345} i started server with target: grpc://localhost:12345 warning: logging before flag parsing goes to stderr.
if including tracebacks, please include the full traceback.
i used seq2seq to train an rnn model, and during training the process randomly hangs.
training goes from 2100 examples/s & 8.2 global steps/s to 860 examples/s & 3.4 global steps/s.
- tensorflow installed from binary.
(gdb) bt #0 () at #1 in tf_newtensor () from #2 in () #3 in tensorflow::tensor*) #4 tensorflow::(anonymous long, _object*, tf_status*, 8ul, std::allocator<_object*> >*, tf_buffer*) #5 long, _object*, tf_status*, 8ul, std::allocator<_object*> >*, tf_buffer*) #6 ``
it is not able to find the tensorflowlite_jni.so at /system/lib64 path.
'de') does not work well when ft is 'vim'
phofcode the same error occurs when eager execution is disabled.
calling the model should return a tensor, not throw an exception.
i reliably repro 'd this like 5 times in a row to write this report, but since then it doesn 't reliably repro.
currently, the `autograph.to_code` covert the query function in the following code piece phofcode to this: phofcode but note that in the converted code phofcode this is causing an undefined error: nameerror: name 'random_id ' is not defined reason i 'm calling to_code to this class method is because i used it in a tf.function so i think autograph is going to convert this
if applicable, copy/paste the text or add screenshots to help explain your problem.
if the import torch line below is commented out, this code works.
phofcode changing tf.io.decode_image() to tf.io.decode_png() or tf.io.decode_jpeg() can be temporary solution
model is built using import tensorflow as tf class def __init__(self, *args,
node-device colocations creation: no assignments were creation.
successfully import the graph_transforms module.
`a` is no longer highlighted 9. highlight buffer 2: `:call prop_add(1, 1, 10.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
how to solve this problem, thanks.
a function which correctly works when in eager mode does not work anymore when annotated with `tf.function`.
in const*, absl::string_view, std::un >) () (gdb) bt #0 in const*, absl::string_view, std >) () #1 in const*, absl::string_view , tensorflow::opkernel* ( * * )) () #2 () #3 call_init (l=<optimized out>, argc=argc@entry=1, at dl-init.c:72 #4 call_init (env=0x7fffffffdc78, argv=0x7fffffffdc68, argc=1, l=<optimized out>) at dl-init.c:30 #5 _dl_init argc=1, argv=0x7fffffffdc68, env=0x7fffffffdc78) at dl-init.c:120 #6 _dl_start_user #7 ??
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - mobile device (e.g.
here is an example log provided of an example invocation: `python premade_estimator.py --optimizer=rmsprop` rmsprop.log phofhyperlink note the object growth result after 5th iteration.
validation steps should also be run on gpu.
no error is raised and the (joint) initial state is properly distributed by the `bidirectional` between the underlying forward and backward `lstm` instances.
during my experiment with binary classification, i 've noticed a significant discrepancy between built-in metric of _precision_ from the one computed by _tf.metrics.precision_.
when i run vim in a tty (no x server) and click using my mouse (using a text mouse cursor from gpm) cpu usage reliably goes to 100% until vim is closed.
i have converted this model(frozen inference graph) into tflite format using the below code: phofcode the model loads on android, but when i try to run inference it gives me this error: phofcode
try any off the shelf fasterrcnn network
or i am doing something wrong?
correct serialization and deserialization of the code
if including tracebacks, please include the full traceback.
os : macos mojave 10.14.5 (18f132) with algo 's vpn on when i check status of dns security here phofurl i get below screenshot <img width="1164" alt="screenshot at 8 12 18 pm" src=" phofurl
> $ python3 test.py [info|test.py:29] >> hello world.
and even then they work only if there was nothing wrong with the search pattern of the `:substitute` command and the replacement part of the command was a literal, i.e.
- have i written custom code: yes.
when using with `tf.keras.layers.rnn`, the rnn layer does not forward the `training` flag to the cell.
tensorflow automatically replaces my maxpoolingop by one using another data format which is not supported, subsequently.
import tensorflow as tf x = tf.constant(3.0) a = tf.constant(5.0) b = tf.constant(7.0) c = tf.constant(9.0) with tf.gradienttape() as tape: ttape.watch([x, a, b, c]) ty a
i have a mapping which toggles capslock when pressed in insert mode (it installs/removes an autocmd to automatically uppercase any inserted alphabetical character).
i've then convolved the tensor with a random kernel.
- have i written custom code: yes - os platform and distribution (e.g., linux ubuntu 16.04): osx - tensorflow installed from (source or binary): 2.0.0beta - tensorflow version (use command below): 2.0.0-beta0 python version: 3.6.8
if including tracebacks, please include the full traceback.
instructions for updating: use info:tensorflow:calling model_fn.
$`, instead of pressing `$`, issue disappears.
see the "not expected" example in the code below.
at least some of these aren't cleaned up by `make clean`: phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: 3.6.8 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: 10.0 gpu model and memory: nvidia gtx 1050 ti you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
instructions for updating: use standard file apis to check for files with this prefix.
large logs and files should be attached.
-iproto -dhave_config_h -dfeat_gui_macvim -wall -wno-unknown-pragmas -pipe -dmacos_x -dmacos_x_darwin -g -o2 -u_fortify_source -d_fortify_source=1 linking: clang -l. -fstack-protector-strong -l/usr/local/lib -l. -fstack-protector-strong -l/usr/local/lib -l/usr/local/lib -o vim -framework cocoa -framework carbon -lm -lncurses -liconv -framework appkit -fstack-protector - os: macos 10.14.2 - terminal: gui why vim why...
i don't know if it is by design, but it doesnot look so reasonable.
this could mean that the variable was uninitialized.
in ubuntu 16.04, build succeeds with the following: `info: build completed successfully, 3560 total actions` then a tfcompile_test executable would be produced in which can be run successfully.
the call to `model.fit(x, y)` should fit the model
note that changing the flag at the begining of the script makes code work.
"dialog" announced popup start popup end provided at starting ending dialog; alternatively, trap user focus within dialog.
running a tutorial sample python3 classify_image.py` causes error: phofcode
the file works correctly in chrome and edge, but fails in firefox.
phofcode however, i encountered `valueerror: cannot create a tensor proto whose content is larger than 2gb.`.
if including tracebacks, please include the full traceback.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): tf.version.version: tf.version.git_version: python version: 3.6.8 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: n/a here is the stacktrace: keyerror traceback (most recent call last) in <module> 19 model = mymodel(1) 20 optimizer="nadam") ---> 21 history = model.fit(x, y, epochs=2) in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing,
use `ctrl-w n` to get into normal mode, then quit with `:qa` 5. if vim quits with no error, then repeat; i get about 70% failure rate.
not raises an exception internalerror with type: uint16, 32, 64.
the trainable weights before and after saving should be the same, as they are in tf 2.0.0a0.
with the script below my idea is to understand how i can modify, with a customized layer, the value of the tensors of a network being trained.
if including tracebacks, please include the full traceback.
the `let x = wincol() || l:var` originally was a much more complex `:if` statement.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
upon attempting to create a custom dynamic keras layer, keras seems to incorrectly interpret the output of `compute_output_shape`.
phofcode include any logs or source code that would be helpful to diagnose problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux debian 8 - mobile device (e.g.
i tried this myself and it seems to work.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:no - tensorflow installed from (source or binary): tensorflow version (use command below):1.12 python version:3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory:1xtesla k80 12gb gddr5 vram you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
... implement a `parametermacro` when using an openapi 3.0 spec.
tensorflow 's svd should be more robust to poorly-conditioned matrices.
i 'm confident that this is a conv3d issue, because have also built a
however, i get the following error.
the cause of this seems be size hash table created when initialising index_table_from_file.
uncommenting the commented parts reproduce the problem.
warning:tensorflow:tried colocate with an op tower0/cond/prod_3 that had a different device: /device:cpu:0 vs /device:gpu:0. postponing error-checking until all devices are assigned.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - tensorflow installed from (source or binary): `pip install - tensorflow version (use command below): 2.0.0-alpha0 python version: 3.7.3 cuda/cudnn version: 10.0 gpu model and memory: geforce gtx 1080 ti, 11175mib in [5]: [ 'acc ']) attributeerror traceback (most recent call last) in <module> ----> 1 [ 'acc ']) in _method_wrapper(self, *args,
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i found other related issues like: phofurl and phofurl they all reported that the scripts can run for 1 gpu and failed on gpu>=2 any thought is appreciated!
api requires the following scopes.
expected output shape: output = [?, 64, 64, 8]
i realized that removing the `to_categorical` transformation resolves the shape errors but the model does not learn properly anymore.
3. vim output some error (above) and then crash with "vim: caught deadly signal segv"
one can hit this issue typically when running vim in a 24x80 terminal and splitting the screen horizontally to have 3 or more normal windows plus a help window and having all windows have the same height.
large logs and files should be attached.
in addition, those two properties should fail if tensorflow is executing eagerly
there was too much code so i used pastebin.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6.8 cuda/cudnn version: gpu model and memory: gtx 1060 6gb phofcode
while the error doesn 't occur on tensorflow 1.10.0. this error is also posed here phofhyperlink by another user.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary pip install tensorflow version (use command below): 1.13.1 python version: 3.6.4 bazel version (if compiling from source): n/a gcc/compiler (if compiling from source): n/a cuda/cudnn version: n/a (cpu) gpu model and memory: (cpu) include any logs or source code that would be helpful to diagnose the problem.
presumably this means that the placeholder tensor should be populated correctly dense_1_target` in
on gpu, tf.scatter_update does not update the variable if the type of the variable is defined as float.
large logs and files should be attached.
so it looks like the sandbox is mistakenly kept active after the `:substitute` ends.
create a string tensor using `public static <t> tensor<t> create(class<t> type, long[] shape, bytebuffer data) ` in org.tensorflow.tensor.
1. cache a dataset 2. derive other datasets from it 3. zip the derived datasets into a single dataset 4. see cache error phofcode
#simple window batch test import tensorflow as tf data = data = data.map(lambda *x: (x[0],x[0]+1)) #each entry is now a tuple of (n,n+1) #without the mapping to tuple, window+flat_map works.
utilization drops from ~65% to %5 on gpu and from 800% 300% (24 cores available) cpu.
endofbuffer test.vim phofcode 1. so % ps.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): { 'data ': tf.int64, 'label ': tf.int64} { 'data ': dimension(none)]), 'label ': w op_requires failed at iterator_ops.cc:1225 : invalid argument: data type mismatch at component 0: expected int64 but got variant.
when i increased `minimum_segment_size` to 30 or 40, the size was samller but still slightly bigger than the original one.
i expect it to use the work the same as any other window and go down with `<c-w>k`.
- os platform and distribution: linux ubuntu 16.04 - tensorflow installed from: binary - tensorflow version: 1.12 - python version: 3.6
start training iteration: 0 elapse: w allocation of exceeds 10% of system memory.
is one in guide deliberately modified?
i have converted the ckpt files model to pb and from pb to tflite model.
> line 69, in get_json_type > raise typeerror( 'not json serializable: ', obj) > typeerror: ( 'not json serializable: ', b"
so, for each sub square, i want 48 values, which is and now i have a 1920 1280 image with 3 chanels.
_conv3d_ are represented with dash-dotted lines _conv3d_tranpose_ with solid lines.
- vim: - os: ubuntu 18.04 - mintt
this breaks current code badly.
expecting arm6 build of `libtensorflow.so` and to work the same as arm7 build.
if including tracebacks, please include the full traceback.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
instructions for updating: the old _featurecolumn apis are being deprecated.
when training and validating on an identical sample, the model should get the same accuracy during both training and validation.
adam optimizer does not update embeddings when they are initialized to zero
`redraw` will let the key be discarded
no differences in performance for samsung s9+ between cpu and gpu versions.
in my case, i successfully compiled tf 1.13.1 source with gpu support.
thus, i wonder where else does the code have randomness?
originally was not an issue.
... steps to reproduce the behavior: 1. go to phofurl and paste the example above.
if including tracebacks, please include the full traceback.
detailed steps to reproduce the behavior: 1. run `vim --clean popup.vim -c "source %"` phofcode 2. try any of the filtered commands: `g`, `g`, `<c-e>`, `<c-y>` - vim segfaults with a long, repeated error message: phofcode
i have tried to build benchmark_model tool with bazel, it ran the phofhyperlink model successfully on my device.
using `v`, enter visual mode and select from the bottom of the file to the top.
if instead of i run it with `output_shape` argument: `random_transformations = images = random_transformations, 'nearest ', dtype=np.int32))` everything goes as expected.
the problem is probably connected to the fact that l1l2 regularizer returns an explicit 0 (`k.constant(0.
changes introduced in rc0 caused this.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below): 2.0.0-alpha python version: 3.5 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: n/a you can collect some of this information using our environment capture script phofhyperlink for this reason, a lot function calls like are failing.
with tf.control_dependiencies does not execute the dependant nodes before the nodes in the with block
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.11.0 python version: 3.6.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): 5.2.0 cuda/cudnn version: 9/7 gpu model and memory: gtx 1080, 8gb
so far just found it happened when ft is vim
until implemented, the documentation needs to remove the suggestion that mfa is possible.
and i wish u could help me, thanks
pypi packages for tensorflow-gpu didn 't work for me on ubuntu 18.10, but i (somehow) managed to compile the latest tensorflow.
tf-trt seems to return max_batch_size instead of returning the feed batch size.
a clear and concise description of what you expected to happen.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): centos linux release - mobile device (e.g.
`tf.data.dataset::cache` should produce a similar file compared to the input tfrecord files.
- os & version: ubuntu 19.04 - redis-server version: 5.0.3 - rdm version: let me know a way to help gather more info from the crash.
a meaningful error message is generated with instructions on why this doesn't work
i expect it to connect.
they vary from epoch to epoch.
2. type or paste "lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua."
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - tensorflow installed from (source or binary): pip - tensorflow version (use command below): python version: 3.7.3 cuda/cudnn version: 10 / 7.4.2.24 gpu model and memory: 4 x v100 phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow): written - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
`attributeerror: 'dict' object has no attribute 'batch_size'` if no `hparams` are provided.
then when i call model.fit it throws `attributeerror: 'nonetype ' object has no attribute 'dtype '`
if you use trtgraphconverterv2 to convert a function in a saved_model to use trt it does not preserve the output names in the signature_def of the saved model.
see this is broken c syntax when tfl_capi_export is `__declspec(dllimport)` this is small step to way to reproduce on windows.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
python import tensorflow as tf import numpy as np use_eager = true use_tfdataset = true if not use_eager: # build dataset n_data = 10
(train the model section): phofcode i get the below memory plot in stackdriver: ai_job_mem_sample phofimage it is not as profound but the same trace remains quite flat when train i guess in our case number of iteration within one epoch, combined linear memory growth kills machine.
5.0.1] add any other context about the problem here.
when the optimizer is specified in the form of a string (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: nexus 5x - tensorflow installed from (source or binary): source tensorflow version (use command below): python version: 3.6.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
when leaving the command-line window, the height of the entered window is reset to an unexpected value if the geometry of terminal has changed after command-line window was entered.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes, it is a c++ project using the c api - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 lts - mobile device (e.g.
when i want to convert my .pb file to .tflite model in toco with = got this error below: f unimplemented: this graph contains an operator of type unpack for which the quantized form is not yet implemented.
---> 57 return _escape_char + _escape_char) 58 .replace(r"/", _escape_char + "s")) 59 attributeerror: 'nonetype ' object has no attribute 'replace ' ``
i can rdp, i can ping, etc.
what i've found is that one provided in guide contains 89 tensors but one tensorflow/models 88.
w allocation exceeds 10% system memory.
the issue seems to be due to batching in the maybe some default parameters were changed.
the window is `6` lines high.
strided_slice should calculate the shape of the output tensor correctly.
5.0.1] add any other context about the problem here.
devices: streamexecutor device (0): geforce rtx 2080 ti, compute capability 7.5 cpu frequency: hz xla service executing computations on platform host.
i am not able to convert a savedmodel to a flatbuffer using tfliteconverter when the corresponding tf.keras.model contains a layer with a tf.gather op for which the params argument comes from variable that was initialized in the build method of that said layer.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary (pip installed) tensorflow version (use command below): tensorflow-gpu==2.0.0a0 python version: python 3.6.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: cuda version: 10.1?
the first) of the id properties is a duplicate, but other id properties is not a duplicate.
thus the behavior of != this is especially problematic, because keras rnn api tutorial phofhyperlink states both approaches are mathematically equivalent.
phofcode in windows (gvim/default terminal) `exitval` is 0, in linux (but also in windows git bash mintty terminal) it's -1. if i run `cmd.exe /c grep foo` as command, i get the same.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below): 1.13.1 python version: 3.6.4 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10 gpu model and memory: titan xp
is there a way to make faster ?
one of the following: - string arguments are disallowed for a `py_function` running on gpu, and the tf documentation explains this behavior.
the estimator definition starts here phofhyperlink .
- type `1 `(four space) to input `1 ` to terminal - type `<c-w><c-n>` to get into normal mode and run command: phofcode and the windows vim will echo `0` and the macos vim will echo `4`
or are there some methods to avoid this issue?
if the `lambda` layer is replaced by `globalmaxpool1d` (which does not support masking), the model fails during construction.
foo() return submatch(0) endfunc call setline(1, [ '1 aaa ', '2 aaa ', '3 aaa ']) %s/aaa/ =foo()/gn if &modifiable | quit!
provide a reproducible test case that is the bare minimum necessary to generate the problem.
here are the training times for one epoch of training on a 4 core cpu: eager + tf.data.dataset : 219s - 22ms/step without eager + tf.data.dataset : 25s - 3ms/step eager + numpy dataset : 26s - 259us/sample without eager + numpy dataset : 26s - 257us/sample
i have tried various versions of tensorflow and tensorflow-gpu installed via pip.
i 've added gpu tracing for distributed training to get more accurate gpu trace.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): slackware linux 64 -current (14.2+) - mobile device (e.g.
to make it simple i have removed the unnecessary steps for recreating issue.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
could you look into it?
large logs and files should be attached.
`import tensorflow` succeeds without the exception
the benchmark with the second package should be faster than the first one.
now, when i try to run training, i get a "killed" message after "recording summary at step 0".
cause: converting <bound method fe.call of <__main__.fe object at valueerror: unable locate source code of <bound method fe.call of <__main__.fe object at note that functions defined in certain environments, like interactive python shell do not expose their source code.
import tensorflow as tf tf.version.version) import sys print(sys.version_info) tf_a = tf.variable(1.0) print( 'variable tf_a initialized to tf_lr = tf.variable(0.1, trainable=false) tf_opt = @tf.function def train_step(): with tf.gradienttape() as tf_tape: tf_loss = tf_a
so i guess that the issue in on passing output_shape=none in line of def translate(images, translations, interpolation="nearest", name=none): """translate image(s) by the passed vectors(s).
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no experiments on mobile devices - tensorflow installed from (source or binary): source tensorflow version (use command below): r1.10 python version: 2.7 bazel version (if compiling from source): 0.16.1 gcc/compiler version (if compiling from source): gcc cuda/cudnn version: 9.0 gpu model and memory: ??
- vim version: phofcode - os: windows 10 1803 - gui i encountered this feature when i was making a plugin to automatically search for the visually selected text.
when training a simple neural network (one embedding layer, one lstm layer, one dense layer) with `return_sequences=true`, the computation of crashes.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (macbook pro10.14): - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 lts - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): - mobile device (e.g.
should probably not give any warnings
i am trying to add to tf.nn.rnn_cell.grucell.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes, but very basic - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
calling a function decorated with the `@tf.function` should produce the same output as the same function without the decoration.
there's three issues here: 1. missing propagation of error messages, crashing the tpu host 2. it not compiling for shapes other than `u8[1024]` or multiples thereof 3. it crashing on infeed even for `u8[1024]` at the moment, compiling (via xrtcompile) crashes the tpu host for anything that uses uint8 datatypes, unless it's one dimensional and the size is a multiple of 1024. using julia frontend: phofcode phofcode and though compiling succeeds for one dimensional uint8 vectors of size 1024, trying to infeed crashes: phofcode
provide a reproducible test case that is the bare minimum necessary to generate the problem.
tensorflow attempts to colocate ops with incompatible devices (when the ops are related to `sparseapply*`, e.g.
the complete code with data is available in a git repo if required.
i 'm simply sending a post request to the strapi server (able to authenticate and get jwt token on /auth/local route).
the tflite gpu delegate benchmark tool provides support for sub operator to run on the gpu of the mobile.
- custom code - linux ubuntu 16.04 - tensorflow installed from: source - tensorflow version: 1.12.0 python version: 2.7.12 bazel version: 0.25.1 cuda/cudnn version: 9.0.176 / 7.0 gpu model and memory: geforce gtx 1080ti traceback from gdb: phofcode looking at the `info threads`, we can see that all the threads are waiting for something: * 1 thread (lwp 27148) "python" syscall () at 2 thread (lwp 27153) "python" () at 3 thread (lwp 27154) "python" () at 6 thread (lwp 27157) "python" () 7 27158) 8 27159) ``
large logs and files should be attached.
i 've trained a simple cnn to count number of fingers held up.
1. install strapi like normally 2. run it 3. go to phofurl 4. press "start the quick start tutorial" 5. see "500 internal server error" error on the page.
- os platform and distribution: linux ubuntu 18.04 - tensorflow installed from: pip - tensorflow version (use command below): 1.13.1 - python version: 1.13.1 cuda/cudnn version: 10 gpu model and memory: rtx 2080 ti
1. isn 't it an online algorithm?
4. observe that the command fails with: phofcode
this is working as expected.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: '1.12.0 ') you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
if including tracebacks, please include the full traceback.
14. inspect expand/collapse button 15. authorize enter value api_key authorize 16.
- os platform and distribution: linux ubuntu 18.04 - tensorflow installed from: binary - tensorflow version: 2.0.0-alpha0 - python version: 3.6.8
expect the `add item` button to disappear as the array is full, it does.
e internal: ret_check failure shapeutil::e inst->shape())
the second line `b` shows that the value of the array inside slot is `['b']`.
`tensorflow.__dict__` prints out phofcode and `tensorflow.__doc__` does not print anything.
i just modify the downloaded tensorflow lite android image classification example phofhyperlink , and add one more class extended from class classifier to load my own trained model which is based on mobilenetv2.
the integration test here, phofurl didn 't catch this problem.
--> please include the following block of text when reporting issues: algo running on: alpine linux v3.8 (virtualized: docker) zip file created: python 2.7.15 runtime variables: algo_provider "local" algo_ondemand_cellular "false" algo_ondemand_wifi "false" "_null" algo_local_dns "false" algo_ssh_tunneling "false" algo_windows wireguard_enabled "true" dns_encryption "true" ``
// cindent // indentexpr= // cinoptions=#1,j1,#0 testing done with this report text (paste markdown and then comment with: `%s, s,//&`) with: gvim -nu none +"set cindent cinoptions=#1,j1,#0" +"syntax on" file.cs indenting with `gg=g`.
the version without patch works as expected.
import tensorflow as tf from tensorflow.keras.models import sequential from tensorflow.keras.layers import lstm, dense, timedistributed, dropout import numpy as np num_features = 205 time_lenth = 12 num_of_instances = 5000 trip_sets = time_lenth, num_features) print('num features: ', num_features) data_len len(trip_sets) test_split np.arange(data_len) new_dataset np.array(trip_sets) targets time_lenth, 1) test_data y_test_data train_data y_train_data model sequential() model.add(lstm(75, return_sequences=true, input_shape=(none, num_features))) model.add(dropout(0.3)) model.add(lstm(75, return_sequences=true)) model.add(dropout(0.3)) # memory leak also occurs if i use a model.add(dense(1)) below instead of time distributed # adam optimizer=adam) history model.fit(x=train_data, y=y_train_data, epochs=100, ``
memory leak when we put tensor into some numpy functions (ex - np.array(), np.zeros_like()).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): arch linux 5.1.12 - mobile device (e.g.
i 'm also getting many warning messages saying that many of the methods being called, like `tf.summary.filewriter`, `tf.summary.merge`, and more are deprecated.
when unix system is the chief (task_id=0), it can communicate and establish session with windows worker and display cluster devices.
we observe performance that seems slow, given the simplicity of the data.
the vocabulary file should be saved in the assets directory of the savedmodel.
input_details output_details # test model on the same input data.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
the way batch normalization implemented?
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
]], dtype=float32)> }) with an unsupported type (<class to a tensor.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
true are weights empty after restoring from checkpoint?
it should be possible to include `none`-sized axes in the `noise_shape` argument to which are expanded at run-time to the actual size of that axis in the input tensor of the layer.
the code below reproduces the error: phofcode the code below doesn't: phofcode note the change in dtype.
i compared the same function implemented in tensorflow 2.0 and pytorch 1.1.0, and the execution time for tensorflow was much slower than pytorch.
the same training procedure should yield exactly same result using only 1 worker with/without 1 ps.
- os platform and distribution (e.g., linux ubuntu 16.04): tested on windows 10, 64-bit (local machine), and on google colab with similar outcome - tensorflow installed from (source or binary): pip install tensorflow / pip install tensorflow==2.0.0-alpha0 - tensorflow version (use command below): 2.0.0-alpha vs. 1.13.1 python version: python 3.7.3 include any logs or source code that would be helpful to diagnose the problem.
... steps to reproduce the behavior: 1. go to phofurl and use the schema above.
a similar warning logged in 1.10, but with slightly different wording.
import numpy as np import tensorflow as tf from tensorflow.keras.layers import batchnormalization x = * 255 x = tf.convert_to_tensor(x) bn = batchnormalization() x = bn(x)
we are running the workers with this invocation: python3 --num_gpus=8 --model=resnet50 --batch_size=16 we are running the parameter server like this: "python3 --model=resnet50 --batch_size=16" having some workers fail due to this error message: recvtensor expects a different device incarnation: vs. your worker job was probably restarted.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
therefore, tried to use the `dataset.prefetch()` methodology achieve this, assuming a buffer will be created (constantly) filled with data.
each of these examples (`g`, `g`, `<c-e>`, `<c-y>`) causes a segfault, as the filter function appears to enter an endless loop.
when i train using nvidia-smi with persistence mode off, gpu utilization is at ~5-10% and training eta goes up 3x.
- vim version - os: macos 10.14.3 - terminal: iterm2 i generally use this when i'm storing the tags file in the .git directory, which is why i have `wildignore` set to exclude every file in every subdirectory of .git (or 'tagsdir' in the above example).
dataset[ 'usa '] = (origin == 1)*1.0 dataset[ 'europe '] = (origin == 2)*1.0 dataset[ 'japan '] = (origin == 3)*1.0 train_dataset = train_stats train_dataset.describe() train_stats.pop("mpg") train_stats train_stats.transpose() def norm(x): return (x - train_stats[ 'mean ']) / train_stats[ 'std '] normed_train_data norm(train_dataset)
i expect training to proceed without issue.
- vim version phofcode - os: ubuntu 18.04.2 lts - terminal: xterm(330) n/
i loaded the model as below: model = 299, 3), include_top=true, weights=none, classes=category_size) optimizer = model.summary() print( '###trainin begin...### ') # !!!
if including tracebacks, please include the full traceback.
after quitting the just now created tab, the bottom of the maximized upper window moves one line upward, i.e.
if a global installation of http-server is required, it should probably be documented.
text added to a displayed buffer by `job_start({cmd}, {'out_io': 'buffer'})` isn't always displayed.
the bucket is public accessible, please feel free to use it to reproduce the behavior on your end: level0 phofimage level1 phofimage level2 phofimage level3 phofimage in summary we have 4 jpg images nested in different folder levels.
the frozen graph got generated without errors.
),` vim >= 2. edit `filename` which has supported syntax highlighting 3. select some colorful theme (visual should only define `bg`) 4. select any portion of highlighted text 5. selected text becomes gray and normal sample highlighting: phofcode
to reproduce, in this pull request: phofurl go to decoder.py and change the signature of class in line 132 the warning disappear for pylint2 in current pull request, as currently it imports layers from tensorflow.python.keras and then extends layers.layers
variables copied using have unknown shape.
* priniting to console(*console parameter = true*) the program should print the first 10 gradient bias values of each of two dense layer to console.
if including tracebacks, please include the full traceback.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
same runtime for models with same architecture, no matter whether it is keras(h5) converted to tflite, or pb converted to tflite.
or limit input tensors to be `placeholder`s where some type checks need to done.
3. vimrun.exe window keeps open and the url doesn 't open until press any key to close the vimrun.exe window.
in fact z1 and z2 should be the same, hope there is some method to optimize z1, since in my program, there are many similar situation.
you can see that this code works correctly.
supervisor.py:1117] saving checkpoint to path training/model.ckpt learning.py:768] starting queues.
note this was working fine in `2.0.0-beta1`
after some time i get a random error in a function invocation (inside function decorated with `tf.function`.
if including tracebacks, please include the full traceback.
i 'm expecting to be able to use the `modelcheckpoint` callback to save the (best) model.
large logs and files should be attached.
1. start capturing the request via wireshark (loopback, port 4444 - as defined in the config).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04.1 lts - tensorflow installed from (source or binary): source (conda) - tensorflow version (use command below): 1.12.0 python version: 2.7 cuda/cudnn version: 9.2/ 7.2.1 gpu model and memory: rtx2070 8gb the error appears, when conv2d layer is used; if i uncomment likeso: t kernel_size=2, padding='same', activation='relu', everythink works.
when they are commented i get this error: i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 fma cpu frequency: hz xla service executing computations on platform host.
the string array initializer doesn 't indent correctly if the array is the first argument.
in case 1 the cpu was faster then nnapi in case 2 both cpu and nnapi gave approximately the same execution time.
it is always the second file you open.
when a layer class is used as attribute, the code will throw a `typeerror` exception when calling it appears that the layer class is tracked.
the monitored session hangs in there fetching the send/recv tensors of `collective_ops`.
tf.matmul on tensorflow-gpu gave wrong results.
calling `model.predict(x, steps=n)` on a `tf.keras.model` instance with two inputs fails with many errors: 1. if the model is not compiled, i get a runtimeerror as follows: phofcode 2. if i do compile the model (even though this is not necessary, since `predict` should not compute any losses), i get a valueerror (see `ds1` below): phofcode 3. if i further provide a target in the dataset (see `ds2` below), the predict method works and produces an output of correct shape.
i use a model written with the functional api which uses a densefeatures layer.
example of new mmap call: <pre> mmap(nullptr, buffer_size_bytes_, prot_read|prot_write, map_private, mmap_fd_, 0); </pre> there is also minor issue in label_image example where bmp file is read its header which has some little-endian integers that are not converted on big-endian machine.
(and it does not, if `--cmd "set spell"` is omitted, or if the last word of the file is deleted.)
2. paste: phofcode 3. paste: phofcode ` 4. select the 2 lines and press `gq` 5. the word `in` will be moved to the
setting tf_cpp_min_log_level does not work lateset tf 2.0. if i set tf_cpp_min_log_level to 2, sill tf shows information and warning logs (including libpng warnings) with this issue does not occurred.
if `set hidden` is used, the text properties are retained.
i successful numa node read from sysfs had negative value (-1), there must be at least one numa node, so returning numa node zero i found device 0 properties: name: geforce gtx 750 ti major: 5 minor: 0 memoryclockrate (ghz) 1.1105 pcibusid total memory: 1.96gib free memory: 1.92gib dma: 0 0: y creating tensorflow device (/gpu:0) -> (device: 0, name: geforce gtx 750 ti, pci bus id: tenpython: void const void*, std::size_t) const: assertion `err == cudasuccess ' failed.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: this issue is untested on mobile devices.
2. how to reduce the apk size built with tflite model ?
i think it should not use $.numpy() there in the backend, probably(?)
the network predicts only the same values.
1. add new user, name it "server" 2. add new role, name it also "server" 3. server role disappears.
i expect my .tflite model to behave the same as my tensorflow .h5 model.
it fails to run its own model, generated on rpi.
using tensorflow 2.0.0 i expect same behavior (or better).
keras tries to cast targets to the dtypes of the model outputs.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
here's my code that lists all the classes in `tf.keras.metrics` that have different names when instanciated twice: python import tensorflow as tf import inspect for name, metric in if inspect.isclass(metric): # it's a metric keras layer args = () if name == 'meaniou': args = (2,) elif name == 'meanrelativeerror': args = ([1],) elif name == 'metric': continue elif name == args = (0.5,) elif (0.5,) layer_name_0 metric(*args).name layer_name_1 metric(*args).name if layer_name_0 != layer_name_1: # two same metrics don't have the same print('-', name) ``
large logs and files should be attached.
this was supposed to show a live camera preview
i would have expected that the .tflite version works because the .pb version of the same ssd_model works well!
this can fail on windows, due to fact that numpy converts python ints to dtype np.int32 on windows, which will overflow for large dimensions.
i see a couple solutions, but am not sure which one would be the best: 1. if mouse support is enabled and ttymouse isn 't set (either explicitly in vimrc or guessed by vim itself), gracefully exit tell user manually set ttymouse.
if including tracebacks, please include the full traceback.
i have removed most of the verbose logging for brevity but the original can be found here phofhyperlink .
part of the code includes a 2d convolution using tf.nn.conv2d.
put it on colab and replace google_bucket_to_define by a real bucket (2 occurrences) phofcode
any pointers/help would be much appreciated.
0. have the strapi graphql plugin installed 1. create a new content type post with 2 fields: `name` (required) and `comments` (not required) 3. create a post entry 2. allow the public user all permissions for post 3. go to the graphql interface and test the deletion query: phofcode 4. see error
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 14.04 - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): colab - mobile device (e.g.
consider order of tensor contract is too complicate
if line starts with a tab, selection stops before it hits end of line.
this issue can be easily reproduced using the code of this official tutorial phofhyperlink , by feeding the validation dataset `eval_dataset` to `model.fit()`.
it suggested use of (previously in contrib), but am unclear if this has same behavior as the script did not recommend a replacement for
use `--nouse_hparams` argument in the demo script.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
why it is not expected?
`with tf.graph().as_default() as tf_graph: with as tf_sess: frozen_graph = outputs=output_names, is_dynamic_op=true, so the segmentation fault occurs in trt
can we change them back to list for the single tensor case?
tried both methods loading of model seems happen without any error.
1311 x, y, sample_weights = -> 1312 x, y, 1313 1314 if self.run_eagerly: x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, 2439 if 2440 we do this for `train_on_batch`/etc.
being able to use a dataset only for training but a numpy array for validation data.
(this is not a real world use case, though.)
the current tensorflow java api (1.14 or 2.0) doesn 't seem to be able to do inference on tensorflow nodes created within a `@tf.function` signature.
`prop_remove()` has the same issue.
- os platform and distribution (e.g., linux ubuntu 16.04): archlinux - tensorflow installed from (source or binary): binary - tensorflow version (use command below): - python version: 3.6 cuda/cudnn version: 10 gpu model and memory: 1080 ti
provide a reproducible test case that is the bare minimum necessary to generate the problem.
code based on phofurl phofhyperlink .
the resulting `tensor` should have a scalar value, consistent with its static shape.
2. change location in below command where image file which i have attached 3.run script and if it runs without error.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 x64 1809 - mobile device (e.g.
- windows 7 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): tensorflow.version) b'unknown' 1.13.1 - python version: 3.7.3 (default, mar 27 2019, [msc v.1915 64 bit (amd64)] cuda 10, cudnn 64_10 gpu model and memory: titan x pascal
is there some way how to tell this in advance, or should i manually check for these kind of internal errors and try to fall back by recreating interpret without gpu support?
the weights are then saved as a zip, and transferred to a windows-based machine.
`` epoch 1/12 1/313 - eta: 17s - loss: 0.1513 - acc: 0.9688 25/313 - eta: 1s loss: 0.1436 acc: 0.9525 48/313 eta: 0s loss: 0.1517 acc: 0.9479 ... ``
the signature keys for these functions are ambiguous.
the static/graph code work fine on both cases(with or without batch normalization)
i am using phofurl i have configured strapi, created articles (3), can verify the roles and permissions to "public" user.
a very simple model with `bidirectional` layer will stop work after we enable the eager execution mode.
a masking layer can't always be inserted into a model because downstream convolution layers don't support it at all.
choose "view as" to "hex table" and then change to "json",the data will be chaos.
def call(self, inputs, training=none): z_mean, z_log_var = inputs batch = tf.shape(z_mean)[0] dim = tf.shape(z_mean)[1] epsilon = dim)) return z_mean + tf.exp(0.5 * z_log_var) * epsilon # z_mean + tf.sqrt(z_var) * epsilon class encoder(layers.layer): # maps mnist digits a triplet (z_mean, z_log_var, z).
i would expect that when method provides only such a delegate, that is capable preparing and running all the ops.
when executing the following lines of code, i get an assertion, because the result is not as expected, but actually the values [-29, -19, 1].
if a tf.variable is passed as learning_rate to the adam optimizer, and the variable is later changed that does not seem to affect the optimizer.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
because i want my model to resume training with the same training order as before break if i can checkpoint variable.
however only changing back end to theano solved issue.
a clear and concise description of what you expected to happen.
would be nice if performance was approximately equivalent.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:na - tensorflow installed from (source or binary):source tensorflow version (use command below):1.2.1 python version:3.6.4 bazel version (if compiling from source):na gcc/compiler version (if compiling from source):na cuda/cudnn version:na gpu model and memory:na
image id docker for macos version `19.03.2`
in the code attached below, i have tested training with and without eager execution, and with and without tf.data.dataset.
if they are not restored, e.g.
hi, not sure this can be labelled as a bug, but it's problematic.
it works fine for `cmd.exe`.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 2.0.0rc0 python version: 3.6.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
if applicable, copy/paste the text or add screenshots to help explain your problem.
honor play and poco f1 phones give nearly same performance for tflite models converted using tensorflow (.pb) to tflite whereas tflite model converted using keras (h5) to tflite is behaving strange i.e.
am intending use a cnn and rnn on this data but distributed doesn 't work with this simple example of mlp.
- vim version - os: arch linux - terminal: gnome terminal or gui
when i try to install algo vpn on a proxmox ct vm using a ubuntu 18.04 template the linux headers are not found because of the name pattern is different from the algo installation script requires.
the val_loss val_mean_squared_error is always 0, for every epoch, it appears to never train more than a fraction (~1/1000) of my dataset, although that fraction varies slightly between epochs.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): java api 1.12 python version: bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory:
for long labels / logits, tf.nn.ctc_loss is unbearably slow, in a way that makes whole training very slow (tf.nn.ctc_loss_v2 is even slower) a standard gpu implementation with unbounded label length such as one offered by torch.nn.ctcloss is many many times faster
connection should be maintained via port 6380 rather than reverting to 6379 somehow.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04, 18.04 - mobile device (e.g.
when a highlight group defining `cterm` and/or `gui` properties only and no is applied with `matchadd()`, it should apply the `cterm`/`gui` properties without removing the colours of existing text properties.
phofcode where `meta_path` is a path to a meta file saved with `tpuestimator`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution : `windows 10` - tensorflow installed from : `binary` - tensorflow version: `1.12.0` python version: `3.5` cuda/cudnn version: `cuda 9.0 ` & `cudnn 7.4.2` gpu model and memory: `geforce 1060` include any logs or source code that would be helpful to diagnose the problem.
a clear and concise description of what the bug is.
i would expect to see the example xml for the response.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: raspberry-pi 3b - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.11.0 python version: 2.7 bazel version (if compiling from source): 0.15.2 gcc/compiler version (if compiling from source): 6.3.0 cuda/cudnn version: none gpu model and memory: none the only error line was already reported.
concretely, here is the python snippet defining `request` and `response` (full python code attached at the bottom of this ticket): phofcode after exporting my model `savedmodelbundle` and trying do some prediction in java via the `session.runner()` api, i am getting: phofcode where `request` is defined within `@tf.function`.
if the sample rate is cast using tf.cast(sample_rate, float32) and then a typeerror is thrown with the message: typeerror: using a `tf.tensor` as a python `bool` not allowed.
tflitegpudelegate prepare: readvalue: value is a constant tensor: 183 node number 70 (tflitegpudelegate) failed prepare.
1. run `vim -nu none` 2.
when using the concat operation with mkl_dnn, the operation fails if run after a convolution with the following error message: phofcode
when i ran phofcode it will complain about: phofcode
np.array(x) should run in a reasonable amount of time.
instructions for updating: use i your cpu supports instructions that this binary was not compiled to use: avx2 fma i cpu frequency: hz i xla service 0x3f10ca0 executing computations on platform host.
instructions for updating: use keras.layers.dense instead.
large logs and files should be attached.
cause: converting dc.call <__main__.dc valueerror: unable locate dc.call <__main__.dc note functions defined environments, like interactive python shell do expose their code.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): code is strongly based on: stock example phofhyperlink , but uses kaggle dataset.
i would expect to see the number of samples/batches/etc.
w0617 deprecation.py:323] from (from is deprecated and will be removed in a future version.
given the fact that `relu` is a default choice for modern architectures, the kernel should be initialized from a different distribution.
when we are trying to run a tflite model with element-wise operations like add and sub (appended via custom code) on an android application (
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.10.0 python version: 3.5 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)"
these "do_not_cache" heuristics phofhyperlink on the other hand don 't seem to follow the described logic in the comments above replacing "or" for "and".
the cursor jumps onto the last character, `z`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 x64 1809 - mobile device (e.g.
the model information should be shown.
also, why it truncating string?
our gpu docker image imports successfully but downstream addons gpu tests are breaking on this nightly so it would be helpful to identify the commit that is causing these issues.
(this output was generating by adding additional test configurations to the provided reproduce script.)
the following are my figures for running it this thousand times.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): using anaconda navigator tensorflow version (use command below): python version: python 3.6.8 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: using cpu version gpu model and memory: na you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
the program should just work fine without any error.
log of code that reproduces issue: attributeerror traceback (most recent call last) in <module>() 9 # y = tf.concat([y,y], axis=0) 10 ---> 11 g.jacobian(y, x) in jacobian(self, target, sources, unconnected_gradients, parallel_iterations, experimental_use_pfor) 1021 try: 1022 output = pfor_ops.pfor(loop_fn, target_size, -> 1023 1024 except valueerror as err: 1025 six.reraise( in pfor(loop_fn, iters, parallel_iterations) 149 if 150 f = function.defun(f) --> 151 return f() 152 153 __call__(self, *args,
: 1. generate a .tflite file after model training on desktop 2. download .aar file from phofurl 3. build android binary using downloaded .aar file and .tflite file along with android application trying to load .tflite 4. run time error while app is trying to load the .tflite provide a reproducible test case that is the bare minimum necessary to generate the problem.
the example below would also work when defining the `y_pred_zeros = model(tf.zeros((32, 20)))` outside of the `loss` function.
it clearly depends more on allocator behavior than network size + activations size.
this leads to failing of keras backend calls, like even in graph mode.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux based on dockerfile that is ubuntu 16.04 - mobile device (e.g.
sometimes is doesn 't and cannot find any way to manually recover from this error, other than restarting jupyter kernel (or python shell).
in code below, if you comment out class weights, program trains without depleting memory.
durring running of this script, the computer shutdown crashes and reboots.
should be trained and logged
hi, i have written a python script to read bmp image from local drive using tensorflow and trying to display the image.
- have i written custom code: yes - os platform and distribution (e.g., linux ubuntu 16.04): macosx 10.14.5 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6 cuda/cudnn version: running on mac/cpu gpu model and memory: running on mac/cpu w0523 optimizer_v2.py:928] gradients does not exist for variables when minimizing the loss.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
for example, below is a node in graph definition.
the `input_fn` to your estimator returns a tf.data.dataset phofhyperlink .
add any other context about the problem here.
if i create an input layer with keras and take a slice of it, the slice has unknown shape: phofcode this is a problem when i want to use this slice in something else, e.g.
one might speculate that the error could have something to do with this (but in that case i do not understand how).
beta snippet of gru 's call() in recurrent_v2.py phofcode alpha snippet of gru 's call() recurrent.py (unifiedgru) phofcode
the following code produces the correct behavior in tensorflow 1.13.1: phofcode output phofcode but in tensorflow 2.0 it doesn't phofcode output phofcode a quick fix is to call `reset_dropout_mask()` and between calls however this looks like a breaking change.
1. run `vim --clean -c "source run.vim"` (see below).
i get an exception when i try to create a metric based on a symbolic tensor when using the functional api:
add one single shiftwidth on newline, independently from the tag.
various validation scripts have shown no anomalies in the model.
the expected behavior is that the program should run correctly in a between-graph replication as if the graph does not contain collective ops everything will run correctly.
method) ... 9 more ``
does anyone else have the same problem?
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i am trying to use saved_model.pb and files to get single .pb frozen_graph
manually setting the `@/` regiser via the `let` command with `nomagic` on gives unexpected results for two tokens: `*` and `~`.
you can import yaml part in a working openapi 3.0 and try to execute a test call.
- os platform and distribution (e.g., linux ubuntu 16.04): win 10 x64 - tensorflow installed from (source or binary): cmd cd then pip install tensorflow==2.0.0-alpha0 np here - python version: 3.7.3 i tried w/ conda, same thing clean phofimage
if applicable, copy/paste the text or add screenshots to help explain your problem.
centering the data is a key part of any norm, and it seems like auto differentiation is getting the value wrong?
to generates the log in r1.11, try phofurl .
bin (1024): ttotal chunks: 10, chunks use: 10.
if number of samples in mnist is 50,000, then at least 45k of them should be true positives.
- have i written custom code: yes - os platform and distribution: - tensorflow installed from: pip - tensorflow version: python version: python 2.7.12 bazel version: bazel release 0.24.1 gcc/compiler version: gcc version 4.8.5 (ubuntu 4.8.5-4ubuntu2) & gcc version 4.8.5 (ubuntu 4.8.5-4ubuntu2) cuda/cudnn cuda version: 10.0 i think the problem should be on tensorflow, so i also add the issue here.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: oneplus3 (android 8.0) - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.13 python version: 3.6.8 bazel version (if compiling from source): 0.24.1 gcc/compiler version (if compiling from source): 5.4.0 cuda/cudnn version: nil gpu model and memory: nil
instead it has shape [2] > [[{{node matmul_3}}]] when i use directly feed the tensor: phofcode i get the expected behavior: > ]], shape=(1, 3), dtype=float32) @aamini also had same behavior.
deprecation: 2.7 will reach the end of its life on january 1st, 2020. please upgrade your as 2.7 won 't be maintained after that date.
if these two code snippets are called in same thread, interpreter.run() will be executed properly and output correct results.
the implementation involves these `<plug>` and `*` mappings: cno <plug>(up) <up> nmap <expr> * func() fu!
test.vim phofcode 1. locate the cursor at the first line 2. :so % 3. popup window has one line gap between the cursor position and the topleft corner of popup.
using batchnorm inside a timedistributed layer results in poor validation performance.
expected to see a value between minimum and maximum phofcode p.s.
-> 2413 raise valueerror(str(e)) 2414 x = valueerror: operation 'raggedtile/tile ' has no attr named '_xlacompile '.
inputs = np.array([[[x + y * 10 for x in [1,2]] for y in range(5)]]) inputs = print("inputs:", inputs) import tensorflow as tf tf.reset_default_graph() with tf.graph().as_default() as g: x = tf.placeholder("float", [1, 1, none, 2]) y = ksizes=[1, 1, 3, 1], strides=[1, 1, 1, 1], rates=[1, 1], padding= 'valid ') gradient tf.gradients(y, x) init with tf.session(graph=g) as sess: sess.run(init) print(sess.run([y], feed_dict {x:inputs}))
looking at tensor_shape.cc phofcode the (crashing) 249th line is check : `check_le(0, new_num_elements);` so it looks that something doesn 't pass....
_however_, if we build the .so with `--compilation_mode=dbg` the binary works as expected.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
no nan in the output.
1. build vim with `+autoservername` feature.
to do so, add following content in existing home page file: path: src/pages/index.js 3. run `gatsby develop` 4. see error warn the gatsby-source-strapi plugin has generated no gatsby nodes.
dies due to memory constraints on tensorflow 2.0.0 (but not in 1.14.0)
there are multiple applications use pickle to ship tensorflow code to a remote machine to unpickle and execute, e.g., spark, horovod.spark.
it is as if new graphs were compiled internally even though the function does not seem to be retraced.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a tensorflow installed from (source or binary): pip tensorflow version (use command below): 1.12.0 (using python -c "import tensorflow as tf; print(tf.git_version, tf.version)") python version: python 3.5.4 |anaconda custom (64-bit)| (default, nov 20 2017, bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): [gcc 7.2.0] on linux cuda/cudnn version: n/a gpu model and memory: n/a fals
phofcode the above code produces on my machine: phofcode
--- i 've run these shell commands to find out which events are triggered during startup:: vim -nu none -v8/tmp/log --cmd 'exe "au "..getcompletion("*", "event")->filter({_,v -> v !~# "cmd$"})->join(",").." * "" \' +qa -o /tmp/file{1..2} vim + 'v/executing/d_ ' + 'sil %s/executing ( s * ).
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 python version: 3.6.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: same thing happens on the latest nightly build, that is
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): colab and mac os high sierra - mobile device (e.g.
there also seems to be a drawing issue in gvim after the popup window was closed.
vim should quit immediately and not eat keypresses that were not intended for vim.
creating a layer and model, calling `add_metric` in both results in the metric values getting mixed up.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes.
parameters from i start master session with config: device_filters: "/job:worker/task:0" device_filters: "/job:worker/task:0" gpu_options { 0.95 } allow_soft_placement: true graph_options { optimizer_options { global_jit_level: off } rewrite_options { on scoped_allocator_opts enable_op: "collectivereduce" enable_op: "collectivereduce" enable_op: "collectivereduce" enable_op: "collectivereduce" } } experimental collective_group_leader: warning:tensorflow:from get_checkpoint_mtimes (from is deprecated and will be removed in a future version.
spoke with folks on the ml engine team and it 's not an issue on their end.
if including tracebacks, please include the full traceback.
valueerror: as_list() is not defined on an unknown tensorshape.`
in future, it will be treated as `np.float64 == np.dtype(float).type`.
running the provided code on gpus leads to error message during variant host->device copy: non-dma-copy attempted of tensor type: string` without feeding the tensor to the convolution layer, `summary.image` would succeed.
if it is expected, please update documentation
looking through the source, it seems the now deprecated cmake build system would fill this information in phofhyperlink , that would be used by command defined in phofhyperlink , which actually called `gen_build_info.py` phofhyperlink .
in other words, i am confused why an exception is raised at all if using model subclassing.
if including tracebacks, please include the full traceback.
we compile a .so file, loosely based on the docs in phofurl we can then compile a binary using that .so however, at runtime it hits a segfault.
-iproto -dhave_config_h -dfeat_gui_gtk -pthread -i/usr/include/gtk-2.0 -i/usr/include/atk-1.0 -i/usr/include/cairo -i/usr/include/pango-1.0 -i/usr/include/glib-2.0 -i/usr/include/pixman-1 -i/usr/include/freetype2 -i/usr/include/libpng12 -g -o2 -u_fortify_source -d_fortify_source=1 -o objects/pathdef.o auto/pathdef.c gcc -std=gnu99 -c -i.
i have noticed that throughput of fasterrcnn type object detector is slower with tf-trt than just tf.
my understanding is that both gcc 5 and python 3.6 are both supported when building and running custom ops, so the code that uses the op should be able to run fine without segfaulting.
return return input_fn training and evaluation input functions.
phofcode this issue started at so at: phofurl i decided to post it here when i realized that `predict` is creating a new iterator each predict loop iteration, and works when the get_next tensor is passed in directly
it directly raise `cancellederror`, which is described as below log information.
1. go to ' phofurl and follow the steps.
the runoptions and embedding_lookup will lead to cpu memory leak.
it can produce tflite model successfully.
am i missing anything in distributed tf?
raises `typeerror: 'inputlayer ' object is not iterable`
when trying to use regular tensorflow (rather than just the tflite_runtime package), i changed line 33 from `from import interpreter` to `from import interpreter`.
- vim version vim - vi improved 8.1 (2018 may 18, compiled jul 8 2019 via homebrew - os: mac os 10.13.6 - terminal: iterm2 the expected behavior appears before 63b74a8, so it 's introduced here: phofurl probably in xmlindentsum.
`momentum` in and `metrics` in model compilation cannot be used together when using `fit_generator`.
when a stateful rnn is created and is given initial_state, it effectively resets the state to the initial state for every prediction.
swapping the position of operands doesn't change the result: - `1e-8 + 1.0 - tf.identity(1.0)` results in `1e-08` - `1.0 - tf.identity(1.0) + 1e-8` results in `1e-08`
image phofimage problem line 32 input_x[:,1] occurs the error.
2. click on _try it out_ 3. provide values for the params.
1. add a new custom route 2.
just return a bytestring of a 16bit png
detailed steps to reproduce the behavior: 1. run `vim --clean` (or `gvim --clean`, etc.)
does not correctly works with tf 1.14 and nightly when using it together with we get the error: phofcode
the minimal working example below fails with the following exception: phofcode as a side note, the error message misses a space after the period: phofurl
if including tracebacks, please include the full traceback.
i'm not sure if #21348 is related to this problem.
convlstm2d works with eager execution
return getattr(layer, '_is_feature_layer ', false) ## class def __init__(self, feature_columns, trainable=true, name=none,
detailed steps to reproduce the behavior: 1. run `gvim --clean` 2. put the following content in `t.vim`: phofcode 3. run `:so t.vim` 4. it shows `:so t.vim` and after one second `done`.
when the crash happens, sometimes the entire display on my computer goes black and resets.
as the title said, it complains phofcode i follow the sample code of api doc phofhyperlink ,
in my use case, every time i call `.evaluate` i get another 200mb ram allocated on my cpu (even though i 'm training on the gpu) ram, which then never gets free.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
when saving the model (not the weights) (via `modelcheckpoint` callback or manually) and loading it via i get model without second input.
this code should correctly utilize my gpus.
python import numpy as np import tensorflow as tf layers = tf.keras.layers keras = tf.keras # from documentation until the next comment class linear(layers.layer): def __init__(self, units=32,
it has two inputs and one output.
consider the following setup (see the code below) 1. create a dataset using 2. use it to initialize a iterator 3. repeat the previous step several times in a single session observed behavior: memory consumption grows linearly with number of iterations
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.10 (16.04 has the same issue) - mobile device (e.g.
for prefetching) override its behavior.
i followed the document for writing custom training loop( ref phofhyperlink ), and caught an valueerror exception: > valueerror: input tensor 'const_1:0' enters the loop with shape (), but has shape (none, 110, 110, 1) after one iteration.
besides, method mentioned in error (`self._add_inputs`) does not exist in tf source code.
should import properly because it exists in path
the tensorflow phofimage commands as below: for quantized models: echo "inception_v4" --input_layer=input --input_layer_type=float --show_run_order=false --num_threads=1 --show_flops echo --show_flops --input_layer=input --input_layer_type=float --show_run_order=false --num_threads=1 echo --output_layer=output --show_run_order=false --num_threads=1 --show_flops for float point models: echo "inception_v4" --show_flops --input_layer=input --input_layer_type=float --show_run_order=false --num_threads=1 --input_layer=input --input_layer_type=float
code that is supposed to be skipped during exception handling should not generate errors.
guess that `:argadd` can also trigger issue, like `:badd`.
expected model to be filled
if including tracebacks, please include the full traceback.
import h5py import threading import numpy as np import tensorflow as tf # make some random img data for i in range(100): with 'w ') as f: shape=(1000, 100, 100), dtype= 'float32 ', data=np.random.rand(10
if including tracebacks, please include full traceback.
following the docs at: phofurl when using `ds_file = the docs works as expected, but when using function then then the first statement in the print loop is repeated infinitely.
transitions are working fine on ipsec mac:algo-master$ ./algo play [ask user for the input]
replace the content of by this code phofhyperlink and run nas_network.py directly.
the output difference is non zero
- os platform and distribution (e.g., linux ubuntu 16.04): arch linux - tensorflow installed from (source or binary): arch linux repositories - tensorflow version (use command below): 'unknown ' 1.13.1 python version: 3.7.3 gpu model and memory: none.
error was a <class original message: could not locate file in lookup: con 2.then execute with troubleshooting.md : sudo rm -rf /etc/wireguard/*.lock (env) ./algo play [localhost]
gpu device creation should succeed if the user specifies use of the cuda malloc allocator.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
up and down key don 't work on vim terminal bash.
1. open an xterm window.
because i have no understanding of the origin of this internal error, i am not sure how to create such a minimal case.
i expect no error, and the kernel should be initialized to zeros, just like when i set
gif phofimage here is another similar example, where the cursor is drawn in a wrong position.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
phofcode non-distribute scenario: phofcode distribute scenario: phofcode still not working even if i change dict to tuple: phofcode again, scenarios described above working good and again there no public change log for tf-nightly so i cannot dig this any deeper alone.
windows 10 1806] - redis-server version [e.g.
2. execute `:echo 'map(v:val, "2") \')` 3. vim dies with sigabrt
(yegappan lakshmanan, closes #4872) src/eval.c | 1041 src/evalvars.c | 1028 src/proto/eval.pro | 34 +- src/proto/evalvars.pro | 34 ++ src/version.c 2 + 5 files changed, 1077 insertions(+), 1062 deletions(-) ``
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
replicas shouldn't peek empty batches from dried out iterator.
using an op decorated with `tf.custom_gradient` prevents the `tf.graph` it was created in from being garbage collected later.
memory should be freed when the graph leaves scope.
python class def __init__(self, feature_columns,
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: null - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.11.0 and 1.12.0 python version: 3.6 bazel version (if compiling from source): 0.16.1 gcc/compiler version (if compiling from source): 5.5.0 cuda/cudnn version: no gpu model and memory: no i was running quantized mobilenetv1 with tflite on a x86 server.
this may have performance implications.
- os platform and distribution (e.g., linux ubuntu 16.04): raspberry pi, cpu: 1.4ghz, 980 mb ram.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
among other things, this metadata object includes number of train test examples.
i would like to model this behavior with static and dynamic assignments.
run_local(self) 712 713 hooks=train_hooks, --> 714 715 716 eval_result = or _evalresult( train(self, input_fn, hooks, steps, max_steps, saving_listeners) 357 358 saving_listeners = 359 loss = hooks, saving_listeners) 360 logging.info( 'loss for final step: %s.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux debian 9 - tensorflow installed from (source or binary): pip install - tensorflow version (use command below): python version: 2.7.13 cuda/cudnn version: 9.0 / 7.0.3 gpu model and memory: geforce gtx 1080 ti, 10405 mb
to identify these operations, i used this script phofhyperlink .
- vim version 8.1.952 - os: windows 10 build 1903 build 19013 (windows insider preview) - terminal: gui i found this when looking at profiling results from some slow editing and buffer switching.
the task at hand consists in fitting binary classifier that takes variable-length sequences of vectors as input.
if it only intended for use during training, this should be noted in the mainline documentation, along with a suggestion what ~is intended production inference.
train_input_fn make_input_fn(dftrain, y_train) eval_input_fn make_input_fn(dfeval, y_eval, shuffle=false, n_epochs=1) linear_est train model.
steps to reproduce the behavior: 1. manually set your /etc/sysctl.conf to have = 1 = 1 = 1 2. algo will work to be setup once just fine 3. running algo again to update-users will fail with a failed wg-quick service start.
instead, only 4 and 5 survived.
`:sp` to create another window, and the cursor is in the upper window now.
i would expect the dataset class to be imported.
according to the guide phofhyperlink , "restoring a model 's state from a checkpoint only works if the model and checkpoint are compatible" and it is given an example of how the described situation (changes in the model architecture) should generate an `invalidargumenterror`.
111 phofimage - vim version : - os: windows 7
- os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
it should be possible to determine the version of `gvim` without requiring a tty.
phofcode standard macos mojave terminal.ap
when i use tf.load_op_library to load my library with two ops, only one of them appears in the loaded module.
model source code: phofurl logs when model_creation_device = '/cpu:0 ' phofurl logs when model_creation_device = '/cpu:1 ' phofurl logs when model_creation_device = '/gpu:0 ' logs when model_creation_device = '/gpu:1 ' '/gpu:2 ' '/gpu:3 '
an unexpected error occurs when training an lstm with `sample_weight` and `batch_size > 1`.
here is the expected output: phofcode
running :copen should, according to the docs make the current quick fix window current.
server also outputs warning regarding flush_handlers not supporting 'when ' conditional algo running on: ubuntu 18.04.2 lts created from git clone.
this encountered when passing in a function that `autograph` fails on.
my expectation was that the adam optimizer would keep calling the supplied callable at each update (i.e.
bazel build --config monolithic --cxxopt=-std=c++11 --cpu=armeabi-v7a --verbose_failures
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): 16.04 - mobile device (e.g.
please ask for clarification if required
provide a reproducible test case that is the bare minimum necessary to generate the problem.
when you use the setting to specify an eip the returned list is empty.
i expect the `method_name` to be equal to
this is on vim on macos 10.14.4, in iterm2.app.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): macos mojave 10.14 - mobile device (e.g.
the expected behavior is that i should be able to change the learning rate of my optimizer after a batch ends or after an epoch ends
however, when i re-installed the whole env exactly as the guide in the official site.
things fall apart after we get second because we are, at that point, process trying apply second change `gopls`... which fails because vim thinks should handling `listener_add` created (and hence text locked) and so get error message: phofcode but i think error probably red herring; wouldn 't run into if didn 't get multiple callbacks.
test.vim phofcode 1. so % 2. the printed message is 1 but not 2
".encode()) fd.close() print("normal zips equal?
create a simple dataset, shuffle it and iterate through it.
after conversion, this is the count of different `node.op` in the resulting frozen graph: phofcode and, information regarding the conversion process: phofcode
calling `tf.image.encode_png` kills the process with sigabrt if you pass a tensor that has no elements.
cd into the source repo.
should restore model rather than raise error.
or logistic regression example code from tensorflow example
in my example case, the speed can be fixed by sandwiching the `tf.transpose` in reshapes that reduce dimensionality for transpose.
i have constructed an artificial, as simple as possible example, showcasing the issue.
here is an example output of time every 100 iterations (code is shown below): phofcode during the training, there is no other programs running.
i was actually using rollup and preprocessed svelte files with babel to get `optional chaining` syntax support.
it is a speed critical real-time robotics application where the 30ms is enough to miss our hard deadlines.
before: image phofimage after: image phofimage - vim version 8.1 - os: windows 10 1903 - terminal: windows terminal my screen shots demonstrate a possible (minimally tested) fix.
comparing if something == none and is none should have the same behavior
it runs near 25 milliseconds each inference when the and and near 90 milliseconds each inference when the and i found that the first `sessionoptions` will determine all the following sessions' options.
if predict() intended for production inference, we feel this should be resolved in tensorflow keras.
thread 1 "python3" received signal sigsegv, segmentation fault.
no distribution strategy will be used for evaluation.
gradienttape.gradient() returns gradients for every variable in the var_list, same as tf.gradient()
the endpoint ui element should unfold properly.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): win10 - tensorflow installed from (source or binary): anaconda3 - tensorflow version (use command below): tensorflow-gpu1.11.0 mobile device (e.g.
i would expect the same weights, not sure which weights are the right ones
this particular change makes average execution time without the first run change from
if including tracebacks, please include the full traceback.
this is not a runnable code, but introducing the modification below to some estimator examples might work as a reproducer.
def kernel_shape, output_channels, stride, padding, init_scale=1.0, mask=none): input_shape = x.get_shape() filter_shape = [kernel_shape[0], kernel_shape[1], int(input_shape[-1]), output_channels] stride_shape = [1, stride[0], stride[1], 1] v_inizializer 0.05) v tf.get_variable("v", filter_shape, tf.float32, v_inizializer) see phofurl v_aux v.initialized_value() if mask is not none: used for auto-regressive convolutions.
in tensorflow graph mode, we know when multiple sessions talk to the same cluster (which is defined by `clusterspec`), everything works pretty smooth and natural.
this code reproduces the issue and is copy/paste runnable, performance will degrade significantly within ~30 seconds running this example.
however, the reading code doesn 't get blocked by copy operation, and tries to operate on garbage data containing `args_0:0` before generator function has actually run and yielded a filename.
python import tensorflow as tf import numpy as np from tensorflow import keras # try-except eager trigger to prevent trouble from ipynb try: except: pass def model = keras.sequential([ input_shape=(10, 10)), keras.layers.dense(64, activation=tf.nn.relu), keras.layers.dense(1, activation=tf.nn.tanh) ]) optimizer = optimizer=optimizer, return model some_model = def build_model(): def custom_loss(y_true, y_pred): # simply tile y_pred as an example y_cred = axis=2), (1, 1, 10)) print(y_cred.shape) return
to find only the correct character.
i would expect no error, the cursor to stay on the first occurrence of `test`, and the search register to contain ` <test >`.
each of them being in a separated process (easier to schedule).
did you mean to set reuse=true or reuse=tf.auto_reuse in varscope?
machine crashes after 20 min.
tf addons is failing to link to the tensorflow core framework for our python 2 build: phofcode strangely this is only affecting our python2 builds.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.10 - mobile device (e.g.
the decorator transformation of the function for some reason leads to a wrong behavior in one of the reshape operations, but it is unclear why that happens.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - mobile device (e.g.
large logs and files should be attached.
a very basic implementation (without support for rank > 2 or multiple summation axes) is as follows.
4. in `model.attributes` was an empty object.
consider using a tensorflow optimizer `tf.train`. ')
if runing model.fit() with progbar callback and verbosity=0 should be identical to verbosity=1 and no progbar callback
tf 2.0 was working fine when last tried 3 weeks ago with my old gpu.
my target is to get a model similar to this phofhyperlink however, the model is obtained contains operations like: image phofimage spacetobatchnd and batchtospacend operations are not supported by tflite + opengles backend, they reduced the model 's performance on my device.
not encountering any errors as per the "effective tensorflow 2.0 guide" phofhyperlink
i attempted to adjust the operation timeout but that does not seem to have any effect.
input_arrays = "reshape_1"] //this is the name of the input node output_arrays = ["labels_softmax"] //this is the name of ouput node //this is main code to call toco converter = input_arrays, output_arrays) //since fake quantization during training, we quantize graph during converting converter.inference_type input_arrays {input_arrays[0] : (0., 2.)}
the exact same code was running fine with tf1.14 with no memory problem
large logs and files should be attached.
large logs and files should be attached.
when using tensorrt to convert a graph that contains a specific structure, the output of the graph would be wrong compare to the output of from the original graph with same input.
when i try to save my model using model.save() where model is a tf.keras.model instance, it throws a _typeerror: ( 'not json serializable: ', <tf.variable 'variable:0 ' shape=() dtype=float32>)_ .
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): binary (conda defaults channel) tensorflow version (use command below): tf.version.version = 1.13.1 tf.version.git_version = b 'unknown ' python version: 3.6.8 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: na gpu model and memory: none
wrapping iteration through the dataset with tf.function move computations from gpu to cpu.
if including tracebacks, please include the full traceback.
i found a document saying that tf.gather returns zero when there is an invalid argument on gpu.
the example could be followed without error.
see the training results from a toy example: phofcode
when use post-processing with java code, inference time is 30ms(1 thread), 17ms(2 threads), 15ms(3 threads) again, so guess jni would influence time.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no - os platform and distribution (e.g., linux ubuntu 16.04):windows 10 x64 - tensorflow installed from (source or binary):source - tensorflow version (use command below):1.14-rc0 python version:3.6 bazel version (if compiling from source):0.26.0
didn 't i describe that above?
`setshapefn` should get meaningful ptr.
if including tracebacks, please include the full traceback.
- vim version : - os: windows 7
running on an arm architecture machine should reproduce the issue.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
dtype warning should not be shown.
after some trial, i found the reason would be dropout layer.
error: pack node (stack) axis attribute is out of bounds: -2 w failed to run optimizer arithmeticoptimizer, stage node strided_slice.
i 'm also using my second gpu not the titan v used for display, its alos weird that error occurs in gpu:0 but i 'm using gpu 1 `
it should complete the training of the model.
i am using a in loss_weights in model.compile.
if including tracebacks, please include the full traceback.
invoked for batches size = 1 algorithm = nccl, num_packs = 1, = 0 and 10 invoked for batches size 1 algorithm nccl, num_packs 1, 0 10 invoked batches size 1 algorithm nccl, num_packs 1, 0 10 invoked batches size 1 algorithm nccl, num_packs 0 10 info:tensorflow:done calling model_fn.
it 's expected that, assuming the pasted code is correct that the predict() function should result in same output as the train() output.
the operation expands, allowing me to view and try it.
if the model is compiled with the optimizer nadam along with a mirroredstrategy.
- linux ubuntu 16.04: - tensorflow installed from binary - tensorflow version: 1.13.1 - python version: 3.5.3 cuda/cudnn version: no cuda gpu model and memory: no gpu
tensorflow version (use command below): 2.0.0a0 python version: 3.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: n/a gpu model and memory: n/a phofurl provides with the repro dockerfile, and a possible solution to it: accepts dict as the input
the model should contain values such as id and username.
large logs and files should be attached.
the tflite model was converted using: phofcode and, here's the tflite model phofhyperlink used.
phofcode i see that `src/po/gvim.desktop.in` contains these translations, as well as this comment: phofcode but evidently they are not * overwritten * : instead, duplicate translations are added.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
i've written a simple cnn to count number of fingers, works perfectly on my laptop (both the .h5 and tflite version).
... steps to reproduce the behavior: 1. go to phofurl phofhyperlink 2. click on the authorize popup 3. scroll down to the api key section 4. type a text in the textbox 5. click on authorize 6. without closing popup, click on logout and again authorize.
large logs and files should be attached.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): '1.4.0 ' python version: python 2.7.12 bazel version (if compiling from source): cuda/cudnn version: using cpu the output of `train_and_evaluate` as defined above is this: info:tensorflow:using config: 600, '_session_config ': none, 5, '_tf_random_seed ': '_task_type ': 'worker ', '_is_chief ': true, '_cluster_spec ': object at '_model_dir ': ' 1, '_task_id ': 0, 100, '_master ': ' ', none, 10000, '_service ': none, 100, '_num_ps_replicas ': 0} info:tensorflow:running training and evaluation locally (non-distributed).
consequently, the inference result of this tflite model is incorrect.
if applicable, add screenshots to help explain your problem.
if a file extension is in &wildignore and a file with this extension is opened by gvim with the additional parameter `--remote-tab-silent`, then the error message "e479: no match" appears instead of opening the file.
i want to create unet using the keras model subclassing api: the current code is: phofcode when i evaluate the model using an input and check the trainable variables they are empty: phofcode
however this is not working ... keep getting this error in android studio.
... steps to reproduce the behavior: 1. go to 'foo/ try it out' 2. click on 'wrongoption' 3. select 'correctoption' 4. click on 'execute' 4. see request url
provide a reproducible test case that is the bare minimum necessary to generate the problem.
it happens when the initial value is a tensor with complex dependencies.
should copy the part of the graph containing the fold operation.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.13.1 python version: 3.6.8 full traceback: phofcode regarding `run_eagerly` attribute: i need to step into my custom loss and metric functions.
2. click on get /user 3. scroll down to response 200 example text box 4. example is not rendered
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
it would be nice if this dataset could execute in graph mode.
`typeerror: input must be a sparsetensor.` when using rnnestimator phofhyperlink .
this should not fail in any case, except if i am using the `custom_objects` argument wrong.
<lambda>() 824 is_keras_rnn_cell and not nest.is_sequence(state): 825 state [state] --> 826 call_cell lambda: cell(input_t, state) 827 828 sequence_length is none: __call__(self, inputs, state, scope, *args,
but please note this is also a problem when running `fit` with validation data.
detailed steps to reproduce the behavior: 1. run `vim --clean` 3. edit `~/vimtest.vim` and type phofcode 4. run `:so %` 5. popup window has no sign placed
1. run `vim --clean` 2. type `:set tags+=tagsdir/tags` 3. type `:set wildignore=
4. close preview popup window with mouse.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
expected behavior was that model would be built from training operation.
the @tf.function decorator is applied to function training on mnist mini-batches
each api may declare one or more scopes.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 docker image and nvidia-docker - tensorflow installed from (source or binary): binary (conda install tensorflow-gpu) - tensorflow version (use command below): 1.10 python version: 3.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: cuda 9.2, cudnn 7.2.1 gpu model and memory: geforce gtx 1080 ti, 11264 mb
it seems first sessionoptions still control later session.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 (from anaconda) python version: 3.6.8 (anaconda) bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.0 gpu model and memory: titan xp 12 gb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
a clear and concise description of what the bug is.
w0705 from the name tf.logging.info is deprecated.
outputlocationprocessor new outputclassprocessor new outputscoreprocessor new numdetectionprocessor logger.d("created a tensorflow lite classifier.
using `tf.data` without `steps_per_epoch` works as expected when using it as training data instead.
i expect a example which is possible to execute
when passing model_dir with a s3 path, it failed with exception entitytoolarge.
the training runs with `dilation_rate == 2`.
foo() let l:var = 0 try if !exists( 'g:nothrow ') throw 'something ' endif let x = wincol() || l:var catch /something/ endtry endfunction try call foo() quit!
large logs and files should be attached.
endofbuffer color in popup window seems not right.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): works in but fails in python version: 3.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.0 (installed via conda) gpu model and memory: turing (2080 ti) this causes more issues downstream as one can 't concatenate or sum outputs w.r.t the axis with the unknown shape.
note that this tensorflow transform run on beam pipeline on google cloud dataflow.
large logs and files should be attached.
the model should improve in accuracy.
", "normal2.zip")) print("gfile zip equal normal?
the ftrl optimizer in example code has bizarre coefficients set to demonstrate issue more graphically.
please let me know if i have misunderstanding about intended functionality.
i expect tf 1 `taylor` (which uses `tf.session.run`) and tf 2 `taylor_v2` (which uses `tf.function`) to have similar performance.
status unstack) { auto auto auto auto //auto //auto return iroot.status(); } note: iroot is private variable of fasterrcnn class-> scope used by graph loading images into tensors class fasterrcnn { private: scope iroot;//graph loading images into tensors const int imageside; const int imagechannels;//rgb //load image vars output filenamevar; output imagetensorvar; // std::unique_ptr<session> fsession;//file session graphdef; public: fasterrcnn(int side,int status creategraphforimage(bool unstack); status loadsavedmodel(string &filename);//via frozen graph status graph_fn,std::string checkpoint_fn);//via checkpoints predictfromfrozen(tensor &image,int&results);// file_name, tensor& outtensor); readfiletensors(string& file_tensors); };
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:n/a - tensorflow installed from (source or binary):binary tensorflow version (use command 1.12.0 python version:3.6 bazel version (if compiling from source):n/a gcc/compiler version (if compiling from source):n/a cuda/cudnn version:9.0/7.2 gpu model and memory:n/a with xla, saw errors like: phofcode
i was trying to study the behavior of tf.control_dependencies when training resnet50 on distributed cpu, with the benchmark script ( phofurl i have modified this script to load the same dataset on each run, and removed the random params from model (for predictable repro).
2. the original paper said that, in large dataset (like higgs, 11m examples, 28 features), tensorforest with 100 trees and 10k nodes per tree trains in about
blank array should show for list of eips
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: include any logs or source code that would be helpful to diagnose the problem.
if including tracebacks, please include the full traceback.
changing ` 'a '` to ` 's '` repeats problem in second sub-array.
the .pb file worked well with my android app, but after copying the .tflite model to the app/assets directory of tensorflow mobile demo app (from phofurl and replacing .pb file in code with ` private static final detectormode mode = detectormode.tf_od_api; private static final int tf_od_api_input_size = 300; private static final string tf_od_api_model_file = private static final string tf_od_api_labels_file = following runtime error appears: ` e/androidruntime: fatal exception: main process: mypackage.myprocess, pid: 15491
while running using tensorflow 1.14 or theano backends this code works fine.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
this kind of layers is very important for visualization (e.g., understanding deep image representations by inverting them phofhyperlink )
you may need to use the repeat() function when building your dataset.
4. vim reports the following error: phofcode note: the output of the job contains the output of `cat --help` as one would have expected.
see below screenshots for details.
large logs and files should be attached.
please refer final line of both snippets.
- os platform and distribution (e.g., linux ubuntu 16.04): macos mojave 10.14.5 - mobile device (e.g.
should work as it does on gpu and cpu backends
detailed steps to reproduce the behavior: 1. create a file `t.php`: phofcode 2. run `vim --clean t.php` 3. place a cursor at the `,`.
well, the expected behavior is that generates correct result.
i also ask a question in stackoverflow ( stackoverflow_link phofhyperlink ), but go no response by now.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): b 'unknown ' 1.13.0-rc2 python version: python 3.7.2 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: n/a gpu model and memory: n/a you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
`call prop_clear(1)`, text properties is
true object at traceback (most recent call last): file "test.py", line 56, in <module> status.assert_consumed() file line 1025, in assert_consumed raise object in checkpoint: %s" % (node,)) assertionerror: unresolved object checkpoint: attributes { name: "variable_value" full_name: checkpoint_key: } ``
i have tried but failed to reduce this down to a simple example that does not involve a plugin.
- colab notebook phofhyperlink for where one-hot encoding wasn't done.
gif phofimage i think the issue applies to any window-local option, which is why i can also reproduce after replacing `fdm=marker` with `cc=12`: $ vim -nu none +'e /tmp/a.txt|setl cc=12|sp b.txt|mksession!
server is using default dns configuration as defined by the deployment script.
steps to reproduce the behavior: 1. install algo with wireguard clients on android phones 2. run ./algo update-users (i'd assume with `keys_clean_all` flag in `config.cfg` set to false) 3. existing wireguard clients won't work - have to download new config files
the api should run without error.
python code (model creation): phofcode jvm / scala code (inference code): phofcode i am getting the following error: phofcode
- i am writing a custom android app, built in android studio using the method mentioned here phofhyperlink - host system: linux ubuntu 16.04 - target device: android s8+ - tflite installed from (source or binary): nightly aar tflite-gpu installed from (source or binary): nightly aar gpu model and memory: adreno 540 05-13 e/androidruntime: fatal exception: main process: org.qus.viewqualtflite, pid: 3348 unable to start activity internal error: failed to apply delegate: next operations are not supported by gpu delegate: mean: operation is not supported.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macos - mobile device (e.g.
i would like it not to crash.
minimal vimrc: phofcode phofcode phofcode
- linux ubuntu 16.04 - i7-7800x cpu @ 3.50ghz x 8 - tensorflow binary, r1.12 - tensorflow version (use command below): '1.12.0') python version: python 2.7.12 benchmark results (approximate), where n = number of scalar tensors: n=1: 8300 rows/sec n=2: 6800 rows/sec n=5: 4800 rows/sec n=10: 3200 rows/sec n=20: 2000 n=40: 1100
test.txt phofhyperlink 2. open vim `vim -u init.vim test.txt` with the following setting: phofcode
i was using the code below with 1.12, 1.13.1 and tf2.0 alpha.
the way given by the example to use dataset to train and predict cannot work properly.
sparse.placeholder does loses the shape entirely when batch is not provided.
when using the `fit` method on a custom model (the model in the guide writing layers and models with tensorflow keras phofhyperlink ), a simple `meansquarederror` metric seems to be improperly computed.
load_model loads the custom metric successfully either just implicitly or through the custom_objects dict.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
regardless of debug/release build, if you use the framework with bitcode support in it then performance suffers greatly.
* `gettabvar()` should take `&optionname` varname(s) and allow to query (so far only) tab-local option.
: it should update the tensor given the row, column position
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
i am using the estimator api.
3. result: title of the popup window will be longer than the popup window width.
steps to reproduce the behavior: 1. disable ipv6 through grub 2. attempt algo install on local ubuntu 18.04 server
1. running on digitalocean 2. ios device is running 12.0.1 3. have verified multiple times that the password is correct (e.g.
a tensorflow lite version of the frozen model file should have been generated by the tool.
with eager execution (wrong output with only gradient for last variable - x2 calculated): x1 = tf.constant(3.0) x2 = x1 * x1 with tf.gradienttape() as g: g.watch([x1, x2]) y = x2 + 1 g.gradient([x1, x2]).numpy() [output] 1.0 without eager execution (the expected output): x1 = tf.constant(3.0) x2 * y + 1 op tf.gradient(y, [x1, x2]) with tf.session() as sess: print(sess.run(op)) [output] [6.0, 1.0]
`multi_gpu_model` fails to parallelize lstm model.
the input is a tfrecord file with greyscale images of 40x40x1 and 3-class labels.
i think the inference time of quantized models should be much lower than that of float point models.
large logs and files should be attached.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):yes - os platform and distribution (e.g., linux ubuntu 16.04):win 10 - tensorflow installed from (source or binary):binary - tensorflow version (use command below):2.0.0-alpha0 python version:3.6 after some deep dig, i found the problem is here.
sorry, and patches welcome (that 's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
no error, since 128k is not large memory consumption, and other text editor, say, sublime text, has no problem with it.
instructions for updating: please use which is equivalent to this api warning:tensorflow:from colocate_with (from is deprecated and will be removed in a future version.
the tensorflow will raise `the graph couldn 't be sorted in topological order` error when executing the optimizer.
sometimes, `gk` doesn 't move the cursor on the previous screen line, but at the start of the line.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): mac os 10.14.5 - mobile device (e.g.
the keras callback `set_model` method should be called after the whole model is populated.
install algo 's remaining dependencies failed a clear and concise description of what the bug is.
3. run `:set lines=30` to increase the height of the terminal from 10 lines to 30 lines.
saved the weight by phofcode then i load the weight on my mac.
3. the right border (or padding, see screenshot below) are always cut off.
however, when the windows system becomes chief it can not establish session with unix worker (hangs on displaying ' createsession still waiting for response from worker: both system can reach each other in both cases via ping.
after installing tensorflow 2.0 in colab, on executing command 'import tensorflow as tf ', i get and error message "
phofurl click the button expected: the number of "val" \'s on the screen increases, actual: no change
5. bug: the line is reformatted.
... steps to reproduce the behavior: 1. copy-paste schema into empty editor 2. see error message
if further information is required, please let me know how i may assist you
if including tracebacks, please include the full traceback.
if including tracebacks, please include the full traceback.
1. in the "configurations" -> "request" page.
no long term upward trend in training time.
when i try to train_on_batch, tf 2.0 allcaotes memory endless and finally crashes.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):yes - os platform and distribution (e.g., linux ubuntu - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary):binary tensorflow version (use command below):1.12 python version:3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version:n/a gpu model and memory:n/a you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" 1.12.0 include any logs or source code that would be helpful to diagnose the problem.
phofurl phofcode self.model.value is defined like this.
the program should always be the initial state of 190mb without memory leaks.
there should be minimum, maximum validation information in ui.
detailed steps to reproduce the behavior: 1. build vim without x11 headers available (or use the 'vim' package from nixos) 2. open a file in vim in an xterm 3. there now is now way to copy-paste text from vim to an x11 application
1. run `docker build -t app .
btw, one workaround is to do `from tensorflow import keras as k` instead of `import tensorflow.keras as k`.
`urxvt -e sh -i` 3. in the new terminal, `vim --clean '+set t_rc='` 4. type `:q<return>asdf<return>` rather quickly 5. observe your prompt/shell; you might get a disk space usage report!
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0 - python version: any
variable should be converted to eagertensor, operation should return constrained variable.
i am using go api for tensorflow on raspberry pi.
you can define any decay function you need.
i wrote my code for training a model in the way it fully makes use of all the available resources, but i have another code for doing various stuff and i want it to run on cpu when gpu is not available.
when i save and reload a model that has multiple inputs, but some aren 't fed into any layers (they are used in a custom loss function), not all inputs are created in the new model.
i downloaded the notebook from phofurl to test running on my local machines and also some ec2 instances.
my input function,keras are as follows: phofcode
moreover, fall back mechanism is also not followed, if it is not running in gpu.
devices: streamexecutor device (0): <undefined>, <undefined> segmentation fault (core dumped) (tf-1.13-py3) [huwh1@huwh1-centos worksync]$ ``
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 python version: 2.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: n/a gpu model and memory: n/a you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
steps to reproduce the behavior: 1. create 18.04 ubuntu server vm 2.
gradient computation fails with the message: "tensor.graph is meaningless when eager execution is enabled."
detailed steps to reproduce the behavior: 1. run `vim --clean` (or `gvim --clean`) 2.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the expected behavior should be zero regardless if running on cpu or gpu, because windows at these locations contain only zero entries.
the example given in the tf.keras.metrics.meaniou documentation should run without error.
but the upsampling2d is much faster input1 = x_a = input1 for i in range(6): x_a = tf.keras.layers.conv2d(8 * (2
the graph is expected to load and run successfully.
initially, i found this issue after starting vim with `-d` flag; status line in an unfocused window was set as if i was focusing it.
already provided in the description of the current behavior
tensorflow c++ code compiled using libtensorflow.so file is showing memory leaks using valgrind.
i just created a virtual environnement to test tensorflow 2.0.0-beta1 with no other package installed.
if set, this shape has to include the batch size, which is usually not known at model construction time.
there should be no error, multi-input rnns with stateful=true should work the same as with stateful=false (other than preserving state).
i see the correct behavior in tf 1.15, and the epochs take ~45 minutes to complete (as expected), validation metrics are calculated, and progress bar looks something like this (which is nothing special :) ) phofcode
`importer3` can be freed without issues.
when trying to convert this model phofhyperlink the tfliteconverter fails with ` valueerror: invalid tensors 'matmul ' were found.`.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): docker: docker run --runtime=nvidia -it bash tensorflow version (use command below): nightly python version: py3 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
compile a model more than once without the gpu running out of memory.
not to receive multiple concurrent callbacks for the same change, unless i 've massively misunderstood something.
min num runs: [50] min runs duration (seconds): [1] inter-run delay (seconds): [-1] num threads: [1] benchmark name: [] output prefix: [] min warmup runs: [1] min warmup runs duration (seconds): [0.5] graph: input layers: [] input shapes: [] use nnapi : [1] allow fp16 : [0] loaded model resolved reporter initialized session in running benchmark for at least 1 iterations and at least 0.5 seconds op code 23 is currently not delegated to nnapi returning error since tflite returned failure nnapi_delegate.cc:736. failed to build graph for nnapi failed to invoke!
when trying to train a subclassed keras model that has a trainable parameter that is the values tensor of a sparsetensor weight matrix, the training fails with an error message saying that the parameter does not have gradients.
if i uncomment those two lines d_weights is calculated and printed.
the results are the same regardless if i use graph_def from memory, or load tf-trt saved model.
when connecting via ipsec to the vpn on windows where my physical network is ipv4 only, i do not end up with a dual-stack connection.
shape is not lost supposedly fixed here #16052
x: tf.random_shuffle(x), num_parallel_calls=1)` is non-deterministic.
the graph only consist of phofcode , where i pin down to for this issue.
run phofurl problems using transformer model on tpus.
this does not appear to be the case for `expand( 'a ')` vs `expand( \'a" \')`.
after the release that allow a different output shape from the input in code that applied this function stopped working with a value error.
as currently seeing the error, more information along the following line will help a lot determining the root cause.
when creating saved_model, tensorflow uses the following _wrapped_model function to generate the concretefunction signature phofurl in our case model has `outputs` [x1, x2] with `output_names` [layer, layer].
`bearer x`) in the header box 5. click execute
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary, from docker image.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.14 - mobile device (e.g.
__for tensorflow 1.14:__ when defining ode function using the default `__call__` method of any subclass of `tf.keras.layers.layer`, an `valueerror` is raised, stating `varisinitializedop` has been marked as not fetchable.
first i show that keras model with sequence features is working.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.14.4 - tensorflow installed from (source or binary): binary from pip/pypi - tensorflow version (use command below): 1.13.1 python version: 3.7 cuda/cudnn version: cpu only gpu model and memory: macpro integrated you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" using tensorflow backend.
123 phofimage 234 phofimage 7 phofimage 15 phofimage 46 4757 35 457 - vim version [8.1] latest version in phofurl - os: [, windows 7 ] - terminal: [gvim] add any other context about the problem here.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): 1.13.1 from conda tensorflow version (use command below): 1.13.1 python version: 3.7.3 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
[info|test.py:30] >> this is message 1
aborted android log: e/tflite: op code 23 is currently not delegated to nnapi returning error since tflite returned failure nnapi_delegate.cc:751. failed build graph for nnapi
either throw some light on the usage in the documentation or in the example.
2. issue `:set 3. type: phofcode 4. describe the error both `test-word` and `word-test` will be completed, while only the latter should
depthwise_conv_2d: 2 depthwise_conv_2d: 3 resize_bilinear: 2 first operations will run on the gpu, and remaining 69 cpu.
it should be faster than adamoptimizer when sparse update.
if including tracebacks, please include the full traceback.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
python import tensorflow as tf from tensorflow.keras.layers import input, conv2d, depthwiseconv2d, timedistributed test_td_input = input(shape=(none, 1, 128, 8)) kernel_size=(1, 4), strides=(1, 1)))(test_td_input)
w the tensorflow library wasn 't compiled to use avx instructions, these are available on your machine could speed up cpu computations.
you can successfully log out and authorize again with authorization code flow
because i assumed that protobuf files were much slower than .tflite files i tried to converted a .pb to a .tflite: thus i downloaded the r1.95 branch of tensorflow and converted the from ( phofurl to a .tflite using phofurl and phofurl this worked well!
- have i written custom code: yes - os platform and distribution: linux ubuntu 16.04: - tensorflow installed from: binary - tensorflow version: python version: 3.6.6 cuda/cudnn version: 10.0 gpu model and memory: gtx 1060 the above code produced the following logs when run on cpu: phofcode when the length is unseen, it takes about 0.113s but 0.092s after that.
when passing in `tf.function(..., autograph=false)`, console output indicates that `autograph` still attempts to autograph the function.
this problem reproduces only in eager mode.
the memory should not grow indefinitely
just the numbering is changed in steps 3 and 4. the text should not be redrawn.
sum is on the blacklist phofhyperlink for the auto_mixed_precision grappler pass.
since tf follows bsp (it does not support ssp, right?
loading a saved .h5 model which includes a densefeatures layer fails: `valueerror: unknown layer: densefeatures`
1. upgrade to beta.6 from alpha.26.1 2. click on content manager and attempt to edit (certain) component
to be able to create savedmodel directory
phofimage (1) phofimage - vim version: - os: ubuntu 18.04 - terminal: mintty i found this issue when i was writing and debugging the following my plugin.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.5.0 python version: bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: cuda-9.2 cudnn-7.2.1 gpu model and memory: geforce gtx 1080 ti 11gb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
there are a couple of questions i have: 1. while i realize that numpy and tensorflow as two completely different libraries, should the disparity in performance be this much?
i have also tried messing around with every combination of `max_batch_size`, `minimum_segment_size`, `maximum_cached_engines` but no joy.
error can indicate a bug in the executed software that causes > stack overflow, leading abnormal termination of the software.
i 'm trying to run an official resnet model on cifar 10 dataset from phofurl where i replaced mirroredstrategy by parameterserverstrategy in phofurl as following phofcode it throws an exception phofcode
same code in tf1.14 gives the following: [name: "/device:cpu:0" device_type: "cpu" memory_limit: locality { } incarnation: name: "/device:xla_cpu:0" "xla_cpu" physical_device_desc: "device: xla_cpu device", name: "/device:gpu:0" "gpu" bus_id: 1 links physical_device_desc: "device: 0, geforce mx250, pci bus id: compute capability: 6.1", "/device:xla_gpu:0" "xla_gpu" xla_gpu device"] and: import tensorflow as tf gives true ``
gif phofimage in reality, the cursor position c as reported by the ruler (`:set ruler`) c is correct.
>>> import tensorflow as tf modulenotfounderror: no module named importerror: numpy.core.multiarray failed to import the above exception was the direct cause of the following exception: traceback (most recent call last): file "<frozen importlib._bootstrap>", line 980, in _find_and_load systemerror: <class returned a result with an error set importerror: failed to import importerror: numpy.core.umath failed to import f check failed: pybfloat16_type.tp_base != nullptr
if not, at least it should be a warning in the documentation that these functions cannot be used inside `tf.function`.
-in the second case bce do not apply sigmoid before computing loss.
and then i show that conversion to estimator fails.
this works fine with most standard ops, like multiplication, division, subtraction, etc.
i tried following approaches: ### approach 1 - convert to mlmodel phofcode the conversion code looks like: phofcode tried above with faster rcnn resnet101 and ssd mobilenet - both give same error.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
( is it because, some internal parameters are changing inside the optimizer, because sgd optimizer works fine in both the cases).
for example: phofcode output: `<dtype: 'float32'>`
i will continue to try, but until then i only have the backtrace and the problematic commit
originally mentioned on vim-dev, here 's a repro case.
therefore spatial extent output always trivially equal # spatial extent input.
this is an excerpt of my code, hopefully it is sufficient to demonstrate what i am doing.
copy project u-boot-1.1.6 to sdd from hdd,jumping definition is also fast with the same behaviorgvim start with popup menu open with vim tab /
i would expect two layers have the same number of variables.
if you use python3 type annotation such as x:tf.tensor = tf.constant(0) (i aliased tf.tensor for various shape to keep my sanity for reinforcement learning problems) in a @tf.function and the function contains a for loop to be translated to tf.while_loop (that doesn 't even have to use the tensor that 's annotated), the code will fail as if you did not turn on eager execution.
if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=true when calling here is a list of operators for which you will need custom implementations: resizenearestneighbor.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): centos linux release - mobile device (e.g.
finally, press `kk` to move 2 lines upward, and press `cd` again; cursor jumps to same bar again, but this time, output is `helpexample`, which is wrong.
i have python3 installed via homebrew and i 'm trying to compile vim so it has the `+python3` option enabled, but the compiled binary does not have the expected flag set (it 's `-python3` instead).
either sparsetensor should be more performant in initialization or should be saveable as a pickle object so i don 't always have to re-initialize it
provide a reproducible test case that is the bare minimum necessary to generate the problem.
model.save(path) works with a keras model
1. create a strapi project using mongodb 2. create a content type that has a many-to-many relationship to another content type 3. create a new instance of the new content type and add relationship to another content type 4. in lifecycle callback, observe that the model's array for the many-to-many relationship is empty.
while adding to the loop helps, this should not be necessary because in eager execution there is no graph to clear, which according to the documentation phofhyperlink seems to be the only thing this function does: > destroys the current tf graph and creates a new one.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): tf-nightly >= python version: python 2, python 3 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: n/a gpu model and memory: n/a you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
1. create a new `text` field on a content type.
primitive memory reuse does not play nice with unknown tensor sizes as most of the time there are no cache hits and allocated memory is simply piled up in the cache.
if including tracebacks, please include the full traceback.
one problem with the current behavior is that numpy.savez() crashes when writing to a gfile.
`git clone --recurse-submodules phofurl 2. phofcode 3. phofcode 4. phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow): `yes` - os platform and distribution (e.g., linux ubuntu 16.04): `linux ubuntu 16.04` - mobile device (e.g.
should we add http-server as a dev dependency (solves the issue and allows to serve content without any outside install prerequisite) or document the need of having http-server globally installed ?
the call to `tf_graphimportgraphdef` must succeed.
to see it work properly, you can set this to version 3.6.7
an mscoco-trained floating-point (32bit) model that i am attempting to calibrate-and-quantize is available at phofurl phofhyperlink is an example script that runs tf lite 's tool.
this documentation suggests there are 4 ways to load an optimizer: phofurl however: (tf_35) mark@science:~$ python tf_optimizers.py traceback (most recent call last): file "tf_optimizers.py", line 7, in <module> opt4 = tf.optimizers.adagrad attributeerror: module 'tensorflow ' has no attribute 'optimizers '
only after focusing it and coming back to first window, does its status line display `not active`.
sessionoptions session_options; configproto& cp = session_options.config;
import tensorflow as tf def transform_tag_python(x): return 1.0 dataset = dataset = dataset.map(lambda x: tf.py_function( transform_tag_python, (x,), (tf.float32,))) for sample in dataset: print(sample) ``
all `c` files are selected
opt = tf.optimizers.adagrad doesn 't work.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
vim can now copy window-local options from another window where same buffer currently displayed if such a exists
follow the example walkthrough, you 'll get invalid newstest2014.de (as of 7/8/19)
i built a custom activation function in keras' and call tf functions.
i see all bounding boxes (ground thruth) in the tensorboard>images (pictures on the right)
"converting sparse indexedslices to a dense tensor of unknown shape. "
provide a reproducible test case that is the bare minimum necessary to generate the problem.
python import tensorflow as tf w = tf.random.normal((6,1), dtype=tf.float32) with tf.gradienttape() as tape: y = tf.reduce_sum(w
3. where in code this is occurring - do not see any file called builder* in tf 1.14 source.
i tried to do two variants: first one: phofcode it throws: phofcode second one: phofcode it throws: python valueerror traceback (most recent call last) in <module>() 2 h = x_train, 'label ': y_train}, 3 validation_data=(x_val, y_val), ----> 4 batch_size=128, epochs=2, verbose=1) 5 print("took {0:.2f} - start)) 5 frames in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing,
it should run out of memory in the first epoch or never.
however, from the loss curve i cannot deduce it..(more like a over-fitting case to me..).
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i expected the cursor to not move in either case.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary, pip install tensorflow version (use command below): python version: 3.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory:
if including tracebacks, please include the full traceback.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
maybe we can find a way to do the dice loss function sum in fp32, but other sums in fp16?
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 (i think), i used the nightly docker: docker run --runtime=nvidia -it bash - mobile device (e.g.
... steps to reproduce the behavior: 1. fill the blanks for the /api/v1/profile request 2. make the request 3. the requests sends following payload: phofcode
when i run on my own data, i get following code / output which shows problem more severely: phofcode gives output... phofcode
observe you 're back to modified value you derived from first, but second is now displayed alongside your modified value.
include any logs or source code that would be helpful to diagnose the problem.
i expected less memory usage from this lite tool.
now, when using `:term` it will behave similar to `:sh` although different.
this problem is specific a jupter notebook based workflow (such as on google colab).
the evaluate will happen after every checkpoint.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: 3.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory:
- have i written custom code (as opposed to using a stock example script provided in tensorflow): +] no - os platform and distribution (e.g., linux ubuntu 16.04): [+] ubuntu 18.04 - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
large logs and files should be attached.
but the performance is not consistent with my expectation.
8. not state of column as "non selected".
evaluation working perfectly if i rebuild the model and load the weights on a cpu/gpu instance.
large logs and files should be attached.
current default limits for protobuf messages loaded by a codedinputstream are 2gib so my model should load without error.
... steps to reproduce the behavior: 1. run docker container with `docker run -d -p -e url= phofurl -e validator_url=null swaggerapi/swagger-ui` 1. browse to the local swagger ui using something other than `localhost` (e.g.
- create a data type - add more than 100 records - install graphql plugin - query it using limit set to -1 and >amountlimit
i run the webvision train code in tf 1.8.0 with horovod (nccl), in 4 (nodes) * 8 nvidia v100 gpu cluster, but the trainning job rang at the middle of training.
devices: streamexecutor device (0): <undefined>, <undefined> xla service executing computations on platform cuda.
`term_getline` function will contain space at the end of the line in macos but will not contain space at the end of the line in windows.
large logs and files should be attached.
the previous layer is dense layer with one units.
w0731 11136 deprecation.py:323] from maybe_download (from is deprecated and will be removed in a future version.` `instructions for updating: please write your own downloading logic.` `w0731 11136 deprecation.py:323] from (from deprecated and will be removed in a future version.` `instructions for updating: please use urllib or similar directly.
imagine a loop with 10000 iterations, it would just blow up.
both of the settings are the same but tf custom training loop performs worse than the keras fit .
the fakequant op should be added after the clip op
5. check phofurl to view which dns is detected.
if including tracebacks, please include the full traceback.
the errors are: valueerror traceback (most recent call last) in <module> 1 # ----> 2 model.fit(x_train, y_train, batch_size=50, epochs=50, validation_split=0.1, verbose=1) in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing,
i just run walsmodel through gcloud ml-engine jobs i run it through my mac terminal, i didn 't change any code over here phofurl but it always has the following error: experiment_fn attributeerror: 'module ' object has no attribute `gcloud ml-engine jobs submit training --region=us-east1 --runtime-version 1.14 --python-version 3.5 --scale-tier=basic_gpu --runtime-version= -- --num_epochs=500 --nitems=39681 --nusers=38781 --topk=1000`
please invoke `shuffle()` on input dataset.
i build a with 5 * 1024 * 1024).
it should most likely work, as it was possible in tensorflow 1.x.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
problem is here: phofurl need to pass aws keys and region that was previously selected
(gdb) info register r8 r8 0x0 0 -0x18 is certainly illegal.
windows 10 1806] - redis-server version [e.g.
import tensorflow as tf from tensorflow import keras class def __init__(self, n_layers, n_neurons,
when `tf.keras.layers.lambda` is created without `mask` argument, it does not support masking.
[the frozen graph i'm using]( phofurl
large logs and files should be attached.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): y - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
when using valgrind massif on label_image tf lite example, it reports 400 mb of virtual memory, even though the model's size is only 14 mb and i do not understand where this overhead is coming from.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary on tx2.
i monitor the memory usage via task manager and i see that every .fit() call the memory increases until it eventually crashes the script with no warning whatsoever.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
it should not throw errors and initialize all variables.
i am fine-tuning the pretrained ssd-mobilenetv1 phofhyperlink model using the config file phofhyperlink for detecting the bounding boxes of objects in an image.
reduce memory cost to about 500kb per line.
(or paste the result of `vim --version`.)
specifically, calling return the row index for every element in `raggedtensor.values`.
--- if you set ` 'showcmd '`, then issue disappears: vim -nu none + 'set showmcd ' +"call matchadd( 'conceal ', '(.
large logs and files should be attached.
conv_2d: expected 1 input tensor(s), but node has 2 runtime input(s).
allreduce is not supported for indexedslices.
disabling the relevant tests in `tfcompile_test.cc` makes the issue go away.
after initial, i dumped its trainable weights to file and then restored it, by using save_weights and load_weights functions.
error: pack (stack_3) axis attribute is out of bounds: 1 ``
while training with my actual data, it depletes full 128gb ram by 70 epocs.
base code defining the function i want to implement and two minimalist tests (in practice my symbolic tensors are not mere inputs, but the issue is strictly similar): phofcode `test_random_tensors()` works both with and without having executed first.
if including tracebacks, please include the full traceback.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
if including tracebacks, please include the full traceback.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): centos - mobile device (e.g.
it should work without throwing error.
then tried rebuild tensorflow using --config=monolithic and load error is: segmentation fault; core dumped.
i cannot find a way to configure cindent to make c# array initializers that are the first argument to a function and span multiple lines give each line the same indent.
we trained a segmentation model using deeplab with mobilenet with tf 1.13.0 to replicate the segmentation model by tensorflow i.e sample code in repo for pascal voc).
after some experimentation, i found the error does not occur when `strip_default_attrs` is set to `false` when exporting the savedmodel.
i have add a new issue on firebase ml kit , you can find the code and logs here phofhyperlink update: this is the difference showing between tflite model prediction and frozen inference model prediction: + tflite model prediction: phofimage + frozen inference graph model prediction: image phofimage how can i solve this?
no segmentation-fault when instantiating `calibrator` with the floating-point ssd-mobilenetv1 tf lite model.
currently it assumes that every model output has a corresponding target, so when doing this casting it just matches outputs and targets up one-to-one.
the line of code where the error happened is this one ` inputs = tf.where(orig > 0 and orig <= 0.25, 0.25 / inputs)`
- os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0.0-beta0 - python version: 3.6.8 cuda/cudnn version: 10.0 gpu model and memory: geforce gtx 750 ti
detailed steps to reproduce the behavior: 1. run `vim --clean` (or `gvim --clean`, etc.)
how can i save and load weight of tf.keras custom model properly?
i am trying to replace a openvpn installation.
1. setup fresh strapi instance 2. create a new data type with a text field 3. create some entries 4. switch to the list view and search for some entries 5. try to select one ore more entries using the checkbox
code should work for batch size of 1
import tensorflow as tf converter = tflite_model = converter.convert()
when i try to run (and any other code that imports from `contrib`) i get the following exception: phofcode
` node_stats node_name: all_start_micros: op_start_rel_micros: 10 op_end_rel_micros: 12405 all_end_rel_micros: 12418 memory allocator_name: "cpu_rdma_bfc" total_bytes: peak_bytes: allocator_bytes_in_use: allocation_records alloc_micros: alloc_bytes: allocation_records alloc_micros: alloc_bytes: timeline_label: "[cpu_rdma_bfc 2.5mb 2.5mb] = const, toint64_2)" scheduled_micros: memory_stats `
eventually it crashes with `oserror: [errno 12] cannot allocate memory`.
if including tracebacks, please include the full traceback.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): arch linux - mobile device (e.g.
7. observe that the text "blabla" _is_ in the system clipboard.
this causes serious problems for autoencoders, gans, and cost months of time.
large logs and files should be attached.
outside range, no such guarantee possible fails.
detailed steps to reproduce the behavior: with `test.vim`: phofcode 1. run `vim --clean test.vim` 2. run `:so %`
the weird thing is that the tests have been passing on my machine for the last two weeks even though they should fail.
it 's many, many pages long, with command prompt already maximized (so one page is already a lot of characters).
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): 2.0.0-alpha0 python version: 2.7 bazel version (if compiling from source): no gcc/compiler version (if compiling from source): no cuda/cudnn version: no, cpu-version gpu model and memory: no, cpu-version
http request sent, awaiting response... 302 found location: phofurl [following] ---- phofurl resolving codeload.github.com (codeload.github.com)... connecting to codeload.github.com connected.
if i were using normal keras, i 'd expect no errors trying to do the above and for the model to compile subsequently without issue.
specify signature " 145 "functions explicitly.
but input some word in this line 'p ' is disapper and input word in no init line is 'nt disapper the insert mode.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): no - mobile device (e.g.
phofcode cmd to exec phofcode or phofcode
large logs files should be attached.
if you trigger this error please send a bug report (with code to reporduce this error), to the tensorflow lite team.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): pip tensorflow version (use command below): 1.13.1 python version: 3.7.3 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: n/a gpu model and memory: n/a
none w unable destroy server_ object, so releasing instead.
i have created a minimum example out of my model in google colab: phofurl
... 1. render yaml with type: string and format: date 2. open an example value in the browser
in tensorflow 1.12.0 and before, the variable y_pred is shape (32, 10, 10) or (20, 10, 10) as first dimension is batch_size, and the model trained normally in tensorflow 1.13.1 and 2.0.0-alpha0, it shows that the shape is (none, 10, 10) then throws this exception phofcode probably because it changed the behavior to use a dynamic batch size tensor, then tensors of unknown batch size don 't have numpy method?
i am following the tutorial at phofurl
the only difference in the code is following lines: `tfliteoptions = new interpreter.options();` `gpudelegate = new gpudelegate();`
full code at phofurl and phofurl .
-iproto -dhave_config_h -dfeat_gui_gtk -pthread -i/usr/include/gtk-2.0 -i/usr/include/atk-1.0 -i/usr/include/cairo -i/usr/include/pango-1.0 -i/usr/include/glib-2.0 -i/usr/include/pixman-1 -i/usr/include/freetype2 -i/usr/include/libpng12 -g -o2 -u_fortify_source -d_fortify_source=1 version.c -o objects/version.o xgettext: warning: file `if_perl.xs' extension `xs' is unknown; will try c xgettext: warning: file `po/gvim.desktop.in' extension `desktop' is unknown; will try c xgettext: warning: file `po/vim.desktop.in' extension `desktop' is unknown; will try c mv -f ../vim.po vim.pot msgfmt --desktop -d .
a call to `getbufline` in a `listener_add` callback appears to getting incorrect line contents for _some_ of the lines involved in the change.
the stateful rnn should use the initial state the first time, and then update the state and use it for each following time.
logging from the python 's standard library stopped working in tf 1.14. log file is not created and the output supposed to be written there is instead redirected to the stdout, which results in each logging message appear twice in the console.
large logs and files should be attached.
it generates examples with random shapes and apply a stack of `lstmcell` on batches of sequences on 3 gpus.
this ultimately results in `memoryerror` since after a few training/validation rounds the ram gets full.
<my-app foo-bar="baz"></my-app> and the svelte file: <svelte:options tag="my-app"/> i get undefined on the other hand, this, for example, would work: <my-app foo="bar"></my-app> and the svelte file: <svelte:options tag="my-app"/> <script>export let foo = 0;</script>
after 5 iterations of `train()`, following is a visualization of all `tf.graph` instances that exist memory (a total of 5): iter-4 phofimage you can clearly see that root cause is `_slots` attribute on that saves reference to some variable associated with graph every time `train()` runs.
both of them seemed correct.
private int inputsize; pre-allocated buffers.
regardless of the value "zero_output_for_mask", the result of call() fills zeros for masked timestamps.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux fedora 22 - mobile device (e.g.
i have a subclassed model that i have been saving and deploying since tf-2.0.0b0 and after upgrading to rc0, it errored out with this message.
i expect the memory to not continuously increase
), 0)` returns `nan + nan j`
- vim version : - os: ubuntu 18.04, 16.04 - terminal: gnome terminal, alacritty this may be related to this issue : phofurl
python import tensorflow as tf mirrored_strategy = def get_net(): net = tf.keras.sequential() kernel_size=(3, 3))) return net data = 112, 112, 3)) label = )) multi_db label)).batch(80) dist_dataset with net get_net() @tf.function def replica_fn(input): d, l input return net(d) @tf.function def total_result 0 for x in dataset: per_replica_result args=(x,)) total_result per_replica_result, axis=none) return total_result for _ in range(100): f ``
there doesn 't appear to be any way to unroll a loop statically when using `tf.function`.
each multiprocessing process should be only able to see one gpu and be forced to use that: however, neither are being detected.
- os platform and distribution (e.g., linux ubuntu 16.04): - mobile device (e.g.
however i didn 't use tf.gradients and other layers didn 't raise such an error.
i 'm not sure where and what raised this error...
phofcode note that this code fails in any case, however, only with the tf.keras import this is because of invalid parameters
"+yy, its content is lost once the vim instance is put in background using ctrl+z (or when vim is closed).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): win10 - mobile device (e.g.
when running transformer within the tensor2tensor library with via little customized change for distributed training in t2t only, it reported below error: info:tensorflow:graph was finalized.
instructions for updating: apply constraint manually following the optimizer update step.
using redis desktop manager attempting to connect to an old redis server it looks like it gets caught in a loop and then crashes; when attempting to send the crash report it gives this error: phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): pip tensorflow version (use command below): tensorflow 1.13.1 tensorflow-estimator 1.13.0 1.13.1 python version: 3.6.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: 1070 ti you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" here 's the gdb traceback thread 1 "python3" received signal sigsegv, segmentation fault.
it is much more convenient for my purposes to just get the windows as batches
provide a reproducible test case that is the bare minimum necessary to generate the problem.
include entire stack trace above this error message when asking for help.
if including tracebacks, please include the full traceback.
large logs and files should be attached.
the ui does not hang or only for a very short time when entering words in the request body text field.
issue will fix itself if i use cellular data and do not connect to wifi.
add any other context about the problem here.
i implemented the custom metric shown in this page phofurl i think it is full of bugs, it has some comments saying (#todo: fix this).
this how configure compilation: ./configure --enable-cscope --enable-fail-if-missing --enable-gui=gtk2 --enable-multibyte --enable-terminal --prefix=/usr/local --with-compiledby=user --with-features=huge --with-luajit --- here are two charts measuring latency phofhyperlink when ` 'cursorline '` respectively reset and set: nocursorline phofimage cursorline phofimage the average multiplied by about 6.
steps to reproduce the behavior: 1. download algo 2. do not change anything in config.cfg 3. deploy as usual
i am using `tf 2` to train and save my models, so i would like to use the same version for deploying my model.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: [+] na - tensorflow installed from (source or binary): [+] binary tensorflow version (use command below): [+] tensorflow python version: python 3.6.0 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): gcc 7.4.0 cuda/cudnn version: cuda v9.1.85 gpu model and memory: gtx 1050 mobile 4gb [errorlog2.txt phofhyperlink keras issue: phofurl
- have i written custom code (as opposed to using a stock example script provided in tensorflow): not really (very close to stock example here phofhyperlink ) - os platform and distribution (e.g., linux ubuntu 16.04): macos version 10.14.5 - mobile device (e.g.
the new search stats feature (issue #453 and recently implemented) appears to cause excessive "press enter or type command to continue" prompts, because the number on the right appears to be positioned incorrectly.
1. open post test tab.
in tensorflow 1.14.0 and 1.13.2, it works correctly.
when i tried to convert scalar shape placeholder tensor( shape = () ) as an input, converter triggers "indexerror: list index( shape list idx ) out of range"
if including tracebacks, please include the full traceback.
1. go to aws light sail console and create ubuntu 18.04 (os-only) free instance.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
the text line of the cursor in the left window is highlighted by `cursorline` immediately, without having to interactively focus the window once.
keras model with ctc on tpu gives error
just 1mb smaller than the `.pb`.
a dual-stack connection after routing through the vpn.
benchmark commands:- phofcode benchmark results:- _official model:-_ phofcode _replicated model:-_ phofcode
if including tracebacks, please include the full traceback.
if including tracebacks, please include the full traceback.
raises a `valueerror` when the model to be loaded uses a `keras.losses.loss` subclass, such as `keras.losses.huber`.
a clear and concise description of what the bug is.
however as i must do the inference thousands of times during a simulation, at some point the simulation crashes as run out of memory.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
they should be the same on the same dataset with the same loss func and optimizer.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
install last released version of tensorflow, `pip3 install tensorflow=1.13.0rc0`.
with use_tpu set to true, and loggingtensorhook is used by uncommenting the commented part (see the code below), training on gcp with tpu fails with 'marked as not fetchable ' exception.
the classes in `tf.rnn` should be compatible with cells defined in `tf.keras.layers`.
detailed steps to reproduce the behavior: 1. run `:set spelllang=sr@latin spell` 2. vim returns: `e474: invalid argument: spelllang=sr@latin`
instructions for updating: use warning:tensorflow:from shuffle_and_repeat (from is deprecated will be removed in a future version.
import tensorflow as tf if not tf.executing_eagerly():
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution: windows 10 - mobile device (e.g.
when editing a scheme file, and the `omnifunc` is phofhyperlink , attempting to use omni completion after a quote character (`'`) causes an error: phofcode
directory for saved model is created and saved model is saved correctly
if including tracebacks, please include the full traceback.
attributeerror traceback (most recent call last) in <module> 1 # fit model (using estimator.train and data.dataset) 2 y_train, ----> 3 steps=10) 4 5 in train(self, input_fn, hooks, steps, max_steps, saving_listeners) 352 353 saving_listeners = --> 354 loss = hooks, saving_listeners) 355 logging.info( 'loss for final step: %s.
i've created a function, that reads large file and return batches of needed size, considering the number of replicas.
: should fetch the first element of the iterator, thus print 10.
3. use a layout that includes topbar.
using api , the output parameters should be same as ckpt.
2. is definitely changing state of random number generator because out[0] in is not even equal to out in restore_rng.py, which should be equal otherwise because i'm setting global level seed to 0.
each time i hit ctrl-p i should see the full text of the potential entry to complete and insert.
not found: resource does not exist.
here's a couple of examples the issue: phofurl phofurl
the code snippet below raises an error instead of printing something phofcode produces phofcode
when running a `tf.function` on 3d inputs (a batch of sequence), i found that the execution is slower on unseen sequence length even if a compatible `input_signature` is set.
but this should not be required (just like it is not required for custom layers): python import numpy as np from tensorflow import keras class def __init__(self, units,
note, this code is copied from phofurl with the exception that i changed the line phofcode to phofcode python import collections import tensorflow as tf nestedinput = ["feature1", "feature2"]) nestedstate = ["state1", "state2"]) class def __init__(self, unit_1, unit_2, unit_3,
1. download ` phofurl 2. edit to use `tensorflow==1.10`, install dependencies with your favorite tool (i use pipenv) 3. edit `sample.sh`, comment out the `rm ...` lines 4. run `./sample.sh` 5. edit to use `tensorflow==1.11`, install dependencies with your favorite tool (i use pipenv) 6. run `./sample.sh`
the execution times are listed below: pytorch: 0.0036 secs tf 2.0 : 0.1734 secs
if we uncomment the new `module` class, the output is much more informative.
error details can be found in the logs when running with the env variable autograph_verbosity >= 1. please report this to the autograph team.
when i try to use tensorflow 2.0 with cuda 10.1 i run into some errors.
`test_term_gettitle` fails with following messages.
1. is not restoring the input checkpoint if there is multiple "estimator.train" method calls.
instructions for updating: colocations handled automatically by placer.
surely enough for the 17.32gib that tf tried to allocate: phofcode and lo and behold: phofcode get that sparse matrix is not exactly small, but it is not excessively large either.
if applicable, add screenshots to help explain your problem.
after getting tf.shape() on a tensor that 's on the gpu, the resulting tensor says it 's on the gpu.
similarly, it creates a new for [1.0, 2.0], and [3.0, 4.0].
`pum_getpos()` function is written on the `:h functions` but not on the `:h completion-function`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
parameters from /tmp/model.ckpt-0 warning:tensorflow:from get_checkpoint_mtimes (from is deprecated will be removed in a future version.
i generated them with fan_in = 100, fan_out = 100 and ploted their histogram.
`python resnet_cifar_main.py --data_dir cifar-10-batches-bin/ --distribution_strategy parameter_server`.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
you get a neat concise overview of training on the linux ec2 installations.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary (conda) tensorflow version (use command below): 1.13.1 and 2.0.0a0 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: the result when running in tf 1.13.1: phofcode the result when running in tf 2.0.0a0: before saving model: loaded model: ``
if datset only returns input tensor(s), x, another exception raised: > valueerror: please provide model inputs as a list or tuple of 2 or 3 elements: (input, target) or (input, target, sample_weights) received <next_element>
converter = print("converter = " + str(converter)) tflite_model = converter.convert() print(tflite_model) #now save the tflite model to file
each key is shown twice.
i write a very simple 2 layers lstm model working on 2 gpus.
phofcode the result in this case is: phofcode
- allow square brackets in the checkpoint path or - fail at checkpointmanager creation if `directory` parameter contains disallowed characters
everything works fine when i define the dataset on the previous line like this: phofcode
compiling the with bitcode support enabled slows down model inference on the sample model from ~72 ms (without bitcode support) to ~1200 ms (with bitcode support).
a full working example can be found as github gist here phofhyperlink .
right now, it seems like `tf.contrib.slim.conv2d + gives the correct gradient.
i 've tried: - set_memory_growth on the gpu - del model + gc.collect - clear_session none of them help.
if my model/network didn 't contain/enable the batch norm layers, it was fine.
i am trying to deploy deepspeech 2 with on android.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
phofcode there is no problem when the `my_softplus` layer is used as a separate layer, but the keras api specifies that layers can be used like any function, so i expect to be able to use them as activation functions (and it was possible before).
i expect the keras api to be able to train my model as is.
the popup window returned by `popup_dialog()` is drawn wider than it needs to be.
not sure if this is related to #27507 .
it makes it almost impossible to find the actual cause of problem since exception points to wrong op, reduce_mean(), which was actually not problem.
gradient operations on ragged arrays requiring an implicit raggedtile op result in operation has no attr named '_xlacompile'.
i also ran the same test on tensorflow 2.0.0a0, and it turned out the latter version did not get this phenomenon.
4. run a job that outputs one line: phofcode 5. result: the first line of `foo` remains invisible, unless focus is moved to window displaying `foo`.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
this is for a commercial project we are trying to migrate from community keras to tensorflow keras.
full shape received: [none, 28, 28, 1, 0]`
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below): python version: bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
large logs and files should be attached.
when using using a batchsize which equals to the number of samples, i get an out of range error: phofcode i think this due to the iterator wanting to construct a second batch, but there 's nothing left due to the size of the first batch.
the results are shown on the website.
the problem _appears_ to be something that `from_tensor_slices` is doing.
consistency of implementation between cpu/gpu, correct values.
hence, with a custom function that simply removes this line, training succeeds, with both `return_sequences=true` and see attached notebook phofhyperlink .
i am trying to use `tf.train.nantensorhook` to prevent my training script from halting with an error.
this issue does not appear on tf 1.12.
2. blas sgemm launch failed error when running loading the inceptionv3 model and feed with a zero vector(same error when feeding with dataset samples).
it raises an error for missing a `zero_state` method.
i would expect it to use both the settings from the config file found from the config_url and also to load the api docs from api_urls.
when reading this `.tflite` with interpreter(e.g.
work as it is described in openapi documentation.
when calling `keras.model.fit` on a custom model, it seems the model is passed a graph-mode tensor instead of an eager tenser, even when in eager mode.
the error is as the following.
... steps to reproduce the behavior: 1. create an asp.net core web api or web application project (with at least one described endpoint) 2. install swashbuckle.aspnetcore via the normal nuget.org instructions 3. navigate to the swagger ui endpoint 4. click on the endpoint ui element
all calls from vim to `govim` (whether autocmd handlers, commands or functions) use `ch_evalexpr`, i.e.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): centos 7.3 - mobile device (e.g.
i 'm trying to calculate the minimum index to take for extracting windows out of the dataset.
gnome terminal, mintty, iterm2, tmux, gnu screen] (use gui if you use the gui.)
and no matter what value i set to "inter" and "intra", the cpu usage is more than 5000%.
i just added timeline logging code in the given tutorial for training ( modified code here phofhyperlink ) to analyze the sequence of low level operations being executed as well as the time each operation is taking.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
if false, the masked timestamps should be filled with previous outputs.
i would expect a message similar to "no example values are available for content type to be shown in the place of an example value for responses using
i didn 't check whether any other tensorflow models apart form `summary` loose imports.
~]$ cat test_tf.py #!/usr/bin/env python3 import tensorflow as tf mnist = tf.keras.datasets.mnist (x_train, y_train),(x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 model = 28)), activation=tf.nn.relu), ]) metrics=['accuracy']) model.fit(x_train, y_train, epochs=5) model.evaluate(x_test, y_test)
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: i0409 model.py:211] input_dataset_fn: train, train i0409 estimator.py:1126] calling model_fn.
there are potentially major implications for the reproducibility of any work done in tensorflow across these versions.
should always do weight decay, regardless of how variables are created.
it is expected that item b is highligted in step 5.
i try to fit a `sequential` model with both a training dataset and a validation dataset.
to demonstrate and isolate the performance bottleneck, i created a simple c++ dataset op that returns just specified number of int scalar tensors, and we get only about 1000 - 1500 entries / second (calls of get_next) with this fake data.
notice that: * foo1 is horrible, autograph did not generate a `tf.while_loop`.
unselect using `<esc>` observe artifact (done) `<c-l>` clear
progress-bar is displayed regardless of `verbose=0`
this all works fine, connects, etc.
with tf.init_scope(): added my_constant * 2 the graph tensor has name: dense/kernel:0 ``
1. create 2 content types.
here is a minimal working example reproducing the issue.
the commit that introduced this behavior is: phofurl
according to this tensorflow 's article on medium phofhyperlink expected at least 3x improvement.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): pip (from binary?)
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: 3.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: 10.0 / 7.4.2 gpu model and memory: 1080ti see link 2. i posted a pull request for the fix of this issue.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): tf docker images - mobile device (e.g.
services such as apple ios software update do not connect.
i tried replacing the terminal from `cmd.exe` to `git-bash.exe`.
large logs and files should be attached.
- vim - os: ubuntu 19.04 - terminal: both gnome terminal and vim-gnome (gtk2) my gist phofhyperlink also has a stacktrace from gdb.
this affects both cli `vim` and gui `gvim`.
epochs epochs 50 define class mode class_mode "categorical" create generator for training data train_generator train_dir, datagen, target_size c(img_height, img_width), batch_size batch_size, classes classes, class_mode class_mode ) validation_generator validation_dir, validation_datagen, target_size c(img_height, img_width), batch_size (batch_size/9), # validation data is 1/9th of training so reduce the batch size classes classes, class_mode define no.
using filter function twice against an object causes sigabrt.
run the number of iteration as specified in the `run -t` command
the cursor jumps to some location way beyond `z`.
i think in the validation step if i set phofcode to phofcode the model should use the learned moving mean and variance as expected but it doesnt seem so.
finally, gradients are not just reported falsely but optimization `b`, e.g.
phofcode it seems like in size of output tensor is corrected after model execution, near line: phofcode it seems like this method checks output dimensions with c++ backend code, and corrects this in java part.
shape mismatch for binary cross-entropy with logits for compiled keras when output shape is determined by model inputs (no error when using eager-evaluation): phofcode
detailed steps to reproduce the behavior: 1. create a minimal .vimrc: phofcode 2. using this .vimrc, edit a file containing a bom.
model training time should be closely equal in both the cases i mentioned.
go to phofurl and see the server list appear correctly on version 3.19.0.
three different tests are run to measure performance on both a gpu and cpu if available.
i guess this needs some thought but i'd expect to see tag-specific (maybe) or path specific base path overrides in the ui.
the last part of the profile is several applications of `applyadam` and the operations they depend on (reshapes, sums, mul, etc).
the offending code is this: phofcode `$$slots.default` corresponds to this: phofcode this fails with > typeerror: cannot destructure property `id` of 'undefined ' or 'null '.
call me naive, but i dont expect `expand("%:p")` touch the file system at all, and i expect it complete in well under millisecond.
from a user 's point of view, it looks like `l` is ignored if `a` is present
but svelte can't create such attribute because there is a read-only property `prefix` on htmlelement prototype.
there should be a way to disable mkl primitive memory reuse globally with an environment variable be it or better yet something else.
below is edited/commented version of an experiment i ran showcasing this bug.
phofurl the notebook linked above is nearly identical to phofurl i 've just added the following line to the training data code cell: idx = tf.py_function(lambda x: x, [idx], tf.int32)``
... steps to reproduce the behavior: 1. see this jsfiddle: phofurl 2. press play 3. try it out on get /pets 4. execute 5. observe generated curl request 6. now comment requestinterceptor return statement and uncomment the `return req;` line 7. repeat step 2 through 5 (this is expected behavior)
the code should work as it did in the earlier release ie tf2.0 alpha
from ~3k steps onward, cpu utilization increased to 80-100% whereas gpu volatile utilization oscillated between 0% to 30%
4. change that `text` field to a `string` field.
compile tensorflow v1.11.0 from source for raspberry-pi target architecture with gcc 6.3.0. then simply try to declare a tensorshape.
using tf.keras.optimizers.adam and updating hyperparameters results in hyperparameters not updating on the first update.
here is a simplified version of the code.
- os & version: [e.g.
the gitignore rule would ignore unix hidden files only: anything that
provide a reproducible test case that is the bare minimum necessary to generate the problem.
model tf.keras.sequential([ 3, input_shape=(28, 28, 1)), ]) model.compile( metrics=[ 'accuracy '] ) model.summary() print out physical logical gpus logical_gpus print(len(gpus), "physical gpus,", len(logical_gpus), "logical gpus") define checkpoint directory store checkpoints checkpoint_dir name checkpoint files checkpoint_prefix "ckpt_{epoch}") callbacks [ save_weights_only=true), printlr() ] steps_per_epoch / batch_size) model.fit( train_dataset, epochs=epochs, callbacks=callbacks validation eval_loss, eval_acc k.clear_session() __name__ == '__main__ ': run() ``
this will only happen in tf 1.12, tf 1.10 didn't have this problem.
1. run `vim --clean` and set a fixed width for the preview popup window: phofcode 2. edit a file with a prepared `tags` file and press `<c-w>}` on a tag with long file name.
this is currently perfectly possible in standard `keras` but raises an error in `tf.keras` (see below).
`expand("a \'")` is noticeably slower than `expand("a")`.
solutions: - when serializing a `sequential` model, all layers need to be serialized (and no filtering of `inputlayer` performed)
standard install from macos to do droplet fails on `strongswan : register p12 payloadcontent` task.
using `tf.function` when enumerating a dataset should not change the looping behavior.
the following is a stripped-down implementation of an rnn for text data loosely resembling the one in the effective tensorflow 2.0 tutorial phofhyperlink phofcode to rule out one culprit, the `tensorarray`s have been removed.
one node for all batch sizes.
3. from the "examples" list, select the example `1`.
other callbacks like modelcheckpoint with the same monitor works fine.
i convert my model like this: freeze_graph --input_graph=eval.pb tflite_convert --inference_type=float --output_arrays=oup
a few observations: 1. i know may have pushed this a bit far.
ideally all of these would work.
zipfiles written with `file=gfile()` should be not be corrupt and equal to those written with `file=<other_type_fd>`.
646 if src_graph is none: runtimeerror: tf.gradients is not supported when eager execution is enabled.
i expected ./algo to run smoothly and allow me to complete setting up my vpn.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i want to extract pbtxt file given an input of tensorflow frozen inference graph.
* foo2 is pretty good, but it 's odd that i have to wrap every integer into a tf.constant.
if including tracebacks, please include the full traceback.
install algo on win10 as a non-admin user providing admin credentials.
i try to to fit model that contains
- os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16 - tensorflow installed from (source or binary): conda environment with pip install tensorflow==2.0.0-alpha0 - tensorflow version (use command below): 2.0.0-alpha0.
the expected behavior is to get the same result running the model in android that when i run the model in python
i have a docker image which has built-in sudo and a sudo user.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 14.04 - mobile device (e.g.
when i rewrote my estimator-based model (dataset + feature columns) to keras i was able to run (train) it.
one possible fix is to use the alternate screen buffer.
large logs and files should be attached.
created instance with: `gcloud compute instances create "tf-1-14-cpu" --zone="us-west1-b" run 1: phofcode run 2: phofcode run 3: phofcode
3. run `gvim --remote-tab-silent file.pyc`.
* i guess there might be reasons to use identity (`is`) comparison on arguments when deciding whether an autograph needs retracing, but if it were possible use equality comparison (`==` ) for arguments that are methods this surprising behavior could be avoided.
the buffer content is updated.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): osx / ubuntu 16.04 for creation of the model - mobile device (e.g.
towards the end of the tasks running i get the following failed error: task [vpn : create links for the private keys]
then on that run train_model.py.
using can be temporary solution, but this trick doesn work with lateset build
need to flush gpu data at each iteration so it won 't run out of memory
the use of the param 'max_value ' in phofurl produces the a quantized graph with the fakequant op just after the relu op and not after automatically inserted clip op.
as part of phofurl i have a `cursorhold,cursorholdi` autocmd setup the populates the quickfix list via `setqflist` if any diagnostics have changed.
behaviour is correct in tensorflow 1.13
so this works (because no unpacking): fu func() let a =<< trim x endfu call func() and this works too (because no extra space around comma): fu let [a,b,c] =<< trim x endfu call but not this: fu [a, b,c] endfu call notice space after comma here: [a, b,c] third, one of line needs to start with backslash.
when i used model.fit(dataset), it only takes ~7 seconds.
expected behavior is the ability to successfully install the profile on an ios device so vpn can be used using the password that was generated during the algo install.
after i have upgraded vim to `gpush` fails to push.
without distribution strategy everything works fine.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): source at tensorflow version (use command below): n/a python version: n/a bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
`y_true` seems to have expected `[64,4]` shape so we cannot squeeze it on its dimension `1`.
warning:tensorflow:from test.py:24: dense (from deprecated and will be removed in a future version.
python-virtualenv libssl-dev python-pip upgraded, newly installed, to remove and not upgraded.
1. run `vim --clean -s reproduce.vim` phofcode
there should be no prompt, since there is plenty of space to fit the [2/2] numbers.
i first create a dataset with an iterator, then create a tpu iterator with the dataset.
it turns out that is passing when sharding enabled as not all of sub-tests being run.
the code in the colab, when run on a gpu, results in teh following error: file line 885, in _bootstrap self._bootstrap_inner() file line 917, in _bootstrap_inner self.run() file line 865, run
_033 phofimage - os & version:ubuntu18.04
vim does not freeze if i name a file `t`
a time sequence of length 1000.
tensorflow can build graph successfully, but when it comes to flow the data into model and update parameters, it always gives the segmentation fault error.
i think keras layer/model should have `output_shape`, but they aren 't.
7.5kib 4.5kib client-requested (4096): 10, 10.
if including tracebacks, please include the full traceback.
note that the bug does not reproduce if the unknown-shape tensor is passed directly to `reduce_mean`, which would suggest some interaction between the two.
sparse and ragged tensors should be acceptable input for keras models.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - tensorflow installed from (source or binary): - tensorflow version: 1.12.0 python version: 3.6.8 valueerror traceback (most recent call last) in <module> 12 13 ---> 14 metrics=['accuracy']) in _method_wrapper(self, *args,
update: i tried the latest nightly building, still no luck and now more error info showed: phofcode
so, if i restore model, i get model which is not changed.
this could mean that the variable was uninitialized.
for tensorflow 1.12.0, the error i get is:`incompatible shapes: 64] vs. [64,4] [op:equal]`.
... steps to reproduce the behavior: 1. load to example yaml 2. see error
in your hosted deeplab model, those three ops are replaced by depthwise_conv_2d v2, which has options set dilation factor.
here is the colab gist to reproduce the result phofurl
instructions for updating: colocations handled automatically by placer.
the expected behavior is that the above snippet will not return errors (log attached below) potential solution: check the input `x` in `dtype()` method in happy to contribute if this is helpful :)
detailed steps to reproduce the behavior: 1. create the file "syntax_prop.vim" containing the following: phofcode 2. run `vim --clean -s prop.txt prop.txt` 2. edit `filename` 3. the 2nd word of line 1 has had italics applied, but the colour of existing text property has been cleared.
after converting a conv1d op to tensorflow lite the interpreter cannot allocate tensors: ` != kinputdimensionnum (3 != 4)node number 0 (space_to_batch_nd) failed to prepare.
the width of the dialog window should not be drawn wider than the text displayed inside it.
don 't introduce extra blank line.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
this could mean that the variable was uninitialized.
datasetb does not correspond to the order of shuffled dataset a.
the visual output is fine in this example, however there are javascript errors emitted (and the online editor shows them).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): there is a custom mwe code snippet included in the issue - os platform and distribution: linux elementary loki (ubuntu 16.04) - tensorflow installed from (source or binary): binary - tensorflow version: 2.0.0-alpha0 python version: 3.7.1 i generated a list of all the objects for the 2 code snippets, using `pympler`.
devices: streamexecutor device (0): <undefined>, <undefined> found device 0 with properties: name: geforce rtx 2080 major: 7 minor: 5 memoryclockrate(ghz): 1.71 pcibusid: totalmemory: 7.76gib freememory: 7.65gib adding visible gpu devices: 0 device interconnect streamexecutor with strength 1 edge matrix: 0 0: n created with 6809 mb memory) -> physical gpu (device: 0, name: geforce rtx 2080, pci bus id: compute capability: 7.5) f non-ok-status: num_blocks, blo ck_size, 0, d.stream(), gen, data, size, dist) status: internal: invalid configuration argument aborted (core dumped) ``
when creating a custom rnn cell containing a few layers (created in the constructor), these internal layers are not properly built the first time the cell is used.
i randomly generalize some data as inputs, and take reduced sum of logits as loss.
i update each plugins except strapi-plugin-graphql in package.json and run yarn in the console.
next(iterator))` is used to call a model on each replica.
although the value is changed, results do not store into checkpoint.
attached archive with broken model generated with tflite converted with default options (no properties changed) and script to create model and tflite file itself.
--config=monolithic # config mostly static monolithic build.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): nightly-2.0-preview python version: the one with custom-ops container bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
there are four input arguments to the eager mode function of dtype tf.float32 and tf.int32.
i expect it to fully install
i draw a picture to show problem: phofimage
no errors will appear because the script was sourced from the command line.
for some images, not for all 108, 56) gives an output size of 109 by 56
such a model should be usable.
each test runs for 1 epoch comprised of 1000 mini-batches.
`def lstm_model(embed, lstm_sizes, keep_prob_, batch_size): lstms = for size in lstm_sizes] drops = for lstm in lstms] cell = initial_state = tf.float64) lstm_outputs, final_state tf.nn.dynamic_rnn(cell, embed, return lstm_outputs, final_state`
when creating a custom model with a `build()` method (e.g., if one of the model 's layers has a size that depends on the input shape, such as a reconstruction layer), the model cannot be trained unless i explicitly call `build()` with `tf.tensorshape()`.
should save a tflite file after conversion
<details> <summary>vim --version</summary> phofcode <details/
from the status, it is shown that it utilized whole 16gb system memory while it utilized 80% gpu memory but temp and utilization status is low.
tflite app crashes with log shown below.
`cat label.pbtxt` > some txt in the file 'label.pbtxt ', overwrite=true)` `cat label.pbtxt` > <empty text file>
libstdc++.so.6 #14 start_thread libpthread.so.0 #15 clone libc.so.6 ``
4. get request for blogs?_q=query 5. all entries is returned, no matter what the query matches.
- vim version also seen in so it 's not a regression.
it also fails for scalars with any value in `axis`.
i installed baseline for openai and this started after that.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - mobile device (e.g.
i created a small repo to test, as the repl site doesn 't allow compiling to web components.
`call prop_clear(1)`, text properties is removed.
an additional information is that size actually increases too: original is 43mb and optimized is 44mb.
here is an example of a simple model where this error would occur.
current behavior in 1.11.0, 1.12.0rcx is that the included script segfaults (11: sigsegv).
i converting unsupported operation: resizenearestneighbor i converting unsupported operation: resizenearestneighbor i before removing unused ops: 88 operators, 126 arrays (0 quantized) before general graph transformations: 88 operators, 126 arrays (0 quantized) after general graph transformations pass 1: 20 operators, 53 arrays (0 quantized) before dequantization graph transformations: 20 operators, 53 arrays (0 quantized) total transient array allocated size: bytes, theoretical optimal value: bytes.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
'vim --clean' and 'i' and try to paste it.
a window outside of vim is opened, however a non-responsive and uninitialized split buffer is also created with the cursor locked in the top left corner.
the layer that seems to be generating this (see trace below) is phofcode i am using several: and statements to control placement of ops within different towers onto specific devices.
(looks a bit similar to #28599 if you squint, but many details differ.)
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:n - tensorflow installed from (source or binary):binary tf-mkl: pip install phofurl tf-no-mkl: wget phofurl pip install tensorflow version (use command below):1.111.0 python version:2.7.5 bazel version (if compiling from source):n gcc/compiler version (if compiling from source):n cuda/cudnn version:n gpu model and memory:n cpu: intel(r) xeon(r) cpu e5-2682 v4 @ 2.50ghz thread(s) per core: 2 core(s) per socket: 16 socket(s): 2 include any logs or source code that would be helpful to diagnose the problem.
`sparse_reshape` should work similarly on both windows and unix-based systems.
for this i build temporary models inside functions and test them.
> check failed: ndims == dims() (2 vs. 4)asking for tensor of 2 dimensions from a tensor of 4 dimensions i cannot reproduce the issue outside of docker on ubuntu18.04 and tensorflow 1.11, everything works as expected.
the resulting highlighted region is `67` but `567` is the correct behavior.
- vim: also tested with - os: ubuntu 18.04 - terminal: gnome termina
the performance should be similar.
here is obtained plot: large complex plot model phofimage the upper part phofimage plot is due to conversion input into sparse tensor, so this is related previous issue phofhyperlink .
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.6.8 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory:
this is with a code that works perfect windows 10 and macos with all kinds of tf versions between 1.13.1 and 2.0.0rc1, gpu or cpu versions.
i use to get the prediction in a function.
i am referencing the args on freeze_graph.py phofhyperlink and failing to find an issue with my usage.
it seems like being able to assign integers to tensors while using distributed training is something that should be possible, given the goal of abstracting away distribution as much as possible.
instead, it _also_ will land on the `{` on line 2, which is a java annotation and should not be reached by the `]m` and `[m` motions.
my iterate through a dataset that i created with some custom images and i set with the function and convert the frozen model to tflite with int8 quantization.
tflite_convert fails to convert simple conv2d model with batch norm file to .tflite.
large logs and files should be attached.
models whose layers have shared/tied weights should not return duplicate weights when accessing the trainable_variables property.
phofcode train_data would be a tf.dataset produced using padded_batch that produces (word_ids, ner_tags) as inputs, labels.
i have not checked lower versions.
this is done by filtering certain normal mode commands, and running them with `:normal` in the popup window.
the print out of the difference should be zero.
i 've attached a small example.
thoughts about the problem has been included in the sample code.
if including tracebacks, please include the full traceback.
the speed should be always very fast (about 7s/100iterations).
1. create a project 2. npm run develop
i expected that a change and return to the original data type, without modifying the buffer data would not cause that amount of decrease to network 's accuracy.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): `import tensorflow as tf` - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 64-bit - mobile device (e.g.
when i run the simple mnist script below (with the tf.summary.scalar line commented out) it runs fine.
my custom conv2d layer shows the following error message.
as a tensorflow package, i would expect the `tf.io.gfile` functions to allow the use of scalar string tensors, or i would expect tensorflow to provide a solution similar to the `.numpy()` function inside `tf.function` for these cases.
it restarts from (state step + 1).
maybe the easiest fix would be to prevent `dataset.__iter__` being called inside `tf.function` if it is not in a loop.
training a conv network without an error
if including tracebacks, please include the full traceback.
as described in tflite document: phofurl `conv_2d` should accept optional bias tensor and does not raise runtimeerror.
1. run `vim -u none` 2. do: phofcode
loading a save h5 file and saving the resulting tensorflow pb results in a well formed pb that can be used for inference.
the model will be loaded successfully.
large logs and files should be attached.
i am using reinforcement learning code to train the robot in a gazebo platform using ros.
large logs and files should be attached.
if applicable, copy/paste the text or add screenshots to help explain your problem.
borrow the benchmark code from here phofhyperlink .
- have i written custom code (as opposed to using a stock example script provided in tensorflow):yes - os platform and distribution (e.g., linux ubuntu 16.04):centos 7 - tensorflow installed from (source or binary): source - tensorflow version (use command below): compile using tags/v1.10.0 python version: 2.7 bazel version (if compiling from source): 0.15.0 cuda/cudnn version: cuda 9.1/cudnn 7 log screenshot image phofimage i did post a stackoverflow question phofhyperlink but no response so far.
there is a workaround for rtx gpus at the top per the comment in #24828 phofcode
to allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.
installing rdm on the server in question and connecting there worked without issue
hope to have a tf c++ kernel for this op, then it could call the kernel directly to get good cache locality on cpu or reduce memory access on gpu.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
au focusgained * echom 'focusgained was fired ' au focuslost * echom 'focuslost was fired ' augroup end 2. from xterm, start vim with this minimal vimrc: $ vim -nu /tmp/vimrc 3. run an ex command whose output contains multiple lines: :echo "foo bar" 4. focus a different program window (web browser, email client, ...).
code was modified from: phofurl run option 1 to use keras with no errors and use option 2 to run with tf.keras and see all the errors.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:n/a - tensorflow installed from (source or binary):pip tensorflow version (use command below):1.8 python version:2.7 bazel version (if compiling from source):n/a gcc/compiler version (if compiling from source):n/a cuda/cudnn version:9/7.1 gpu model and you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
i found that lines with `expand("%:p")` and similar were totally dominating the times.
however i've also tried with a real input (comming from an image), and in this case, results android are totally different.
3. for each rectangle, calculate it's pixels horizontal & vertical gradient's mean and standard deviation.
i am trying to convert a tiny yolov3 frozen graph into a frozen graph with some operations replaced with trtengineops so that they are run with tensorrt.
tensorflow 1.14 changes the relative call order of building the model and the set_model callback in the tf.keras fit_generator path.
info:tensorflow:saving checkpoints for 1 into tmp/model.ckpt.
as far as i ( and others phofhyperlink ) can tell, it does not.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6.7 (default, oct 22 2018, [gcc 8.2.0] bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: ??
i tried to use tpudistribution strategy to use tpuv2 pods for training xlnet, but it gave me error(mentioned in logs below)
code generated the tflite file phofcode code to test in python(from doc) phofcode
when running `fit_generator` on the main thread (i.e.
i was on another do droplet while doing this.
when its unpacked it cannot be opened as a text file (looks like another archive file) as the newstest2014.en can and was in previous versions.
resolve refs and no errors.
choose your default database client mongo ?
t [[{{node additional grpc error information: received from output shapes, got [16], but expected [64].
optimizer runs metric to convergence
in the past when i emailed myself the user.mobileconfig file, i just tapped on it and the ios settings page opened automatically.
4. in this case, where is it getting 0 for batch size.
i expect the highlighting in the popup to be the same as the highlighting in a normal window.
on `tf.1.11.0-cuda 9` maximum batch size for my gtx 1060 6gb is `132` but after upgrade to `tf.1.13.1-cuda 10` tf cannot handle same batch size it produces oom error and maximum now `90` for my card.
`dataset.window` documentation has a few examples of which all crash in the process of graph construction (i.e.
certain ops, like `tf.range` are sensitive to constant inputs due to the need for static shapes.
when we define multiple methods for a class and only decorate one of them with `@tf.function`, the nested methods are not automatically transformed and some errors raise.
i believe softmax outcome should be different in each run.
running `tf.svd([[]])` leads to a segfault.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): i 've provided a link to a colab notebook demonstrating the issue below, comparing keras upsampling to what it should look like with a correct implementation as seen in tf.image.resize.
1.2.9, with the following packages installed: see my comment on the possible source of the problem.
over multiple iterations with a re-compile of model, fit() time continuously increases.
on google colab, use gpu runtime and run this code phofcode other info / logs phofcode
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the errors get are not informative.
no error message will appear now.
after that we tried implement our model using gradient tape and it seemed fine but after loading checkpoint results was different than expected.
i am able to do so from the command line, but not from a python file.
generated a sample.tflite file with 0kb
- os platform and distribution: linux ubuntu 16.04 - mobile device: oneplus 3, one plus 5 and pixel 2 xl - tensorflow lite version on android: 0.0.0-gpu-experimental - have i written custom code: a github repo contains the codes to reproduce the issue.
when i try to log a scalar inside model.call() i get an error message that seems to indicate tf.summary.scalar is seeing the original tensor--somehow--as opposed to the numpy-summed result that i am passing to tf.summary.scalar.
a clear and concise description of what bug is.
inside an `if` block, a list is assigned a heredoc regardless of whether the relevant test succeeds.
this is visible in `docker stats`, eventually crashing docker.
18.5kib 18.5kib 12.0kib (16384): 12, 11.
i have a time-series dataset.
i 'm not aware that `govim` is doing anything special here; indeed i 'm certainly not trying to change/override this behavior in any way so anything i might have done unintentional.
3. enter your user 's email address and click on 'send mail ' 4. check the link in the reset password email sent to your email address.
phofurl or try `{1 === 1}` in your code.
note: i have ran ./configure via a terminal to use force cuda support (such option is not accepted in the example).
meanwhile, tensoflow 's implementaion has no problem.
1. created new project using: npx create-strapi-app <name-your-project> 2. i updated to use `postgres` and ran `npm install --save pg` phofcode and specified these production env vars in my web host (render.com, like heroku; values ommitted): phofcode 3. i set render.com to use these commands, which seem to work as expected: build command: `npm install && npm run build` run command: `npm run start` 4. during build, i get the two errors described earlier.
moreover, when i run same code again, i get a different output (only last line differs).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - mobile device (e.g.
the code should convert the string to the bytes, for example: phofcode
the problem is demonstrated by the following script.
"}; + int arg_is_literal = 0; int flags; d = iobuff; t t /* assume iobuff long enough!
provide a reproducible test case that is the bare minimum necessary to generate the problem.
'active ' : 'not active ' endfu but current issue really about some events are triggered.
a constructor of a tf.keras model that uses with `virtual_batch_size` set and unspecified input shape dimensions throws an exception.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.12, 1.13 and master python version: 2.7 bazel version (if compiling from source): 0.19.2 gcc/compiler version (if compiling from source): gcc 6.3 cuda/cudnn version: n/a gpu model and memory: n/a
large logs and files should be attached.
phofcode to debug, i start another thread to print the current stacks.
select which ones you want to grant swagger ui.
i will attach a full log to end of this issue
in >=1.11, the training step is evaluated once (correctly).
import numpy as np import tensorflow as tf parsed_fields = record_defaults)
if applicable, copy/paste the text or add screenshots to help explain your problem.
can you provide any recommendations as to how to stop it from seg faulting during training (and why it stops at a different point each time)?
phofcode (this particular example can be fixed by manually defining a `@property` variables which combines the variables members from each of the dense layers into one list, but i think this partially defeats the point of using `tf.module`).
the performance on my gcp instance should be the same with gcolab since everything is the same
... steps to reproduce the behavior: 1. go to '...' create a swagger and have the two fields i provide as examples 4. when teh ui vaildation triggers yo'll see the error message for 2) above
tflite micro 's fastint32tobufferleft phofhyperlink causes error c4146 `unary minus operator applied to unsigned type, result still unsigned` on visual c++.
i suspect the correct (intuitive behavior) is to close the window in which the winbar exists.
to force crashing, you can increase the array size: `np.random.rand(xx, yy)` phofcode edit: in the same directory is `hooks.py` containing the following: python class def __init__(self, op, feed_dict): self.op = op self.feed_dict = feed_dict def session, coord): # pylint: disable=unused-argument session.run(self.op, ``
all components should be shown equally.
`: call 'b ') and, in html indent plugin, if i replace `normal!
that can 't be right, can it?
test descriptions: - test 1: uses the keras 'compile ' 'fit ' function - test 2: uses tf.gradienttape does not use @tf.function decorator - test 3: uses tf.gradienttape @tf.function decorator.
it escapes `*` and `glob` is unable to find a file ` *.c`: phofurl phofurl i have tried it removing `fnamescape` -- it worked just fine
i have a fairly large, complicated estimator-based model that i want to train using a tesla p100.
when xla is enabled, cuda contexts are created on every device visible to the cuda driver (excluding devices with cuda_visible_devices works, but in configproto does not).
(.env) $ ./algo [warning]: could not match supplied host pattern, ignoring: vpn-host play [localhost]
shared_embeddings phofhyperlink not surport eager mode, so i call it with @tf.function, but still throw exception: valueerror: variable already exists, disallowed.
the problem is in phofurl where a is called.
the following code results in the gradient being `none`: def f(x): return x
i can reproduce this on both macos and linux
i cannot provide the data as it is proprietary.
windows 10 1806] - redis-server version [e.g.
usage of doesn't throw an attributeerror, when being a suggested solution.
i 've also been able to get this to work (so to speak) in other terminal emulators: 1. if your terminal emulator 's $term contains xterm (like gnome-terminal and konsole for some reason do), change $term to something not containing xterm (e.g.
using the above code i get the output as pbtxt but cannot use it.
as given in this page : phofurl i am trying to change the code in seq2seq as follows: in decoder.py : `class however, while running the pylint stage of sanity_checks it produces the following error: [e1102(not-callable), my_decoder is not callable [e1102(not-callable), my_decoder is not callable `
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - custom code.
i am trying to move some of keras 's image transformation functions phofhyperlink into my dataset constructor function, e.g.
if including tracebacks, please include the full traceback.
the peephole gradient should be taken into account _either_ in the gpu kernel _or_ by the eigen computation, but not both.
1. this code below compiles but errors during the run with the error below.
- using unmodified google colab tpu runtime == cat /etc/issue linux 4.14.79+ #1 smp wed dec 19 pst 2018 x86_64 x86_64 x86_64 gnu/linux version="18.04.1 lts (bionic beaver)" version_id="18.04" version_codename=bionic == are we in docker yes == compiler c++ (ubuntu 7.3.0 copyright (c) 2017 free software foundation, inc. this is free software; see the source for copying conditions.
phofcode to figure out why i get `memoryerror`s, i have used the python interactive mode, and called `.evaluate(...)` and `.train(...)` while observing ram usage, after each call, another 200mb ram got occupied on the cpu ram, eventually depleting all available ram
i would expect it to behave the same across all major browsers.
image phofimage - vim version - os: windows 10 1903 - terminal: windows command promp
(see above) i was able to reproduce the problem with debug cflags as well: `cflags= '-o0 -g3 ' ./configure --without-x --enable-gui=no` here 's some info from gdb: phofcode if i can help by providing any more information, please let me know.
i 've built a tensorflow custom estimator using keras layers, and it worked fine initially when i used `train_and_evaluate`, but i 'm seeing now that when i am using `train_and_evaluate`, it just checkpoints at step 0, the loss being `none` and moves to the evaluate phase.
phofcode provide a reproducible test case that is the bare minimum necessary to generate the problem.
try creating a component lib (one output contains "generate": "ssr", while client output "hydratable": true) and import the lib into your sapper project.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 7 64-bit - mobile device (e.g.
only expected following keys: ['output_1'] ``
), similar to the file name in the `statusline`.
it should be "0 = silent, 1 = progress bar, 2 = one line per epoch."
custom kernel performance should be no worse than implementations in terms of other operations.
in 1.14.0rc0, the dataset passed to the `reduce_func` function of is a `datasetv2` instance instead of `datasetv1`.
i expect `params` in the `model_fn` and `input_fn` to be an instance of hence its parameters should be accessible as attributes.
would that get me a speed improvement?
if in eager mode, the tensors passed to the `call` method of a custom model should be eager tensors.
the issue is that it is utilizing more cpu memory rather than gpu memory.
if you are certain code is graph-compatible, wrap call using original error: could not get source code w0611 23064 ag_logging.py:145] entity <bound method lp.call <__main__.lp object at could be transformed and will be executed as-is.
883 (named_saveable_objects, graph_proto, --> 884 feed_additions) = 885 if object_graph_tensor is none: 886 with ops.device("/cpu:0"): 379 trackable_objects, path_to_root = 380 return --> 381 trackable_objects, path_to_root) 382 383 def object_map=none, to_graph=none): trackable_objects, path_to_root, object_map) 335 object_names 336 for obj, path path_to_root.items(): 337 object_names[obj] 338 node_ids 339 for node_id, node 62 return "/".join( 63 ---> 64 for trackable path_to_root)) 65 66 <genexpr>((trackable,)) 62 return "/".join( 63 ---> 64 for trackable path_to_root)) 65 66 _escape_local_name(name) 55 # edges traversed to reach the variable, so we escape forward slashes 56 names.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
transform should be able to process large number of records.
7. name be provided for combo box parameter content type.
i have converted the model to tflite and performed post training quantization in two virtual environments - one with (following the instructions from here: phofurl and one with tensorflow==1.10.0.
and why it is adding?
in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing,
model.save() should save the model.
large logs files should be attached.
after migrating to tf 2.0, when compiling a custom model i wrote i get an error `attributeerror: tensor.graph is meaningless when eager execution is enabled.` the model compiled as expected in tf 1.13
- does it has something do way as java works floats?
however, when i check the model, original tiny_conv model 's conv2d layer becomes a depthwiseconv2d layer.
phofurl i am implementing resize op for onnx.
by setting clipvalue=0 or clipnorm=0 no training should occur (gradients should be 0!
dummy_labels = tf.round(image_decoded) return image_decoded, dummy_labels list_files files files files.repeat(100) files files.batch(1, drop_remainder=true) return files #basic check to compare #can 't be run after tpu initialisation with tf.session() sess: batch item sess.run(batch) print( 'shape of first item : ', item[0].shape, item[1].shape) plt.imshow(item[0][0]) plt.show() plt.imshow(item[1][0]) def unet_encoder(inp, layer_nb, activation 'relu ', padding 'same ', batchnorm true, is_pool true): inp tf.keras.layers.conv2d(8 * 2
instructions for updating: use tf.where 2.0, which has the same broadcast rule as np.where collective ops may deadlock with `save_checkpoints_secs` please use `save_checkpoint_steps` instead.
if including tracebacks, please include the full traceback.
it should not fail with this error :-)
i created a live demo here: phofurl the source is here: phofurl phofurl the issue i see is rooted here on the generated code: phofurl phofcode
provide a reproducible test case that is the bare minimum necessary to generate the problem.
8. change to server url (to, say, ' phofurl note that it changes appropriately `server` dropdown.
1. override your `user.settings.json` & `role.settings.json` to another database 2. install the documentation plugin 3. try to start your application in `production` or `staging` 4. see error
n/a - ubuntu refer #4693
(from other testing, confirm key here is not `a:` bit, but either path string length or path component length, and havent checked which.)
when i trying to remove training node with the function a identity node named " is not removed.
the sums are done in fp32, and relus and batchnorm gradients are fp32 as well.
i tried to train the model by using image datasets
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary (mkl) (pip install intel-tensorflow) tensorflow version (use command below): 1.13.1 python version: 3.6.8 (anaconda) compared to #15697, i am actually timing the equivalent op in numpy (transpose then copy).
- google colab with tpu
should not produce this error.
i followed instructions for installing tensorflow 2.0 with gpu support here: phofurl another error i get along way is phofcode i 've tried one of solutions in #6698, but `tf.configproto()` was not found.
source code: main.py phofcode when run `time python main.py 1` and `time python main.py 2`, get 4.6s and 1.2s
16/100 4:00 w allocator (gpu_0_bfc) ran out memory trying allocate 218.70mib.
`:inoremap <esc> <esc>jk` 3. enter insert mode (say, `i`) 4. leave insert mode by pressing `<esc>`
example: - after building model: transpose_layer1 has weight tensor (3, 3, 63, 61) - when reloading model: transpose_layer1 has weight tensor (3, 3, 61, 63) after swapping two dimensions via numpys transpose method shape (3, 3, 63, 61) is restored and model successfully trains.
... 1. authorize with oauth2 authorization code flow 2. press log out 3. try to authorize again
@concurrency.py:38] starting enqueuethread queueinput/input_queue @graph.py:73] running op @param.py:158] [hyperparamsetter] at global_step=0, learning_rate is set @eval.py:250] [evalcallback] will evaluate every 25 epochs @base.py:274] start epoch 1 0%| creating cudasolver handles stream @param.py:161] [hyperparamsetter] at global_step=1, learning_rate changes 0%| 0.08it/s] w skipping cancelled enqueue attempt queue not closed traceback (most recent call last): enqueuethread queueinput/input_queue exited.
warning: logging before flag parsing goes to stderr.
since the model behavior should be consistent.
when enabling training on embedding layer using keras subclassed model, an error is raised during model.fit(): e constant folding failed: invalid argument: unsupported type: 21` this has a huge impact on training speed, that scales with the amount of data, in my full code, each epoch take more than 1 minute to complete(dataset of 869 tokenized strings, for train an encoder/decoder model), the code for reproduce this error dont have a noticeable impact because there is only one sample, already tokenized.
similarly, functional api gives no error message.
now i got following error: `process: pid: 12449 cannot copy between a tensorflowlite tensor with shape [1, 10, 4] and a java object with shape [1, 4, 4].
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no , i am using simple audio recognition example - os platform and distribution (e.g., linux ubuntu 16.04): window 7 - mobile device (e.g.
this problem seems happen only macos.
... steps to reproduce the behavior: run in swagger ui provided example.
the `kernel initializer` should be changed to `he_uniform` in `conv`, `separableconv`, `depthwiseconv` and `dense` layers from this __init__( filters, kernel_size, strides=(1, 1), padding='valid', data_format=none, dilation_rate=(1, 1), activation=none, use_bias=true, kernel_regularizer=none, bias_regularizer=none, kernel_constraint=none, bias_constraint=none,
no distribution strategy will be used for evaluation.
... steps to reproduce the behavior: 1. change anything in `swagger-config.yaml` and check
i have an error while import tensorflow (cf screen)
i expect the value of `lispwords` to be `hello`, or at least to contain `hello`.
currently, mobilenet_v2 keras in references the function `decode_predictions` from here phofhyperlink
when the params argument is from locally defined variable, or when using tf.nn.embedding_lookup instead of tf.gather, everything works perfectly fine.
tf.name_scope is applied to wights created by keras.layers output in tf 1.13.1 is the expected behavior phofcode
yes this is similar to issue #23145, but it is definitely not fixed in r1.13.1.
as stated above, the code is directly accessible through the url.
when the corresponding "continuing at {top|bottom}" message appears), search "lags", i.e.
didn't quit alternate screen on recive sigtstp send with kill
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): google colaboratory include any logs or source code that would be helpful to diagnose the problem.
please report this to the autgograph team.
the notebook i used can be found at link phofhyperlink
if including tracebacks, please include the full traceback.
3 means the collective executor works in a multi-task way of executing the graph on `all_reduce`.
first you should know that this piece of code is compiled with the gcc flag : whenever i am compiling for the raspberry-pi target because i was not able to build tensorflow for arm32bit with the new abi version.
1044 """ -> 1045 return flatmapdataset(self, map_func) 1046 1047 def interleave(self, __init__(self, input_dataset, map_func) 3063 self._input_dataset = input_dataset 3064 self._map_func -> 3065 map_func, dataset=input_dataset) 3066 if not datasetstructure): 3067 raise typeerror("`map_func` must return a `dataset` object.")
the speed of fft2d operation `tf.signal.fft2d` is very unstable at different iterations.
this way, i can delegate one gpu for each process (and it doesn 't allocate all of the memory and combine the two gpus into one at the beginning.)
keras model.fit() does not reset validation dataset iterator between epochs.
the same problem occurred not only on this very simple regression task but also on a model trained on utk faces data set for age regression.
graph execution does not effect graph construction, the model should be the same.
i guess without my database entries, it is impossible to reproduce and due to sensitivity of the data stored, its not possible to share the models return by the query.
given model is doing random guessing.
add operations to the graph before calling run().
(this is example is nonsensical, but shows the problem.
- have i written custom code: yes - os platform and distribution (e.g., linux ubuntu 16.04): android 8.0.0 - mobile device: samsung s9+, sony xperia xz2 - tensorflow installed from (source or binary): binary tensorflow version (use command below): (used pretrained *.tflite file) python version: (used pretrained *.tflite file) bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: exynos 9810 octa (samsung), sanpdragon 845 (sony)
... steps to reproduce the behavior: 1. npm install --save swagger-ui-dist 2. attempt to import `swaggeruibundle` from `swagger-ui-dist` 3. start express server 4. see error stating `swaggeruibundle` is undefined
1. why the size of the apk file is increased when i use the quantized model ?
provide a reproducible test case that is the bare minimum necessary to generate the problem.
`runtimeerror: op_context.input->type == ktfliteuint8 || op_context.input->type == ktfliteint8 was not true.node number 0 (dequantize) failed to prepare`.
tensorflows raises warnings like: phofcode
3. eventually crashes after a few epochs and throws following internalerror caused by op defined at: file "main.py", line 70, in <module> gran.initialize() file "main.py", line 33, in initialize config=self.config) file line 32, build_network file line 45, build_model training=self.training) 38, build_model = reuse=false) 37, build_model _x = filters=256, kernel_size=[7,7], nstraint, strides=1, padding= 'same ', 417, conv2d return layer.apply(inputs) 817, apply return self.__call__(inputs, *args,
image phofimage - vim - vi improved 8.1 (2018 may 18, compiled may 8 2019 ms-windows 32-bit gui version with ole support included patches: 1-1296 big version with gui.
certain cnoremap mappings seem to not work when ` 'wildcharm '` is set to `<c-z>` but do work when set to `<tab>`.
the estimator shows a much slower training and a significantly lower final recognition error.
the documentation of tf.scatter_add phofhyperlink states that it requires the following: phofcode however, no exception is thrown when i run the code below.
11. go back to vim, cause you need it 12. file this bug report.
1. even if allow growth is true, tensorflow allocation is taking place in steps.
am supposed to supply some other kind of buffer?
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no, but dump the runtime data - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
we are attempting to run a distributed job.
"you can access each of individual distributed models using `_grouped_model` attribute of your original model" the warning seems to suggest that one should try to access an `undocumented` and `private` property of an object.
i corrected the post since i made a mistake.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
hope that estimation result is tensor output, but cannot read result.
my scripts no longer execute because of this.
i have a quantization-aware trained static rnn lstm, with unstack.
if including tracebacks, please include the full traceback.
i can create the session and readbinaryproto, but when i run this code ,it fails return theis message: `tensorflow::status status_create = > invalid argument: no opkernel was registered to support op 'sin ' with these attrs.
windows 10 1806] - redis-server version [e.g.
define basic stuffs phofcode this works as expected phofcode this also works phofcode but this is not working phofcode
i am using tensorflow 1.13.1 with gpu rtx 2080. i am using reinforcement learning to train the robot in a gazebo platform using ros.
(2) it seems only with tensorflow 2.0 gpu version, this happens.
batch_input_shape=(none, time_steps, input_size), # or: input_dim=input_size, input_length=time_steps, output_dim=cell_size, unroll=true, ))
... steps to reproduce the behavior: 1. go to phofurl 2. input ` phofurl into the textfield at the top.
for example, the following code (with 6 layers) is very slow newer tf1.12.0.
`strapi -- generate:api stuff cms_name:string 2.
doing a clean install of tensorflow-gpu=1.13 and running the script achieves this result.
name: the name the op.
all code required to reproduce this but can be found on google colab: phofurl
valueerror traceback (most recent call last) in <module>() 26 27 if epoch % 2 == 0: ---> 28 29 30 template = ("epoch {}, loss: {}, accuracy: {}, test loss: {}, " 6 frames in handle(self) 578 device = 579 if device is none: --> 580 raise valueerror("`handle` is not available outside the replica context" 581 " or a call.")
sequence generator as validation_data not working if flatten() layer involved in model
the training should be able to run on the gpu of the worker machine
- on lines 42 and 43, there's a use of `tf.contrib.lookup` which should be moved to `tf.lookup`.
running the below code in docker (version 19.03.2) causes the memory to grow without limit.
not using a 0 in place of a o) 4. i 've destroyed a do droplet and re-created a fresh install with the same result put the output here
if including tracebacks, please include the full traceback.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes, and i have produced a boiled down script that can reproduce the problem standalone - os platform and distribution (e.g., linux ubuntu 16.04): linux (rh family), running on intel xeon phi (knl) - mobile device (e.g.
i have a dataset containing 592 examples, but `tf.dataset.map` only processes one of those, as evidenced by a global counter, which i increment in the function given to `map()`.
this function will be called through `tf.data.dataset.map`.
8. select "[modified value]" in examples dropdown.
1. run `vim --clean -s reproduce.vim` phofcode
instructions for updating: use warning:tensorflow:from extract_sub_graph (from l_impl) is deprecated will be removed in future version.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): custom simple keras sequential model with conv1d dilated layers.
it should not be in an ndarray.
i have code that writes .npz files from multiple python processes.
large logs and files should be attached.
when i uninstall tensorflow 1.12 and install tensorflow 1.13 via: `pip3 uninstall tensorflow` `pip3 install tensorflow==1.13.0rc1` then run my model, i get a very strange error which looks like this: phofcode
we expect same loss from both train and train2.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): _yes, app using tflite_ - os platform and distribution (e.g., linux ubuntu 16.04): _tensorflow lite on android_ - mobile device (e.g.
phofurl download, build and start to see the text
when using keras model with
steps to reproduce the behavior: 1. install version 2. connect to my redis server without any credential and succeded.
can be reproduced in repl: phofcode you get the `todo this should not happen` message at repl bottom.
so i would expect the output of my script to be close to 0.
see a full script which reproduces this issue along two different code paths here: phofurl the segfault occurs when reading the dataset a second time.
compile the current r1.15 release candidate and use the train.py script from the od api to train a model.
below is screen output of two conditions: log when normal exit: phofcode log hang: phofcode compare two log, tensorflow do not continue to save checkpoints and output final loss use `nccl/xring` reduce alg.
then ran command to predict: `gcloud ai-platform local predict --model-dir=$model_dir --text-instances ci.txt --framework tensorflow` the ci.txt is a comma separated list of my input numbers.
error when trying to use 2 x tpus v3 in tpuclusterresolver
expected - have two separate graphs, both initialized by the same checkpoin
load local cifarl dataset to numpy array without reporting any errors
multi worker on multiple cpus with evaluator as a separate node.
the only trustworthy metric i see in this labyrinth is nvidia-smi, but only for the batch sizes at which it suddenly steps up.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
gradients should be correct and there should be no difference in gradients whether a placeholder or another variable is used.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: samsung galaxy s6 - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.9 python version: 2.7 bazel version (if compiling from source): 0.19.2 gcc/compiler version (if compiling from source): 6 cuda/cudnn version: 9.0/7.14 gpu model and memory: geforce gtx 1060 6gb
source code of the program is listed in the `code` section below.
when executing a sample rest endpoint, i am noticing a warning on the console.
generates random jpeg quality on graph creation, which is then fixed.
being able to import from tensorflow_core (eg: from import model) with no problem.
2. open a large file, like vim 's `evalfunc.c` file.
the call() methods of tf.keras.layers.gru and others (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i 'm currently working on a costum keras layer and need to use `tf.linalg.expm` several times.
4. the error message "e479: no match" appears.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): osx mojave - mobile device (e.g.
the signature keys for these functions are ambiguous.
tensorflow lite should correctly read a flatbuffers tflite file and build an internal model structure for its interpreter without any errors.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 x64 1809 - mobile device (e.g.
yes, cleanly install windows 10, update to the latest 19.03 release with windows updates, install the latest anaconda, install the tensorflow 2 beta 1 binary, and run this notebook locally.
i read in tutorial phofhyperlink that wrapping iteration through the dataset with tf.function should increase performance, but instead of it computations move from gpu to cpu and as result slow down.
same code works if i use
provide a reproducible test case that is the bare minimum necessary to generate the problem.
run a convolutional neural network with no error messages
component lib 's package.json "main" points to the ssr lib while "browser" points to client output.
simple rnn seems to be doing the right thing, re-sampling dropout masks after each call.
but when i use the test code in the tf doc, it just shows core dumped.
1. install and init a strapi project locally 2. configure an admin user and api permissions 3. create a content type, add some posts 4. configure api permissions 5. deploy on heroku with heroku cli or with github 6. wait 24h 7. after 24h admin users and api permissions are reset and need to be reconfigure
in rnns, the dropout masks should be reset after every call.
the code also works fine without the `opt_a` call.
if including tracebacks, please include the full traceback.
i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 fma traceback (most recent call last): file line 1334, in _do_call return fn(*args) file line 1319, in _run_fn options, feed_dict, fetch_list, target_list, run_metadata) file line 1407, _call_tf_sessionrun run_metadata) error while reading resource variable my_dense/dense/kernel from container: localhost.
sum should not be on the blacklist, and resnet50 should get 709 images/sec.
phofcode and adding: phofcode error looks like: phofcode changing to: phofcode also does not work an end up with:
in 63b74a8 a change was introduced in for which if you add text as tag content on a new line the indent applied is twice shiftwidth().
it shows that value of array is still `['a']` yet each block referencing array from slot, `['b']`.
the x axis represents the resolution which determines the input shape in following manner for _conv3d_ `(1, resolution, resolution, resolution, 16)` determines output shape for _conv3d_transpose_ `(1, resolution, 1)`.
in the first example bellow, the shape of the kernel weights of `dense` should be `(5 * 3, 2)` = `(15, 2)` instead of `(3, 2)`, which is shape of `dense2` (as expected in case `dense2`).
~~sorry, i do not have a simple snippet to reproduce this issue.
i am using java api for prediction and i tried to create a tensor of string type using the following java api: phofcode however, when calling i got the following excetion: phofcode i looked into the code: phofimage it uses as the size of element in byte array.
1382 tuple of tensors `x, weights`.
this error has also been reported in comments of phofurl i have tried with tf v1.12.0 and v2.0.
when assigning to a tf variable with the `=` operator in a tf function, i get a really cryptic error message.
non-tensorflow code gets executed only on the first run of tf.function
should only close those writers that will not be needed any longer.
when verbs support is enabled, getting phofcode
based off of the documentation: > tf.bitcast ( input, type, name=none ) > given a tensor _input_, this operation returns a tensor that has the same buffer data as _input_ with datatype _type_.
`sudo apt install -y python3-virtualenv` 4.
the issue does not occur when it starts running on the main thread instead: phofcode
large logs and files should be attached.
this behavior is reproduced if one or two app windows are open
while all intermediate tensors appear to have fully defined shapes, `xla.compile` will still raise an exception: phofcode
the test_negative test should pass, as the call to bincount with a negative input value is expected to throw an invalidargumenterror.
strapi welcome page opens without any issues.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
iphone 8, pixel 2, samsung galaxy): n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): python version: python 3.5.3 bazel version (if compiling from source): 0.24.1 gcc/compiler version (if compiling from source): 6.3 cuda/cudnn version: 10.0/7.4 gpu model and memory: v100-pcie-32gb
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): arch linux - mobile device (e.g.
capturing via an intermediate variable should never change code behavior.
please specify location of python.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i expect that this should log a warning and halt training.
the e1102 warning should not be raised.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 python version: 3.6.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: 9.0/ 7.4.2 gpu model and memory: gtx 1070 with 8gb memor
i tried this in python2 and there is no such issue.
v3.6.3, it's smooth: phofurl v3.6.4, it's glitchy: phofurl note that it doesn't matter which child has the transition on it.
is this the expected behavior or a bug?
nvidia-smi 2. import tensorflow as tf image_model = weights= 'imagenet ') new_input = image_model.input hidden_layer = = hidden_layer) x tf.zeros([1, 299, 299, 3]) # just use a zero tensor instead of dataset tensor # this will raise error: "internalerror: blas sgemm launch failed : m=5329, n=80, k=64 [op:conv2d]"
play [ask user for the input]
large logs and files should be attached.
the first model in the swagger 3.20 ui and try get /products (clocks).
`node { name: op: input: input: "const_35" input: input: "toint64_107" device: attr { key: "_class" value { list { s: } } } attr key: "dtype" value type: dt_float } attr key: "has_known_shape" value b: true ` however, runmetadata contains below node stat for the node.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
it can 't get input shape in loss function, is that right?
577.49mib 577.49mib 513.98mib 2, 2.
when editing a remote file via netrw on windows 10, the name of target temporary file is in bad form (starting with slash `/`) and makes scp complains about "no such file".
calling `:redraw` after `:echon` a partial line doesn 't make that text visible to user with gtk 3 gui.
applying batch normalization on top of batchwise tf.map_fn should pass without issue, as in this case it should emulate batchwise matmul.
there should not be any dimension error during back propagation for the embedding layer when using subclass api, when the same layer works in functional api with same model/inputs.
aws deep learning ami (ubuntu 16.04, tensorflow 12) on aws p3.8 and p3.16 instances.
i even tried evaluating my old model with these scripts and it still gives me the same error.
the problem is with the android platform, i got false and different values compared to values from python script
i have ~408k samples in my `tf.data.dataset`, am using a batch size of 256, and train my model with 1,000 steps per epoch with 1000 epochs.
putty bash] add any other context about the problem here.
when starting `gvim` with `govim` phofhyperlink which is a channel-based plugin, it fails to fork/detach from the terminal.
<img width="784" alt="a" src=" phofurl - vim version
if including tracebacks, please include the full traceback.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux #1 smp tue jun 5 cst 2018 x86_64 x86_64 x86_64 gnu/linux - mobile device (e.g.
traceback (most recent call last): file "<stdin>", line 6, in <module> file line 179, in sparse_image_warp grid_locations = image_width) file line 34, in _get_grid_locations y_range = np.linspace(0, image_height - 1, image_height) typeerror: unsupported operand type(s) for -: 'nonetype ' and 'int '
load_weights throw exception on a doubly nested model
need substantial infrastructure (roce) and code to reproduce.
vim-issue phofimage - vim version phofcode - os: ubuntu 18.04 - terminal: securecrt -- xtrem mode add any other context about the problem here.
() at no such file or directory.
i found a temporary solution by modifying shared_variable_creator: python def *args,
1. go to phofurl phofhyperlink 2. click on `add new administrator` 3. fill in the fields as normal and click `save` 4. view the `strapi_administrator` table in the postgres db (you should see the new admin created and a hashed password) 5. now click pencil icon to edit admin you just created.
custom loss with multiple input layer works consistent in both distribute and non-distribute env.
i would expect pattern validation to apply tot he default/example fields correct,
the read loop within `govim` is separate from the processing of events.
if including tracebacks, please include the full traceback.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): mac - mobile device (e.g.
` import tensorflow as tf from tensorflow.keras.models import sequential from tensorflow.keras.layers import dense, activation, flatten import numpy as np input1 = np.array([[0, 1, 1, 1, 0, 0, 0, 1, 1]]) doutput1 = np.array([[1, 0, 0]]) model = sequential() initializer = stddev=0.05, seed=1) model.add(dense(25, input_dim=10, initializer stddev=0.05, seed=2) model.add(dense(25, initializer stddev=0.05, seed=3) model.add(dense(10, sgd momentum=0.0, decay=0.0, nesterov=false) optimizer=sgd, metrics=['accuracy']) for i in range(100): sgd momentum=0.0, decay=0.0, nesterov=false) optimizer=sgd, metrics=['accuracy']) model.fit(input1, doutput1, epochs=1, batch_size=1) output1 model.predict(input1) output1 `
terminating a command with `job_stop()` should always result in a non-zero exit code.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): source from tf2.0 branch phofhyperlink tensorflow version (use command below): 2.0.0a0 python version: 3.5.2 bazel version (if compiling from source): 19.2 gcc/compiler version (if compiling from source): 6.3 cuda/cudnn version: na gpu model and memory: na running on cpu, machine don 't have any gpu
the original notebook (from francois chollet) is here: [link to github phofhyperlink includes correct output.
filereadable() won't return _true_ for filepaths relative to users home directory e.g.
- have i written custom code: no - os platform and distribution: windows 10 (will also test on ubuntu) - tensorflow installed from: binary - tensorflow version: 2.0 preview python version: 3.6.6 cuda/cudnn version: 10 gpu model and memory: gtx 1070 8gb
bundle any svelte app with rollup 1.21+.
`return tf.matmal(x,x)` does not yield this extreme leak of memory.
border and padding of popup windows, like after calling `popup_atcursor()`, are always cut off on the right side when the content doesn 't fit on the screen.
... steps to reproduce the behavior: 1. view a document with an operation that (a) has a very long path and (b) has a non-empty summary or a summary with single very long word (like namespace) 2. note the 'squished' appearance of the operation summary or the path.
example: phofcode 4. now press `g` to jump to the bottom of displayed buffer.
large logs and files should be attached.
`vim -u bug.vim -p a b c` 3.
detailed steps to reproduce the behavior: 1. run `gvim --clean` on pc with touch screen 2. split window ctrl-w v 2. type long text with many rows 3. activate left window touch it 4. try scroll left window touch the window with your finger and slide it up 5. right is scrolled.
screenshot phofimage screenshot phofimage screenshot phofimage screenshot phofimage
according to the documentation phofhyperlink : `given a list of tensors or ragged tensors with the same rank r (r >= axis) [...]`.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: n/a gpu model and memory: n/a traceback from model subclassing code: phofcode
the cursor is positioned inside the popup window and can 't be moved out of it (when going into insert mode, text will be inserted into the popup window).
phofhyperlink want to do this because am trying do model parallelism for this rnnlm (reccurrent neural network for language modeling) code and want place ops after running a heuristic graph partitioning algorithm which takes as input estimated compute time as well dependencies for each operation.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i couldn 't even kill it.
related: #4718 (and closed in
- have i written custom code: yes, but very little - os platform and distribution: centos 5 - tensorflow version (use command below): both 1.18 and 1.12 - python version: both python 2 and 3 gpu model and memory: titanx 1080ti
large logs and files should be attached.
but here is crux of it, phofcode and fit a `tf.keras.model` like so, phofcode
if including tracebacks, please include the full traceback.
- eta: 0s loss: 0.2971 accuracy: 0.9123traceback (most recent call last): file "c:/users/harald line 42, in <module> model.fit(x_train, y_train, epochs=5, callbacks=[gradient_cb, tensorboard_cb]) file "c: users harald line 643, in fit file "c: users harald line 664, in fit file "c: users harald line 439, model_iteration epoch_logs) "c: users harald 295, on_epoch_end logs) "c:/users/harald 32, on_epoch_end data=t) 77, histogram tensor = _buckets(data, bucket_count=buckets) 139, _buckets return tf.cond(is_empty, when_empty, when_nonempty) 1382, cond_for_tf_v2 return cond(pred, true_fn=true_fn, false_fn=false_fn, strict=true, name=name) 507, new_func return func(*args,
phofcode for most part, i followed this guide on how work with duplicatestrategies: phofurl
the current behavior is that the graph first gets converted to a trt graph with trtengineops, but then when it gets calibrated for the int8 quantization, an error occurs.
large logs and files should be attached.
large logs and files should be attached.
but, do not get any prediction when use left which generated using above script am these predictions on open cv dnn module `tensorflownet = how do convert mobilenet frozen inference graph into proper format so that can get inference ?
- vim version: - os: ubuntu 18.04 - terminal: gnome terminal the problem is probably caused by: phofurl when `base` contains single quotes, they are not escaped, resulting in an invalid `expr` passed to `filter`.
6. the output of `:echo "foo bar"` has been cleared.
- have i written custom code: na - os platform and distribution: ubuntu 16.04 lts - mobile device (e.g.
i 'm not sure why it takes so much longer using the custom train function.
1. create new content type (let's call it `page`) 2. add new field, for example a string type with the following name - `title_en` 3. go to: 4. open filters panel, choose `title_en` is `123`, press enter 5. you will see the following filter text: image phofimage so it cannot search because of this.
the files are sometimes corrupted on s3.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): binary tensorflow version (use command below): tf version = tf git version = python version: 3.6.4 bazel version (if compiling from source): na gcc/compiler (if compiling from source): na cuda/cudnn version: na gpu model and memory: warning: logging before flag parsing goes to stderr.
the proccess is very demanding in terms of memory and reaches over 19 gb just before it stops.
devices: streamexecutor device (0): <undefined>, <undefined> traceback (most recent call last): file "landmarker.py", line 140, in <module> landmarker = landmarker(); file "landmarker.py", line 18, in __init__ self.model = file line 146, in load_model return custom_objects, compile) file line 215, in load_model_from_hdf5 model.layers) 749, layer, weight_values, original_keras_version, original_backend) 374, weights = 350, convert_nested_model 374, weights = 350, convert_nested_model 374, weights 350, convert_nested_model 374, weights 350, convert_nested_model 362, 455, if != weights[0].shape: indexerror: list index out of range it is worth noting that serialized model is 92 mb while checkpoint is over 500 mb.
the model (*pb file) works fine in ubuntu.
i would like to pass my flattened data into either a dense or an rnn layer.
after applying fix cebce4a phofhyperlink for bug #25327, the code provided below results in `valueerror: input 0 of node was passed float from lstm/kernel_1:0 incompatible with expected resource.`.
see below for the reproducible example.
windows 10 1806] - redis-server version [e.g.
n, m = dense_shape r = num_edges = int(mean_edges * n) flat_index = * n * m).astype(np.int64) # flat_index np.arange(0, n * m, m)], axis=0) flat_index dtype=np.int64) i, j (n, m)) # pylint: sparse_indices np.stack((i, j), axis=-1) weights return sparse_indices, weights def sparse_sum(sp, axis=1): """sparse.reduce_sum."""
a future version of pip will drop support for python 2.7.
i was trying to upgrade my tensorflow from 1.13 to tensorflow 1.14. i noticed that the training speed for my models dropped a lot.
getting this error when i am tring to start the training in images.
vim will echo the `e325` message.
1. :set fileencoding=latin1 1. write an emoji into the buffer.
but in tensorboard graph, i cannot see any differences on fusedbatchnorm part with and without relu.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: oneplus 3 , android 8.0.0 - tensorflow installed from (source or binary): source tensorflow version (use command below): tf 1.13 python version: 3.5 bazel version (if compiling from source): 0.24.1 gcc/compiler version (if compiling from source): 5.4.0 cuda/cudnn version: 5.4.0 gpu model and memory: nil 1.
when the batch size is small (e.g.
some specific set of circumstances skip over assigning `packed` to a value.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
unfortunately, when passed python scalars or arrays of scalars, it seems to create a new function for each different value, instead of each different type (see code example below).
pop phofimage - vim version : - os: centos 7 - terminal: gnome terminal it would be much appreciated if the pop up window can be focusable.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): pip install tensorflow==2.0.0-alpha0 tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6.8 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: na gpu model and memory: na traceback (most recent call last): file line 1161, in visit super(astannotator, self).visit(node) file line 127, in visit super(basevisitor, self).visit(node) file line 253, in visit return visitor(node) file line 47, wrapped f(self, node, *args,
explicitly calling .gpu() makes tf.cast() work, even though .device tells me it 's gpu.
at this point, mismatch arrises.
note that object is never used inside model, hence we can also pass `none` instead.
but bizarrely fails when run via travis ci, using exactly the same docker image: phofcode the failure above indicates the autocmd was _not_ triggered, because the quickfix window was empty when written to `errors`, hence comparison failed.
should be able to import tf.broadcast_weights in tf 2.0
i don 't understand the error message.
`model.evaluate()` prints out an insanely long progress bar at the end.
4. hit `i` once to change to long listing style.
phofurl 3. expand an operation
explicitly in phofcode what am i missing?
` std::vector<int> sizes = {1, 240, 240, 3}; sizes); if != ktfliteok) { log(info) << "failed to allocate tensors!"
... steps to reproduce the behavior: 1. statically serve the `swagger.json` shown above 2. type some invalid json (e.g., foobar) in the only field 3. click execute 4. see error
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary (pip) tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6.8 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: this could be related to #28710?
i think the problem is related to the magic behind tf.function.
i was hoping to find a way to save and restore the state of the random number generator.
it seems to be associated with randomshufflequeue or fifoqueue because they seem to create threads.
so there's going to be set of 1 image, and maybe that doesn't work with batchnormalization.
i except to be able to save any tf.keras model (except those with dynamic layers) that uses only standard layers.
my command for running the experiments: phofcode ##
image phofimage the response content type render correctly, and we can chose the item in the drop-down box successfully.
when the decorator is omitted it returns a ragged tensor.
small example to reproduce the issue: import numpy as np import tensorflow as tf from tensorflow.python.keras import layers from import model_to_estimator_v2 x_shape = (3,) n_class = 5 batch_size = 10 x = name="x", dtype=tf.int64) class def build(self, input_shape): self.y tf.random.poisson( lam=10, shape=(batch_size,) + x_shape, dtype=tf.int64 ) def call(self, inputs,
i get a `typeerror` exception when i call a `dense` layer which was created with
should be able to connect after a restart.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 x64 1809 - mobile device (e.g.
expected editor not to crash.
tensorflow mistake primtive type objects as tensors.
algo fails to generate wireguard private keys, then fails to generate public keys because they 're missing.
i load dataset from `tensorflow_dataset`.
unexpected exception, this is probably a bug: cannot pickle '_io.textiowrapper ' object" 9. i then "./algo -vvv", and get output i put in full log section of this issue.
detailed steps to reproduce the behavior: 1. run `vim --clean` (or `gvim --clean`, etc.)
if overloading the loss function in `tf.keras.losses.loss` the created model can be compiled, trained and saved but not loaded.
large logs and files should be attached.
`runtime/gvim.desktop` contains duplicate esperanto translations for genericname and comment.
i've already reported an issue phofhyperlink where the `call` method of `sequencefeatures` used unnecessarily requires its input to be a `sparsetensor` instead of a regular `tensor`.
steps to reproduce the behavior: 1. roll out a vpn service to any provider (i used azure), default settings plus dns ad blocker.
a note in `dense` documentation phofhyperlink says that > note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.
> t [[node cond/matrixsolve (defined at ]]
provide a reproducible test case that is the bare minimum necessary to generate the problem.
if including tracebacks, please include the full traceback.
i have previously running code that explodes on import prior to any runtime execution.
does the wifi network exclusion functionality not work with the wg client, is it ikev2 only?
2. when the `#{cursorline: 1}` option is passed to `popup_create()`, the popup 's cursorline is also visible in window that displays buffer that was passed to `popup_create()`.
... `docker run --publish --rm --read-only
@tower.py:130] building graph predict tower 'tower-pred-0 ' on device /gpu:0 ... @collection.py:151] size these collections were changed tower-pred-0: loading annotations into memory... done (t=0.51s) creating index... index created!
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): `master` (reproduction instructions below) python version: 3.7.2 bazel version (if compiling from source): 0.21.0-1 gcc/compiler version (if compiling from source): 8.2.1 cuda/cudnn version: n/a gpu model and memory: n/a you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
the image family says they are intel optimized packages but when i rung benchmarks with verbosity , i do not observe any mkl related stuff.
i am overriding the __init__, build and call functions while making the appropriate super calls as per your article.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): deep-learning image tensorflow version (use command below): 1.11 python version: 2.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: n/a you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" when running using google deep learning image m9 on gpu machine (image : tf-latest-cu92, m9) .
582 return valueerror: `handle` is not available outside replica context or a call.
that the topkv2 op functions as specified, or that the `top_kv2` python binding is exposed.
as dy/dx = a2 + b, i expect result of 32.0 for dy_dx
on gpu, the _conv3d_transpose_ operation shows a huge spike in terms of execution time when increasing input size from `(1, 128, 128, 128, 16)` to `(1, 256, 256, 256, 16)`.
i 'm trying to use contrib/quantize module to perform quantize aware training and convert the trained model to quantize tflite model.
i changed the current demo app to gpu experimental and i had to do some minor changes in the code to get the app running successfully on gpu.
large logs and files should be attached.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): include any logs or source code that would be helpful to diagnose the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux (google colab) - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): custom-ops container - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux fedora 26 - mobile device (e.g.
this makes training results non-deterministic since nadam optimizer depends on current iteration number when calculating weight updates.
... 1. open phofurl 2. hit the "initialize" button 3. observe oas3 badge disappearing
stacktrace of failure: traceback (most recent call last): file "../_task_commons.py", line 59, in _get_experiment experiment = file "../__init__.py", line 233, in _new_experiment_fn file "keras_example.py", line 76, in experiment_fn file line 484, model_to_estimator config) 367, _save_first_checkpoint saver.save(sess, latest_path) 1441, save checkpoint_file}) 929, run run_metadata_ptr) 1152, _run feed_dict_tensor, options, run_metadata) 1328, _do_run run_metadata) 1348, _do_call raise type(e)(node_def, op, message) input/output error [node save/savev2 (defined at keras_example.py:76) = savev2[dtypes=[dt_float, dt_int64, dt_float, dt_float, dt_float, ..., dt_float, dt_float], phofhyperlink ]]
dataset dataset.repeat(n_epochs) in memory training doesn 't use batching.
these problems did not occur in tf 1.12: 1. i get an exception if i call `fit()` again on the same model: phofcode i can workaround this problem by recompiling the model.
- os platform and distribution: ubuntu 16.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0.0-alpha0 - python version: 3.6.7 phofcode
provide a reproducible test case that is the bare minimum necessary to generate the problem.
it crashes the rtx 2060 (there's a bug report for that already, this is not this issue) if i don't set options allow_growth or limit ram usage.
5. observe output (should be `formatoptions=croql last set from .../ftplugin/vim.vim`)
calling `appendbufline()` against an empty file will introduce an extra unexpected line.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary):binary tensorflow version (use command python version:3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version:9.2, cudnn7 gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
* reproductions should be small, self-contained, correct examples c phofurl occasionally, this won 't be possible, and that 's fine c we still appreciate you raising issue.
# binarization >= .5] = 1.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
snmek z phofimage (the first comment is wrong, the second (multiparagraph one) is right (fixed with temporary ``set paste``)) - vim version: both brams vim 8.1 (2018 may 18), 1-2148, and neovim built from the master branch commit phofurl (post 0.4.2) - os: opensuse/tumbleweed - terminal: gnome terminal, 3.34.2 (but was able to reproduce it a long time ago) probably of interest to @marshallwar
i 've set up a cnn and have created callbacks for model checkpointing, tensorboard visualization, and early stopping.
get an error about bad model identifier.
most mkl primitive factories ignore and even some that know about it may choose to ignore it based on other heuristics like batches smaller than 32 phofhyperlink for example (i am not sure this is intended behavior).
i am expecting that i can get a custom model to work on the coral tpu.
phofcode looks like response is a list and not a dictionary.
perhaps related to the `tf.function` compilation?
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): 18.04 - mobile device (e.g.
;-) or else, i can work around the issue by using `autograph=false`.
in the same way, i 'm using randomly generated data, but this happened to me when training with real images and masks, and also with higher batch size (i was using the recommended 1024, but i used 128 here so example executes quicker) phofcode output: phofcode
please help me get an accurate measurement of how much memory my current model and batch size is really taking to run on a gpu with tf.
when include tensorflow header to use the c api in my project, it always gives me warnings warning c4190: 'tf_newwhile ' has c-linkage specified, but returns udt 'tf_whileparams ' which is incompatible with c. it is a bug introduced by the tensorflow c api header.
we load tabular data from a database to a training script, and this tabular data contains quite a many columns (30 - 40), most of them scalar.
if including tracebacks, please include the full traceback.
by simply calling the `call` method of `densefeatures` and `sequencefeatures` defined with and feature columns (along with `embedding_column`), we get warnings that the deprecated attributes `_num_buckets` are used (instead of the non-deprecated `num_buckets` i guess).
i used both the estimator.evaluate function hand-made prediction calculation with exported embedding matrix, got same low auc value.
i set inputs are tensors which contain only ones.
run the following code: phofcode here the `rhs` argument to `cholesky_solve` ends up as an empty tensor (it has a shape of (0, 1)), which appears to cause the segfault.
in this code, the terms in the condition above have below values.
to be able to send keys/commands to the popup window without them being intercepted by the filter menu.
these two classes are graphs compiled by xla.
if required, i can spend time setting up a concrete sample.
... create a new angular project then do `npm install swagger-ui` and add code to the app.component.ts: phofcode then run `npm start` and you will get an error
using results in failed to create a directory.
there are several steps that i will describe to accurately reproduce my problem.
if including tracebacks, please include the full traceback.
but if you look at vim window, you will see following:
memory is released after `sess.run` and closing session, like in case 3 above but for gpu.
i have fo+=an, tw=75 and autoindent set, and select 2 lines and press gq, and the word moved to the next line ignores the numbering-indent, only after a further edit the indent jumps in correctly.
1. after (a)borting or (q)uitting, vim opens an empty popup window: phofimage 2. pressing `<c-w>z` to close the preview window doesn 't work: phofimage
this effect does not affect eager mode or v1 graph mode where the execution directly runs at its target speed and memory usage.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): centos linux release - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
but when we enable xla, traffic between gpu is suspend until finish of back-propagation my figure (blue line).
float16 matmul is way slower than float32 matmul on cpu
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux rpi3-0 #1159 smp sun nov 4 gmt 2018 armv7l gnu/linux - mobile device (e.g.
parameters info:tensorflow:saving checkpoints for 0 into info:tensorflow:loss final step: none.
(you may need to `touch ~/templates/new text document.txt` if you don 't have one.
`vim --clean -c 'set noesckeys'` 2. enter insert mode and type <kbd>c</kbd>
the cursor should end up on the 'e ' character right after after the search in step 4.
expecting screenshot like below, i got this when i have installed cloudflared locally and configured it as dns server without connecting to vpn.
steps to reproduce the behavior: 1. go to '...' 2. click on '....' 3. scroll down to '....' 4. see error
after nat routing inside vpc, both clusters use port 6379) 3. connect to cluster with non-6379 port via nat, note the keys shown in rdm 4. refresh data for the cluster, and note keys are now shown for 6379 cluster.
i am trying to load a model saved in 1.x in 2.0.0. here is the model: phofurl this is the error i am getting: keyerror: <tf.tensor 'map/while/switch_1:0' shape=() dtype=float32> the same model loads fine in 1.13
is this a bug, or am missing something my understanding of timeline?
when changing the type to tf.float32 the error is not raised.
dist/index.html) updated `index.html` with url ` phofurl open the `index.html`
cindent indentexpr= cinoptions=#1,#0 string[] defs = finddefs( new string[] { "postprocessing.editor", "recorder", }); c# also has anonymous functions, so jn option is indispensable.
tflite_convert --graph_def_file --inference_type quantized_uint8 --input_arrays truediv --input_shapes --output_arrays softmax_tensor --std_dev_values 1 --mean_values 0 --default_ranges_min 0 --default_ranges_max 6 --output_file mobilenetv3.tflite
phofcode the output is: phofcode
* there also appear be two callbacks made, when in reality there could (i think) be one
if including tracebacks, please include the full traceback.
if including tracebacks, please include the full traceback.
phofurl - remove debug, it compiles properly - remove the inner loop, and it compiles properly.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): windows - mobile device (e.g.
in first 50 have faced above issue 5 tf.image.decode_bmp works remaining 45 images.
behaviour should be the same for lists or tuples.
it upsampled kernels instead of space_to_batch + batch_to_space.
then, applied same changes object detection as did current demo before (by using knowledge of demo version r1.13).
large logs and files should be attached.
segfault at exit when unloading the tensorflow plugin in autodesk flame 2020.0 error message phofcode stacktrace phofcode symbol at c++filt nsync::(anonymous see: phofurl
particularly, error is not properly calculated, so the network is not trained at all
phofcode phofcode phofcode warning:tensorflow:from colocate_with (from is deprec ated and will be removed in a future version.
1. run `vim -u .vimrc` where .vimrc contains the single `set shell` line of configuration.
moreover, maybe because we are in the early stage of the development, i'm unable to convert a loop that loops over dataset to its graph representation.
->89.83% 0x6b8e6b9: mmap (mmap.c:34) | ->48.74% 0x6b0a3cf: new_heap (arena.c:438) | | ->48.74% 0x6b0ac1f: arena_get2.part.3 (arena.c:646) | ->48.74% malloc (malloc.c:2911) ->48.74% operator new(unsigned long) (in ->32.49% 0x4bdd65: ???
in particular, the first run takes a * long * time and a * lot * of memory, both scaling seemingly exponentially with the `n_iter` parameter below.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below): tf2.0 alpha python version: 3.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
space cost comparison tensorforest: import time import tensorflow as tf (x_train, y_train), _ = x_train = x_train.reshape((-1, # produce more examples y_train = y_train.repeat(3, x_train = x_train.repeat(10, axis=0) # repeat 10 times to produce more examples y_train y_train.repeat(10, axis=0).reshape(-1) print(x_train.shape, y_train.shape) # totally examples # 1024) start_time time.time() est_args { 'num_classes ': 10, 'num_features ': 1024, 'regression ': false, 'num_trees ': 500, 'max_nodes ': 10000, 'base_random_seed ': 0} params
i should see "value: **** * * "
provide a reproducible test case that is the bare minimum necessary to generate the problem.
[*] reading checkpoints... warning:tensorflow:from checkpoint_exists (from is deprecated and will be removed in a future version.
the bias vectors should be shaped as they were in the original freeze graph model.
but if that reason, then why `tabenter` and `tableave` when invoked `-p`?
assert tf.executing_eagerly can trigger an assertion failure.
workaround for me is to get the code ad the previous version: phofurl
> algo running on: ubuntu 18.04.2 lts (virtualized: xen) zip file created: +0000 python 2.7.15rc1 runtime variables: algo_provider "local" algo_ondemand_cellular "false" algo_ondemand_wifi "false" "x251bgw=" algo_local_dns "false" algo_ssh_tunneling "false" algo_windows wireguard_enabled "true" dns_encryption "true"
should not be trying to access gcp metadata endpoint.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): 10.14 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): python version: 3.7
the code in the function passed as an argument to .map() should not be run until the generator has yielded a value for it to operate on, or an input argument for map allowing this behavior to be specified should be added.
without making any changes whatsoever, boot up today and run the same models get an error telling me that cudnn cannot initialise as tensorflow could not create a cudnn handle.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i think that somewhere in the code of the `call` method of `densefeatures` and `sequencefeatures` there is a use of `_num_buckets` that should be replaced by `num_buckets`.
* placing the cursor at line 8 using `ip` operator should select lines 8, 9 10.
a result of image classification should be returned.
when i run tf.gather, it returns invalid argument error.
the automatic naming of keras metric layers is not consistent.
which one is correct set we can see queuedequeuev2 is on
i am using colorscheme phofurl
do `./configure [your flags] && make && make test`.
pixel values should be the same regardless if loading the image by pil or by tf
here is the stacktrace: pycon typeerror traceback (most recent call last) in <module> 4 inputs = 5 hidden1 = ----> 6 mean = in __call__(self, *args,
there are comments in the example ( repl phofhyperlink ).
3. run vim, see cpu usage is normal 4. click somewhere, see cpu usage go to 100% until you quit vim.
no error regardless of the terminal column size.
this is because the rnn code checks that cell explictly defines `training` flag as argument, which does not.
this should work the same as when using a lower level implementation with a gradienttape (working fine, no error)
- reproduced on macos and on colab.
i would expect that in the right window: - the ` 'list '` option is set to `1` - the end of lines are displayed with a dollar character - the tab characters are displayed with `i` because, according to `:h local-options`, vim should have remembered that `x` buffer was displayed in left window, and in that window ` 'list '` option is set to `1`.
tf only use 1 thread and 1 cpu core
basically we are executing this code on each worker: phofcode full code example is here: phofurl
got the following error: task [delete the ca key]
1. create a model in content type builder 2. make every action public on this model 3. launch a "find" query on this model, adding an authorization header with random content (using postman, curl...) 4. see result: phofcode
have tried many hyper-parameters settings.
--config=gdr # build gdr support.
i recommend to restart the program if it did not occur within 1 minute.
when trainable set to true, it gets following error: invalidargumenterror: operation expected a list with 3 elements but got list with 4 elements
just running a basic image classifier with keras.
using various techniques i will describe below, load the using tf.keras, introspecting the summary inputs outputs.
saved the model with model.save_model().
* phofurl * traing using two virtualdevices after 10 epoches * when only one gpu is detected, two virtual devices will be created.
3. open and source the saved script 4. :call startupcurrentlinebug() 5. a popup window with items a, b, c is displayed; item a is highlighted (_wrong_) 6. press 'j ' to select the next line 7. item c is highligted (_correct_)
1. if the state of the random number generator is saved and restored, then out[1] in save_rng.py should have the same value as out in restore_rng.py.
and crashing with the following error messages "runtimeerror: is not supported when eager execution is enabled."
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no, using the sample code in tensorflow docs - os platform and distribution (e.g., linux ubuntu 16.04): win10 - tensorflow installed from (source or binary): pip install - tensorflow version (use command below): 2.0.0-alpha0 python version: 3.7 cuda/cudnn version: cuda10/cudnn7.5 gpu model and memory: 2*1080ti include any logs or source code that would be helpful to diagnose the problem.
if including tracebacks, please include full traceback.
if including tracebacks, please include the full traceback.
make it so that tensors don't always have be divided?
can someone tell me what went wrong, or what i need to change to make this work?
- on line 63 there's a call to which should be moved to `tf.sparse.to_dense` - on lines there's a tf.session context and `tables_initializer` which no longer exist.
unfortunately it crashes with segfault(core dumped) on the terminal.
i have predicted image from frozen graph model and tensorflow lite using python script and it works fine.
tensorflow should refuse to perform a sparse-dense multiplication that requires broadcasting.
6. press ctrl-l 7. observe: the cursor jumps to correct position - to 'e ' char.
stride1 1 the stride of the sliding window with num_in_channel w tf.get_variable( 'w ', dtype=tfgraphnumbertype, shape=[filter_size1, filter_size1, num_in_channel, num_filters1], w) b tf.get_variable( 'b ', dtype=tfgraphnumbertype, shape=[num_filters1], b) layer tf.nn.conv2d(padding0, w, strides=[1, stride1, stride1, 1], padding="same") layer += b conv1 tf.nn.relu(layer) shape padding1 padding(conv1, 'pad1 ') pool1 tf.nn.max_pool(padding1, ksize=[1, 2, 2, 1], strides=[1, 2, 1], padding="same", name= 'pool1 ') 2nd convolutional layer filter_size2 5 convolution filters are x pixels.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04.5 x86_64 - mobile device (e.g.
psi_tf @tf.contrib.eager.defun def f2(p): rephrase high-dimensional transpose as matrix transpose.
a user should be able to decide what format options they want by editing it in their vimrc.
only few values each data/tfrecord file (of which many exist) are relevant.
squash other windows which you don 't want to).
phofcode output log tensorflow 2.0.0-alpha0 model: "res_net34" layer (type) output shape param # conv2d (conv2d) multiple 9472 batch_normalization_v2 (batc multiple 256 re_lu (relu) multiple 0 max_pooling2d (maxpooling2d) multiple 0 sequential (sequential) global_average_pooling2d (gl 0 dense (dense) total params: trainable params: non-trainable params: 17,024 ``
after enabling log output on the wireguard save step i saw "aborting, target uses selinux but python bindings arent installed".
phofhyperlink introduced a small regression.
scatter_max does not work with mirroredstrategy since v1.11.0.
(init_gpu()) use the following command to run the script phofcode results after 10 epoches phofcode traing using two gpus (non virtual) after 10 epoches if more than one gpu is visiable, no virtual device will be created.
during handling of the above exception, another exception occurred: invalidargumenterror traceback (most recent call last) <module>() 3 x = feed_dict_x, 4 y = feed_dict_y, ----> 5 batch_size = batch_size 6 ) fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps,
the maven setting: phofcode phofcode
actually svelte won't define any attribute if there is a property with the exact name.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
-iproto -dhave_config_h -dfeat_gui_gtk -pthread -i/usr/include/gtk-2.0 -i/usr/include/cairo -i/usr/include/pango- 1.0 -i/usr/include/atk-1.0 -i/usr/include/cairo -i/usr/include/pixman-1 -i/usr/include/libpng16 -i/usr/include/pango-1.0 -i/usr/include/harfbuzz -i/usr/include/pango-1.0 -i/usr/incl ude/glib-2.0 -i/usr/include/freetype2 -i/usr/include/libpng16 -i/usr/include/freetype2 -i/usr/include/libpng16 -g -o2 -u_fortify_source -d_fortify_source=1 linking: gcc -l. -wl,-bsymbolic-functions -wl,-z,relro -wl,-z,now -fstack-protector -rdynamic -wl,-export-dynamic -wl,-e -l/usr/local/lib -wl,--as-needed -o vim -lgtk-x11-2.0 -lgdk-x11-2.0 -lpangocairo-1.0 -latk-1.0 -lcairo -lgdk_pixbuf-2.0 -lgio-2.0 -lpangoft2-1.0 -lpango-1.0 -lgobject-2.0 -lglib-2.0 -lfontconfig -lfreetype -lsm -lice -lxpm -lxt -lx11 -lxdmcp -lsm -lice -lm -ltinfo -lnsl -lselinux -ldl -l/us r/lib -llua5.1 -wl,-e -fstack-protector-strong -l/usr/local/lib -lperl -ldl -lm -lpthread -lcrypt -lpython3.6m -lpt hread -ldl -lutil -lm -lruby-2.5 -lpthread -lgmp -ldl -lcrypt -lm - os: [e.g.
1. start vanilla vim: `vim -u none -c 'set ft=java | syntax on'` 2. paste the above example java code.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 x64 - mobile device (e.g.
dataset initializes and no segfault when creating next batch
4. load your swagger ui website.
... steps to reproduce the behavior: 1. go to phofurl 2. see?
segfault when running an estimator test
i train neumf model in distribute environment, and i find that tensorflow hang all the time when training finish.
follow the steps of the contributing guide: phofurl 1. git clone phofurl 2. cd strapi 3. yarn setup 4. strapi -v
the last `gk` moves the cursor at the start of the current line.
no error and able to get checkpoint path.
warning:tensorflow:using temporary folder as model directory: /tmp/tmpigyst0 info:tensorflow:using config: 600, '_session_config ': gpu_options { force_gpu_compatible: true } allow_soft_placement: true , 5, '_task_type ': 'worker ', '_train_distribute ': object at '_is_chief ': true, '_cluster_spec ': object at '_model_dir ': '/tmp/tmpigyst0 ', '_protocol ': none, none, 10000, '_service ': none, '_num_ps_replicas ': 0, '_tf_random_seed ': none, 100, '_device_fn ': 1, '_task_id ': 0, 100, ' ', '_eval_distribute ': 0, '_master ': ' ', none} model_fn (<function model_func at includes params argument, but params are not passed to estimator.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): pip install tensorflow version (use command below): 2.0 python version: 3.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: friction log: phofurl
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command 1.11.0 python version:3.6.5 bazel version (if compiling from source): 0.15.0 gcc/compiler version (if compiling from source):5.4.0 cuda/cudnn version:9.1/7.1.2 gpu model and memory: aws g2.8large grid k520 4036mib error log: phofhyperlink
provisioning in ec2 causes the error `ec2instance create_failed: the requested configuration is currently not supported.
i could successfully run individual `tf_library`s and get compiled graph classes bazel_genfiles/, and both undefined symbols are "extern c" variables found graphs ' header files, and it seems root definition is not found somehow.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
database name: my_api_db host: host: +srv connection: (false) false +srv connection: false port (it will be ignored if you enable +srv): (27017) port (it will be ignored if you enable +srv): 27017 username: ' ' username: ' ' password:
1. clone the mlperf reference from phofurl 2. pass config object to estimator.train that turns global jit compilation on.
specifically, i am using 4000 images from the wider faces data set for training/validation, and using for fine-tuning.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - running with `model.run_eagerly` works.
phofcode phofcode 2. on the other hand the below code does not compile: phofcode phofcode
the following code is adapted from the guide writing layers and models with tensorflow keras phofhyperlink .
as described above, the normal code with a attentioncellwrapper, and the function that triggered error is as follows.
i have checked that the second argument to `reduce()` is indeed a generic iterable with an `__iter__` method.
i am getting two different variable names from keras layers when variables are built.
notebook1: save the keras model like this (abbreviated steps): from import inceptionv3 base_model input_shape=input_shape, classes=nb_classes) base_model.layers.pop() base_model.outputs = output = output = new_model = model(base_model.input, output) notebook2: load the keras model with tf.keras new_model attributeerror: module has no attribute 'slice '
... lnowak (#1238) noted an issue where a file download triggered via the content-disposition header will result in a link to download the file, but the file will be corrupt.
the speed of mobilenet 's inference on snapdragon 845 is expected faster than 100 ms. and how could i know whether the benchmark is built with neon or not?
... steps to reproduce the behavior: 1. go to phofurl 2. use example definition above 3. click "try it out" 4. put 10000 for `test` 5. click execute
however, when i tried the deeplab mode with xception65 phofhyperlink , the tflite perform differently on cpu and opengles delegate.
while training the model after a number of epochs, a memoryerror suddenly occurs with top error as `operation 'simple_rnn_1/while ' has no attr named '_xlacompile '`.
when called repeatedly, compute_output_shape may return incorrect results due to a faulty caching implementation in keras.network
a result, possibly incorrect (due to too small dtype), or some other way to deal with the issue, e.g.
repl 1 with bug: phofurl repl 2 working without slot default: phofurl
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: oneplus3, poco f1 - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13 python version: 3.6.8 bazel version (if compiling from source): nil gcc/compiler version (if compiling from source): nil cuda/cudnn version: nil gpu model and memory: nil
phofcode the model i used phofurl
- os platform and distribution: ubuntu 16.04 - binary installation (sudo pip3 install tf-nightly-2.0-preview) - tensorflow version (use command below): python version: 3.5.2 cuda/cudnn version: 10.0 gpu model and memory: nvidia gtx 1080 ti 11gb
i currently have it as a frozen graph.
there is no warning shown.
moreover, i cannot specify an `input_shape`.
`tf.function` raises an `oserror` exception with the message `"could not get source code"` when i run it in a python shell.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): 16.04 - mobile device (e.g.
compute_bleu.py should work, but does not (only when i replace newstest2014.de with an older version (text format)
note that for the sake of keeping the example simple, i simulate loading the weights and the values of min/max nodes (learned during training) by simply running phofcode gives following error: phofcode when i look at content of tf.global_variables(), i see nothing related to bias_quant: phofcode same thing when i look as graph in tensorboard: phofcode picture-of-tensorboard phofimage
test fails when terminal has many columns (>= 203 columns).
issues: error: expected member name or ';' after declaration specifiers pytype_slot *slots; /* terminated by slot==0.
- vim version - os: ubuntu 18.04 - terminal: gnome terminal
1 installed using `npm i -g strapi@beta` 2 tried to create a new project using `strapi new cms --quickstart`
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.12.0 python version: 3.5 bazel version (if compiling from source): 0.19.2 gcc/compiler version (if compiling from source): gcc (ubuntu 5.4.0 cuda/cudnn version: n/a gpu model and memory: n/a i have a patch that i will post shortly.
it takes some time until cursor moves to next match.
5. run it thousand times (`let starttime = localtime() | for i in range(1000) | call expand("%:p") | endfor | echo localtime() - starttime`).
this also only seems to affect vim 8+.
model.evaluate should work and provide a result close from the last fit iteration
although adding a seed argument to random_shuffle makes the problem go away, random_shuffle should still use graph-level random seed when seed argument is unspecified.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): python version: 3.5.6 (anaconda) cuda/cudnn version: cpu version i am trying to export a very simple model (see code below) into tensorflow lite with loss function [1] and gradients [2] calculation support using the official tutorial [3].
if including tracebacks, please include the full traceback.
large logs and files should be attached.
- os: ubuntu 16.04 64-bit - both terminal and gvim
when i try to run estimator in distribute with strategy, the train_and_evaluate api do not run evaluation after model save checkpoint.
i follow the instructions from phofurl and in the past i was able to run training on their dataset (not inside a docker container).
steps to reproduce the behavior: 1. start fresh ubuntu 18.04 x64 instance on ec2 (t3.small) 2. installed docker - `sudo snap install docker` 3.
vim crashes (sometimes) with segv when clicking a winbar in a non-current window and the menu command closes the current window.
3. start strapi for the first time.
i tried doing it the other way round, looking at `/mnt/c/` files from linux vim (since linux accessing `/mnt/c` is even slower than windows accessing ` wsl$ arch`), but it didnt demonstrate the issue.
3. create a file through nautilus (right-click -> new document -> new text document).
dict inputs and outputs seem to be allowed in the code (and in the tf.keras documentation), however the functionality seem not to be functional yet.
gradient descent uses wrong gradients).
this example can reproduce this bug using a single conv2d layer.
read on an empty `w+` file should return an empty string.
a border of the popup shows without the breaks.
after a reboot of the server, dnscrypt-proxy no longer works on vultr.
... steps to reproduce the behavior: 1. go to phofurl 2. paste the example 3. see error
as side note, seems adding `timedistributed` layer `dense` layer or when `return_sequences=true` does change anything
the color should be the same as background of popup window.
it is also not fixed in r1.14, which i confirmed by compiling from source (although the line number changes).
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 python version: 3.6.7 anaconda 64bit bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: na gpu model and memory: n
if including tracebacks, please include the full traceback.
an exception is raised when trying to stack multiple `tf.keras.layers.lstm`, while the sequence length changes across batches.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
try to change score of zset menber: 1. open a zset 2. select a member of zset 3. change score 4. commit 5. got error try to open a zset twice at the same time: 1. open a 2. open it again 3. got two blank windows
otherwise this can become a needle in a haystack and gets annoying real quick
the random seed set via is not set in the context in which the functions passed to `tf.data.dataset#map` are invoked.
it was working fine in previous version.
large logs and files should be attached.
allow the user to use the `normal` highlighting group for popup windows.
the caller indicates failure, may mean could performance gains more were available.
i specified rate=[1,1] and the input kernel is constant (tf.constant).
the values are computed inconsistently, and a bit incorrectly, in a single tensor.
no warmup data file found at successfully loaded servable {name: 1} running grpc modelserver at ... [warn] getaddrinfo: address family for nodename not supported exporting http/rest api at:localhost:8414 ... [evhttp_server.cc : 237] raw: entering the event loop ... building a new tensorrt engine for map/while/trtengineop_1 batch size 224 w defaultlogger tensor datatype is determined at build time tensors not marked as input or output.
the password does work on macos mojave when double clicking the mobileconfig file to install the profile on a mac.
derivatives of non-holomorphic functions are incorrect when compared both against ad and finite differences.
upon using tf.keras batch normalization at test time, a given input sample x should have the same output regardless of the other samples in the batch
1. setup directory with `foo.c` and `.gitignore` files: 1.1 `touch foo.c` 1.2 `echo '.
the cs runtime files, vim-cs phofhyperlink , use indentexpr but it relies on cindent().
see the build logs in phofurl
maybe it is because of the recursive structure of the operation.
self.opt def loss(self, x, y, return_func=false): def loss_(): return * self.w + self.b - y)) if not return_func: return loss_() return loss_ @tf.function def fit(self, x, y, steps=1): for _ in range(steps): y, return_func=true), [self.w, self.b]) tf_model linearregressiontf() tf_model.fit(xs_train[:, 0:1], y_train.reshape(-1, 1)); [cal_house.json.gz phofhyperlink
the script is attached code section.
on arch linux run (assuming git is installed): phofcode
5. watch it become unresponsive.
if you look at `conv2d_transposed` layer 'up1 ' number of outputs is set to 16 but kernel shape indicating that input dimensions are 16 even though they are 32 from `conv2d` layer 'conv3 ' preceding it.
the model is quite complex.
there should be no error.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
tensorflow log: phofcode nvidia-smi log: phofcode
2. in the "roles & permissions" page.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): '1.12.0 ')` python version: 2.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: phofcode phofcode
tensor should be reshaped internally and crash should 't occur.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):yes - os platform and distribution (e.g., linux ubuntu 16.04):linux ubuntu 16.04 - mobile device (e.g.
test.vim phofcode 1. so %, text properties is displayed.
when i convert the keras model to the tensorflow model, but i get an error above.
try it directly in google colab.
given a tensor x with shape (1, 1, 256), matmul gives slightly different output for matmul(x, w) and matmul(tf.tile(x, [1, 2, 1]), w)
i created a session option with inter=1 and intra=1.
returns only the operations from name scope, and preferable shorted by construction just like in the original list.
we expect the model to run and build predictions.
the existing quick fix menu is made current.
when trying to preview a tag in a popup window for which a swap file already exists, vim opens an empty preview popup window.
training a model using dataset scaled to range 0..1 i am getting improbable metrics consistently phofcode i don't understand these metrics.
distribute strategy cannot properly handle dict/tuple type inputs in while there is no problem in non-distribute scenario.
tpu devices can be found on colab when runtime is changed to `tpu` and using: - - `tpu_strategy =
is this feature the making?
following the official guides ( [1] phofhyperlink , [2] phofhyperlink ), i was training psenet phofhyperlink for text detection.
if including tracebacks, please include full traceback.
both trainable and untrainable variables are updated by the optimizer
normally it should just cut the 'whatever' - without the end '#'
the error is `valueerror: column dtype and sparsetensors dtype must be compatible.
use mnist.py phofhyperlink and replace keras with tf.keras
`popup_create()` should also accept buffer-local text property types.
no error model phofcode error phofcode
i expect there to be no error message!
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 lts - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
7. ran "./algo" 8. i get following error message: "[warning]: could not match supplied host pattern, ignoring: vpn-host" "error!
i would have expected logging to place a minimal overhead cost on training performance.
phofcode output when is set to 20 (so `iterations` starts at `0`): phofcode output when is set to 1 (so `iterations` starts at `1`): phofcode
provide a reproducible test case that is the bare minimum necessary to generate the problem.
inside that function, it will call `converted_call` which is defined in `autograph.impl.api.py` and convert py_function to graph_funcion via autograph lib however, inside `converted_call`, it should shows some log information about conversion current version, but my simplest example, it does not actual execution.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
according to following line: / num_true` (one of last lines function), first 1000 elements in each row `out_labels` are: 1./1000 = 0.001, while last 20 are 0. the potential problem is that output is then fed to a sigmoid cross entropy (
however doing so makes vim redraw the screen and clear a multi-line output of an ex command when focusing a different program window (like the one of web browser for example).
<nomodeline> % ' endif endfu nno <silent> <s-f18> :<c-u>call nno <silent> <s-f19> :<c-u>call and if want the events to be restored in all modes, not just normal mode, need 2 additional mappings for each missing mode.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): tf docker images - mobile device (e.g.
n/a - vim version phofcode - os: ubuntu 19.04 - terminal: xterm(330) add any other context about the problem here.
in other words, when multiple contexts talking to the same cluster (which is defined by `clusterspec`), should they know how to coordinate with each other, just as tensorflow core is architectured and designed before?
attached is a test case that reproduces the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): using estimator and train_and_evaluate - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
it can be executed as such: phofcode
`call setbufline(nr, 1, 'sss')` 4.
but only difference between both paths being that first is an absolute path to sourced script while second is relative to users home directory.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): y - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.13.6 - mobile device (e.g.
... steps to reproduce the behavior: 1. go to phofurl 2. expand /pet/findbystatus 3. make sure response content type is application/xml 4. example value for status code 200 has "xml example cannot be generated"
(issues related to the runtime files should be reported to their maintainer, check the file header.)
configuring tf to build with cuda support and choosing all the default option for all the other questions in ./configure including the default for nccl "please specify the locally installed nccl version you want to use.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
a file named `new_unexecute_ops.txt` will be created and each line contains name of one of these operations.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.13.0 python version: 3.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory:
the `testconcataxistype` test in `concat_op_test` is failing on an assertion for s390x: phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution windows 10 x64 - mobile device (e.g.
calling keras layers on some input results in different variable names compared to calling layer.build: phofcode in the first case, variable names are: phofcode while in the second case, they are: phofcode i expect the variables in the first layer to be prefixed with `dense_1/`.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): r1.13.1 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with: 1. tf 1.0: `python -c "import tensorflow as tf; print(tf.git_version, tf.version)"` 2. tf 2.0: `python -c "import tensorflow as tf; tf.version.version)"` phofcode
large logs and files should be attached
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): window7 - mobile device (e.g.
session; options; tensorflow::configproto & config = options.config; int corestouse = 1; if (corestouse > 0) { } status = session); tensorflow::metagraphdef graph_def; metamodel_path, &graph_def); run_model();
vi improved 8.1 (2018 may 18, compiled sep 22 2019 included patches: 1-2065 - ubuntu 18.04 should be some runtime plugin caused this - i guessed
indeed, models use operations like `space_to_batch`, but as far as i know these operations should only affect speed performance.
if including tracebacks, please include the full traceback.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): source tensorflow version: 1.12.0-rc0 python version: 3.6.7rc1 bazel version (if compiling from source): 0.19.0 gcc/compiler version (if compiling from source): 7.3.0 cuda/cudnn version: / 7.3.1.20 gpu model and memory: nvidia m1000m / ~4gb
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): window 10 - mobile device (e.g.
running the commands as per the keras installation instructions in a virtual environment.
i am testing the benchmark tool found on this link: phofurl i am building and running in android to benchmark mobilenet v1 model on some devices.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): true - os platform and distribution (e.g., linux ubuntu 16.04): centos linux release (core) - tensorflow installed from (source or binary): pip install - tensorflow version (use command below): 2.0.0-beta0 python version: 3.6.7 one guess is that protobuf or python->cpp marshalling is broken.. python, tensorflow, protobuf version are the same across machines.
to this end, i rotate the image by some angle a and the points by same angle.
additionally, the test phofhyperlink for this seems, well, broken: phofcode i cannot see how the given `:cdo` command is expected to fail performing the substitute; further, the test actually wants it to succeed, and only "fails" if substitute is not done!
bugreport.zip phofhyperlink model: lite_model_v2.zip java: liteevaluator.java dependency: equal behavior exists for both implementation and implementation
i expect tf.keras.layers.add can process tensors of any dimension.
i generate 640 sequences of 5 random integers between 0 and 11. and i want to train an lstm to predict 5th element of a sequence given 4 previous ones, with a batch size 64. hence code looks like this: phofcode
... steps to reproduce the behavior: 1. go to phofurl 2. click on 'execute. '
this likely means that function-local variables were created and not referenced elsewhere in the program.
"bar" and some text to the left of it get selected
steps to reproduce the behavior: 1. add more users in config.cfg and enable auto reboot 2. disable ipsec in config.cfg 3. run ./algo 4. choose digitalocean as the cloud provider 5. enable ad blocking and disable ssh tunneling 6. choose sfo2 as the region 7. import the generated configuration file to iphone xr 8. turn on on-demand for both wi-fi and cellular 9. switch from extender network with same ssid and channel as main router
log the metrics and loss as described in the official docs [ phofurl phofcode
save a png image of the model.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
it seems tf.layers.flatten is inaccessible using the name, when name is provided as part of tf.layers.flatten 's argument.
expect that loss and val should be almost equal to value in fit() progess?
with beta 0 and 1 the training/validation history is as follows: beta 0/1 phofimage this is a very low validation accuracy.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): rhel7 - mobile device (e.g.
this is print out: phofcode when call in tf 1.15, behavior is same as tf 2.0.
i also tried 8 workers and different batch size, the failure always reproduce.
1. download and run the login-react phofhyperlink example 2. download strapi.
@registry.py:133] group0/block0/conv2 output: group0/block0/conv3 @batch_norm.py:164] wrn [batchnorm] using training.
unfortunatley there is no example yet in tf 2.0 documentation.
tf_config={ "task": { "type": "ps", "index": 0 }, "cluster": { "chief": ["machine2:2222"], "worker": "evaluator": ["machine5:2225"], "ps": ["machine1:2218"] } }
`tf.io.write_file` creates file in eager execution but produces no output file when decorated with `@tf.function`.
`-h "x-irest-conn: "` this is seen in the curl command.
but, since it 's a warning, i still get the processed files okay.
i am unsure in colab how to select a windows based environment.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
(updated) 1. create a strapi project according to the quickstart 2.
my custom layer: class def __init__(self, out_size): super().__init__() self.rnn_layer = return_sequences=true, return_state=true, def call(self, x,
the code takes 50 seconds execute on gpu(i monitored gpu usage so know it is using gpu indeed).
i then upload that to my bucket in the google cloud.
check link in example phofurl
large logs and files should be attached.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):yes - os platform and distribution (e.g., linux ubuntu 16.04):linux manjaro illyria 18.0.2 - mobile device (e.g.
... steps to reproduce the behavior: 1. go to phofurl .
function a() let a = [ [ 's ', 'r '], [ 'a ', 'bar '], [] ] endfunction observe that the ` 's` is highlighted as a "vimsubst", the following single quote is a "vimsubstdelim", and then "vimsubstpat" matches until the fourth single quote, then there is an error flagged at first closing bracket.
when using class_weights in fit_generator causes the training process to continuously consume more and more cpu ram until depletion.
i have variable sized axis on the input for a dilated conv layer which fails when inputting the second batch(data is padded batch wise).
the result looks like this: unbenannt phofimage
when the current working directory is in wsl, and potentially when its on a network drive of other types, `expand("%:p")` and the likes are slow.
according to the doc, the configuration should be simple: set correct tf_config and pass the distributestrategy.
flatbuffers already provides a utility function for that: unfortunately this requires a loop over all data in strides of the size of tensor element.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
large logs and files should be attached.
i include a flag in my status line to know whether the capslock state is on or off.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
[y/n]: n no opencl sycl support will be enabled for tensorflow.
running on a object raises `typeerror: 'inputlayer ' object is not iterable`.
happy for them to be consolidated if they are too similar.
may not be popup related but i found it while testing popups.
in bash: phofcode then in python: phofcode
profiling works for both machines.
i 've used ssd_mobilenet_v2_coco phofhyperlink to train my dataset for object detection.
if applicable, add screenshots to help explain your problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.6.5 bazel version (if compiling from source): no gcc/compiler version (if compiling from source): no cuda/cudnn version: gpu model and memory:
please revert to the behavior available in
note: a quote character is seen as argument to the
--- the issue disappears if syntax highlighting disabled: vim -nu none + 'setl cul ' + 'set ft=vim|syn off|/readline#yank ' + 'setl <(curl -s phofurl here are two reports obtained with `:syntime`, after maintaining `c-n` pressed for 5 seconds: - report while 'cursorline ' set phofhyperlink - report while 'cursorline ' not set phofhyperlink for some reason, when ` 'cursorline '` set, regexes are used and match several dozens of times more often.
1. run algo with aws variables: phofcode 2. pick amazon ec2 as the cloud provider 3. watch it fail at `task [wireguard : generate public keys]`
when trying to convert the very same frozen graph file with the uint8 option, i get an error: runtimeerror: quantization not yet supported for op: fake_quant
(issues related to the runtime files should be reported to their maintainer, check the file header.)
i 'm using tensorflow speech_command example 's tiny_conv model from here phofhyperlink .
the code for running inference on the phone for the tflite models is the following: import import import android.graphics.bitmap; import android.os.trace; java.io.fileinputstream; java.io.ioexception; java.nio.bytebuffer; java.nio.byteorder; one.realnote.app.util; public class { private static final string tag = "tfliteapialex"; // only return this many results.
normal inference time: about 50ms (pixel 2) abnormal inference time: about 3ms (pixel 2) the same problem occurs in other phones.
that the function is not going to be autographed.
somehow, scope seems be lost through experimental_run call.
i then used the qr code to add the config to my iphone.
@dataset.py:45] instances loaded 100%|| @timer.py:48] load groundtruth boxes minival2014 finished, time:0.0315sec.
i want to use the tflite file in android, however, i even can 't use it in python
if including tracebacks, please include the full traceback.
in this case the object type is
file isn 't saved because of the conversion error
jupyter notebook should authenticate its own token.
it should be serialized without any error.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macos mojave 10.14.2 - tensorflow installed from (source or binary): binary (pip) - tensorflow version (use command below): '1.12.0') python version: 2.7.10
reincarnation of bug #25327 for different layer type (lstm rather than embedding).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
please use this code for your testing: phofurl it contains code that you can use for testing - the conversion works for tf 1.11.0 and tf.12.0 but not for tf 2.0
it should have saved three .ckpt files namely: -checkpoint -checkpoint.ckpt-00.data -checkpoint.ckpt-00.meta
here it is only partially using new keras functions: phofurl phofcode
[y/n]: n no xla jit support will be enabled for tensorflow.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): win7 same issue on ubuntu - mobile device (e.g.
image phofimage vim - vi improved 8.1 (2018 may 18, compiled oct 28 2019 ms-windows 64-bit gui version with ole support included patches: 1-2231 compiled by appveyor@appvyr-win huge version with gui.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.11 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: k80
it can be seen from phofurl that the `cudaroot()` function is used by tensorflow to: 1. get the path to libdevice library 2. get the path to ptxas binary however, the return value of `cudaroot()` function is a constant that's determined during compilation.
however, it emits type error as typeerror traceback (most recent call last) in _apply_op_helper(self, op_type_name, name,
i am using python 3.6.6 with ipython 7.8.0. if you do: phofcode and press `tab` after the `.`, you get dozens of warnings like: phofcode you can actually reproduce this with the docker images, although that produces fewer deprecation warnings than the released package does: phofcode (unfortunately, i wasn't able to find a more up-to-date official docker image)
... steps to reproduce the behavior: 1. open swagger ui using provided yaml file in eclipse 's internal browser.
the output of decode_wav is tuple of (wav_data, sample_rate) sample_rate is int32 but expects a float32 sample_rate.
... steps to reproduce the behavior: 1. import the doc into the online swagger editor 2. click on the path of first item - 3. observe that top 2 items have both expanded together
the loss calculation is not correct when working with `tf.keras.`.
the error doesn 't occur when mouse selection was made inside popup window before it is closed.
i use to make my code be able to run on tpu,but it took 170 hours to finish an epoch while cpu took the same time and gpu took only 40 hours per epoch.i tried to adjust batch size but nothing changed.and i 've tested the input function may take up 20% of the run time when running on gpu, so i think it 's maybe not the main reason.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): both source and binary tensorflow version (use command below): 1.11.0 python version: 3.5.2 bazel version (if compiling from source): 0.15.0 gcc/compiler version (if compiling from source): gcc 5.4.0 cuda/cudnn version: n/a gpu model and memory: n/a i 've attached valgrind results of the example running on tf version 1.11.0 with and without mkl.
1 means the bug where `all_gather` fails to collect tensors across tasks.
i would expect roughly the same training accuracy after 1000 steps for each test.
using tfbdg command 'run -f has_inf_or_nan ', we get a cascade of errors.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution: linux fedora 30 - tensorflow installed from: binary - tensorflow version: 1.13.1 python version: 3.7.3 cuda version 10.0 gpu model nvidia titan v black the below bash script uses the above code to display the issue.
(or paste the result of `vim --version`.)
i expect to be able to use the tensorboard callback without issues.
it was suggested to create an issue here, originally reported on phofurl and phofurl i had vim version installed where `gpush` command worked without any problems.
the issue is traced to this phofhyperlink line, where the function is used to generate a cache key.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes.
i suspect the compiled graph uses cpu for the inference.
however, when i try to go to admin page i see a blank page.
phofurl doesn 't implement variables() method (because it doesn 't call base class `__init__` nor override it), so it fails when used within keras model training with the following stack trace on model.fit_generator call: phofcode
large logs and files should be attached.
import keras,the same code run perfectly.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04):linux ubuntu 18.04 - mobile device (e.g.
however, it seems that when converting the raggedtensor back to tensor, the shape is not evaluated correctly.
i find this when doing a coursera online course.
ideally, a dynamic_rnn should support tf.in32 types.
even a script as simple as below have thrown 3 deprecated warnings phofcode phofcode phofcode phofcode
steps to reproduce the behavior: 1. enable 2. deploy algo on ec2 with default ubuntu image (ubuntu-disco-19.04) 3. change image to "ubuntu-bionic-18.04" 4. deploy algo on ec2 again
8 workers and 1 parameter server.
this value is != 1.0 for all outputs without gpu delegate, but == 1.0 with gpu delegate.
command executed: phofcode you can find the `do.py` script and the `utilities.py` script referenced from it attached, and the `tiny-yolov3_frozen.pb` frozen model here phofhyperlink .
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): conda and pip tensorflow version (use command below): 1.8 python version: 2.7, 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: n/a gpu model and memory: n/a
assume `batch_size = 2`, `num_true = 1000`, `num_sampled = 20`, then `out_labels` will be a tensor of size `2 x 1020` (1020 is the addition of `num_true` and `num_sampled`).
this error message should be formatted and it should highlight exactly which input is incompatible.
the sescond value can be a tensorrt context pool.
the evaluate method must provide results close to the fit method, especially after 100 epochs
provide a reproducible test case that is the bare minimum necessary to generate the problem.
thanks, david class mymodel(model): def __init__(self, n_layers, h_dim, b_dim, activation_fn, kernel_init, batch_norm): super().__init__() ... skipping ... @tf.function def call(self, inputs, training=false): y = inputs[0] h = inputs[1] n_var = inputs[2] x = layers.concatenate([y, h]) if self.n_layers >= 1: x if self.n_layers >= 2: x if self.n_layers >= 3: x if self.n_layers >= 4: 5: # scale output by 1/n_var #return self.d_out(x) / n_var return
`:set lines=25` `:set columns=69` scroll to bottom, move cursor to last character.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): macos mojave 10.14.4 - mobile device (e.g.
- os & version: [e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): - mobile device= pixel 2, samsung galaxy 6, 7, 8 - tensorflow lite version : experimental-0.0.1 gpu : adreno 540
- vim - vi improved 8.1 (2018 may 18, compiled jul 4 2019 macos version included patches: 1-1635 - os x 10.13.4 - terminal: iterm2 i 've started to use the @ char as the namespacer for zsh functions.
is making building networks much more pleasant in general by forgoing the need to wrap everything in `lambda` layers.
the program can load model file successfully.
gif phofimage in the past, `quickfixline` highlighted the whole current quickfix line.
i was unable to get the commit log message for that specific change, so i 'm unable to tell if this is intentional (and i 'd like to see the rationale of it) or a bug.
previously when running the code from the cvae tutorial phofurl i used in the sample function `if eps == none:` which worked fine, however with rc0 it now throws an error from comparing a tensor to none and only works if the following is used `if eps is none`
in particular, the fixes introduced in that were mentioned, do not resolve the issue for this test case.
i 'm trying to feed tensors produces by a parseexample op when inference an exported graph to avoid tf record of input examples.
sess = tf.session() print(sess.run(hello)) hello_gpu = tf.constant('hello, tensorflow-gpu!')
tpu distribution strategy does not support model.fit_generator, and repeated model.fit calls result in a 50x slowdown presumably because it adds operations to graph.
no error messages thrown since the job output is correct.
n/a phofcode - vim version [e.g.
as result, there are only 2 items (`xxx` and `xx`) for 3 targets (`a`, `b`, `c`).
i would like to use keras layers within the custom layer, and have them trainable.
(or paste the result of `vim --version`.)
in an html file, when i press a key to insert a character or move the cursor, the keypress is sometimes consumed by the html indent plugin.
spec selextor textbox updates with changes made by interceptor.
when i use tf version 1.13.1 to convert pb to tflite, it shows dim not matched error, but when i use 1.14 to convert, it succeeds to save the tflite file.
memory saturated at some point, but broke from 1.13, and we see same effect with 1.14.
detailed steps to reproduce the behavior: 1. have wsl set up.
if including tracebacks, please include the full traceback.
tfrecords_path is the path to mnist dataset as tfrecords files with the features described bellow: phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): tf.version.version: tf.version.git_version: python version: 3.6.8 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: n/a here is the stacktrace: attributeerror traceback (most recent call last) in <module> 27 8])) # <= works if i add this line 28 optimizer="nadam") ---> 29 history = model.fit(x_train, y_train, epochs=2) # <= error!
- have i written custom code (as opposed to using a stock example script provided in tensorflow):yes - os platform and distribution (e.g., linux ubuntu 16.04): macos - mobile device (e.g.
it 's supposed to train the model and not throw the error.
using tf lite with gpu developer preview, when i run inference on my mobilenetv2 retrained model (with two outputs) on most devices it works nice.
the mnist.py script is supposed to to achieve >99% test accuracy after 12 epochs of training.
dense_net should have shape information: `<tf.tensor 'lambda_6/identity:0' shape=(none, 10, 5) dtype=float32>`
features included (+) or not (-): add any other context about the problem here.
* output from data input_fn: ================ input file(s): batch size: 1024 epoch count: 1000 mode: train thread count: 32 shuffle: true ================ ( 'target_dtype ', <tf.tensor 'decodecsv:9 ' shape=(?,) dtype=int32>) info:tensorflow:create checkpointsaverhook.
in effect, the following code raises an error: phofcode phofcode whereas this code works: phofcode phofcode
the code takes the decision in model.train_on_batch() function phofhyperlink in this phofhyperlink line.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 python version: 3.6 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: 10.0 gpu model and memory: rtx 2070 (8gb) seems to be related to #21889 (the error message is different)
it should also probably be mentioned in the documentation of tf or keras: phofurl phofurl it's sort of suggested but not well enough imo, especially with the hard to find fail it produces
i installed tensorflow 1.10, and it worked fine.
2. navigate to a directory.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
in case of standard types, this would instantiate new one as well, if i am not mistaken.
//mean, std_dev [from this tensorflow tutorial phofhyperlink tflite_model converter.convert() open("tiny_conv.tflite", //the resulting .tflite file.`
detailed steps to reproduce the behavior: 1. put this in (create the file) phofcode 2. to debug the highlight groups add this in your `.vimrc`: phofcode you can replace `<f9>` by any other key you want.
given two files, save_rng.py and restore_rng.py as below: save_rng.py phofcode restore_rng.py phofcode save_rng.py is executed first, and then restore_rng.py.
if i focus left window, then it gets highlighted.
i could not find a smaller way to replicate this.
when i turn on persistence mode, gpu utilization goes to 60% and stays there.
edit 1: added windows caveat
i expect the data api to allow me exploring data in eager execution mode even if i use the csvdataset class.
to install pve test repository add `deb phofurl stretch pvetest` to _sources.list_ , download the gpg key `wget phofurl -o and update thet apt to get the pve linux headers.
while trying to train a neural network with my gtx960 after installing tensorflow-gpu, and choosing my gpu with the below code, i can see on the windows task manager that it 's only using about 10% of the gpu, and thus making it way slower than training it with the cpu.
when using the api for training models on the tpu, providing a `validation_split` parameter does not remove the validation samples from the training set.
this works well apart from the fact that there appears to be a changing of buffers that happens that causes a flicker on screen... and for the "current" window (excuse almost certainly loose terminology) to not be restored.
i got the tflite model which is of size 85 mb and built an apk file which comes with size 113 mb and i quantized the model with following command phofcode it gives me a tflite model of size 21 mb and when i use this quantized model to build an apk it generates an apk file of size 135 mb
tower0/cond/while_2/prod tower0/cond/while_2/prod tower0/cond/while_2/prod tower0/cond/while_2/prod tower0/cond/prod_1 tower0/cond/prod_1 tower0/cond/prod_1 tower0/cond/prod_1 tower0/cond/prod tower0/cond/prod tower0/cond/prod tower0/cond/prod tower0/cond/prod_2 tower0/cond/prod_2 tower0/cond/prod_2 tower0/cond/prod_2 tower0/cond/while/prod tower0/cond/while/prod tower0/cond/while/prod tower0/cond/while/prod @training.py:347] includes 0 operations.
inside the inference since the input audio is of dynamic length, i try to call `resizeinputtensor` in the c++ api on android to match the length of audio, which throws num_input_elements != num_output_elements` similar to #23600. in inference code there is a line of `reshape` which is from [-1, 1, 38, 25] [-1, 1, 38*25], (note that it collapses fourth dimension but doesn 't do anything with batch dimension.
using a tf.session the uint16 input is always transformed to uint8
phofcode run with the path to the attached file as a command line argument.
(this is just from the error onwards) task [strongswan : register p12 payloadcontent]
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
for x in assert (shape[1]==x) ``
... steps to reproduce the behavior: 1. go to main page 2. click on any service 3. click on 'try it out ' and complete a query parameter or path variable 4. click on execute 5. see query parameter or path variable is not in generated curl and not used to call service and input field is empty.
i think i do feed a tf.variable to scatter_update, but it still return the error message.
session_config = tf.configproto() = true run_conf = runconfig( model_dir=out_dir, save_summary_steps=400, keep_checkpoint_max=1, tf_random_seed=none, train_distribute=none, device_fn=none, protocol=none, eval_distribute=none, ) estimator = tf.estimator.estimator( config=run_config, params=params )
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below): python version: bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
5468 = context.eager_mode 5469 # pylint: disable=protected-access valueerror: must be called at program startup
the same code was runing with tf 1.12 and i could train the model using estimator
peak memory = peak memory = (+49,152) peak memory = (+45,056) (+49,152) (+73,728) (+40,960) (+49,152) (+45,056) (+36,864) (+36,864) (+53,248) (+45,056) (+45,056) (+49,152) (+40,960) (+61,440) (+40,960) ... (+53,248) (+53,248) (+40,960) (+53,248) ... ``
when `formatoptions` is set in a vimrc file, it will still be edited by ftplugin, creating undesired behavior.
if applicable, add screenshots to help explain your problem.
it also seems to be what linux version of vim does as problem does not reproduce if you run vim from wsl.
how to use custom value view formatters
at method) at at at source:8)
you have bazel 0.17.2 installed.
in particular, it complains about `valueerror: tf.function-decorated function tried to create variables on non-first call.`, even though the function is always called with different parameters.
run tensorflow well in both files.
the model accuracy is far off between virtual gpus and physical gpus.
the error appeared in 3 out of 5 runs.
follow this link and run the scripts as mentioned: phofurl
it seems that saveableobject is not recreated correctly.
`dense` should flatten its input like the documentation says.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 2.0-alpha python version: 3.6.8 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
if we provide extra columns to model in model.predict() model behaves reliably by processing inputs using however if model.call() is used this doesn 't happen and orders columns based on alphabetical order as per nest.flatten(), thus crashes with type cast exception if extra columns are of a different format.
it is dense with sigmoid activation in the last layer and binary cross entropy loss.
wrapping the optimizer in a lossscaleoptimizer should work transparently with keras models, as it implements tf.optimizers.optimizer base class.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): jupyter notebook on phofurl - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): arch linux - tensorflow installed from (source or binary): pypi binary - tensorflow version (use command below): python version: 3.7 the issue is present both when manually iterating over it in a custom training scenario and when using `tf.keras.model.fit()`.
multiple call to same sess.run(op) return different results.
large logs and files should be attached.
please see code provided to reproduce this issue.
the issue is due to `synid()`, whose output is sometimes id of `helpexample`, while it should be id of `helpbar`.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): conda tensorflow version (use command below): 1.11.0 python version: python 3.6.6 :: anaconda, inc. bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: include any logs or source code that would be helpful to diagnose the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
image below, blue dots are on dark squares).
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): pip tensorflow version (use command below): python version: 3.6.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10 gpu model and memory: geforce gtx 1070 8gb
logger throws warning, that the use of 'tf.keras.layers.lstm' is not optimized for performance on gpu.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
inference accuracy for resnet50 and densenet has dropped to zero on cpu.
*) ')|set cole=3 cocu=n nowrap" +"pu=repeat( \'x \', &columns/2).. \')z \'" then press `$` to move the cursor on the last character of the line, which here is `z`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes, see below - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12.0 python version: 3.6.7
large logs and files should be attached.
in my case, the depth multiplier should be 1 and the size of the input channel dimension should be 6. i'm not sure why it says it is 0?
... steps to reproduce the behavior: 1. go to the rest api page 2. click on book tag 4. see error image phofimage
for example, with a training dataset (1825, 401, 401, 3), validation dataset (140, 401, 401, 3), `epochs=1`, `batch_size=16`, `gpu_number=2` phofcode the training apparently goes fine phofcode but the weights have nans, e.g.
i am trying to quantize two different yolo models (one tiny, one normal) with tensorrt.
the images marked with the boxes.
if including tracebacks, please include the full traceback.
for example consider 00001.bmp image(attached attachments) size: height=351 width=572 channels=3 is for second case.
allreduce is not supported for indexedslices.
i think it is same problem with layers.gru the problem does
i have even tried to setup local dns in wireguard app still getting not secure.
- n/a vim - vi improved 8.1 (2018 may 18, compiled may 22 2019 included patches: 1-1362 should one of 'builtin' plugin caused the issue, but not sure which one .. since if i removed all ext-plugins, the issue still there .. and if i started the vim with '-u' : vim -u xxx, then it would work.
1) the complete (relevant) stacktrace is file "/applications/pycharm line 1758, in <module> main() file "/applications/pycharm line 1752, in main globals = none, none, is_module) file "/applications/pycharm line 1147, in run globals, locals) # execute the script file "/applications/pycharm line 18, execfile ", file, 'exec '), glob, loc) 90, <module> model=model, optimizer=adam, loss_obj=bce) 65, fit loss_values, grad_values = grad(model, x, y, loss) 57, grad loss_value = loss(model, inputs, targets) 50, loss y_pred = model(x) 712, __call__ outputs self.call(inputs, *args,
provide a reproducible test case that is the bare minimum necessary to generate the problem.
here is the error i get: `>>> from import input_data` futurewarning: conversion of the second argument of issubdtype from float to np.floating is deprecated.
info:tensorflow:error reported to coordinator: traceback (most recent call last): file line 297, in stop_on_exception yield file line 795, in run self.main_result =
'execute ' 4. inspect response body
run this shell command: vim -nu none + 'set wmh=0|sp|call "resize 0") \' the bottom window is `1` line high.
however, i'm not certain and any insights you guys have would be useful!
2. press `gx` on url to open it.
i have trained an autoencoder and want to convert it to a tflite model.
* `gettabwinvar()` should allow to query `&cmdheight` value for an existing window in another tab page, instead of returning (wrong) value for current tab page.
because only the chief does the checkpoint, i think the chief must wait others finishing, otherwise the model is partially trained and evaluator will not stop.
steps to reproduce the behavior: 1. do this.. 2. do that.. 3.
fails to run because of an import error whenever i try to import tf-gpu,
i am using tensorflow 1.12.0 with gpu rtx2060 with 6gb memory.
.tflite model converted with post-training quantization fails on nnapi with following error message: phofcode we share the same code across all cases.
that one returns a value, but it's always the one for _current_ tab page, so not good either.
the model can still be called, weights are not zero.
1) create a file type field with a multiple mark 2) add multiple files to it 3) save item image phofimage if the content editor wants to delete the file 1 out of 10 he will have to look for it for a long time by touch.
tflite conversion from h5 file increases in size from 141kb to 508kb
if including tracebacks, please include the full traceback.
we use a different session and graph for every thread so every thread is actually isolated from the other threads.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): docker tensorflow version (use command below): 2.0.0-alpha python version: 3.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10. gpu model and memory: v100 you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
this could be because one of the network devices (e.g.
writing ~90 layers of batchnormalization should take smaller amount of ram.
the job should keep training without failure
if including tracebacks, please include the full traceback.
our data generator looks as follows: phofcode but it is quite problem specific and we use it with a large data set o(100k) samples.
i have `trainer` class which has the following methods: `pass_one_step_gan`, `__pass_critic` and `__pass_generator`.
any attempt to run one of the tf lite utilities (e.g.
gcc4:` _m_invoke(const _any_data& __functor, _argtypes... __args)` gcc5: `_m_invoke(const _any_data& __functor, _argtypes&&... __args)` while this change is abi-compatible, it produced segfault in situation where gcc4-compiled code is calling function defined in gcc5-compiled plugin.
is there a specific reason why `tf.data.dataset::cache` files use a lot more disk space and don 't split the files into chunks?
batchnormalization seems to silently produce nan weights when training a `multi_gpu_model` if the training dataset size is not a multiple of `batch_size`.
if including tracebacks, please include the full traceback.
_this throws a compilation error but should output the `item.label`._ phofcode _this compiles but renders poorly._ phofcode _this renders correctly but you need to maintain some sort of label -> item lookup if that 's even possible._ phofcode
most other similar issues are due to a mismatch between a binary (pre-compiled) tensorflow and the cuda installed by user.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no - os platform and distribution (e.g., linux ubuntu 16.04):ubuntu 16.04, - mobile device (e.g.
the following code raises an assertion error: phofcode
tensorflow 2.x should work with a custom nested layer as tensorflow 1.x can.
if including tracebacks, please include the full traceback.
is there runoption or sessionoption that i 'm unaware of controls how long until autotune (or what i 'm presuming autotune) "forgets"?
however, experiment 1 fails, but 2 and 3 succeed.
large logs and files should be attached.
here 's a minimal example: phofcode
running the tf_upgrade_v2 script with a file containing the @ operator results in an exception (see below).
the visual presentation of text and images of text has a contrast ratio of at least 4.5:1, except for the following: - large-scale text and images of text have a contrast ratio at least 3:1 - text or images that are part an inactive user interface component, that are pure decoration, that are not visible to anyone, or that are part a picture contains significant other visual content, have no contrast requirement - text is part a logo or brand name has no minimum contrast requirement
if exceptions produced here are expected behavior due to errors in developers code a more meaningful error message would be appriciated.
here is the model 's signature def info: phofcode here is the cmd line and error: phofcode the problem occurs at saved_model_cli.py line 487 ( phofurl phofcode it checks if list `feature_list` contains string values, and then try to append list to feature 's `bytes_list` which expects element to be bytes, not string.
`import tensorflow as tf import pdb import cv2 import base64 with tf.session() as sess: img_path = with open(img_path, 'r') as the_file: image_string_base64 = the_file.read() image_string = image = image 108, 56) print(image) img_array sess.run(image) print(img_array.shape) cv2.imwrite('test.png', cv2.cvtcolor(img_array, cv2.color_bgr2rgb))`
[also, if you 've already started vim without a ttymouse value and set `:set mouse=a` while it 's running, it 'll just hang there, unresponsive to mouse or keyboard] not only will it not start, but ctrl-c won 't terminate it.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): public colab runtime - tensorflow version (use command below): 1.12 - python version: 3
work in progress notebook can be found here: phofurl i am using a very basic keras model with phofcode
the first line of the fold was originally: fu!
if including tracebacks, please include the full traceback.
t [[node (defined at ]] caused by op defined at: 23, <module> called_model = 554, __call__ outputs = self.call(inputs, *args,
the example uses however, this example does not work when using `tf.keras`, since there is no package.
4. see following error: phofcode
i add the min/max nodes to the graph by calling then i load the trained values (both the weights and min/max values).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no.
i trained the model on the server and deployed the same version of tensorflow on tx2, but when i run the trained model with gpu on tx2, i get a lot worse than on server, but running model cpu of tx2 does not cause this problem.
large logs and files should be attached.
rendered to continue to work and not crash allowing validation to reassert itself.
see above for screen recording links - vim version phofcode - os: ubuntu 18.04.2 lts - terminal: xterm(330) if i can help provide any additional information here please let me know.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
(specifically this is to do hyperparameter optimization on cloudml, but i observe this bug on my local machine as well).
merge, concat) the generated class was incorrect as it took a single input instead of an inputlist.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): raspbian stretch 9 - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): el7 - tensorflow installed from (source or binary):source - tensorflow version (use command below):1.12 python version:3.7.2 bazel version (if compiling from source):0.19.2 gcc/compiler version (if compiling from source):gcc6.3
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): v1.12.0-rc0 python version: 2.7.15 bazel version (if compiling from source): 0.20.0 gcc/compiler version (if compiling from source): 10.0.0 cuda/cudnn version: n/a gpu model and memory: n/a
i want to get values from output tensor with input an image to predict the eye region landmarks.
reproduced this issue on an example that uses mock data (i.e.
there should not be an performance decrease in this drastic
... steps to reproduce the behavior: in swagger-ui 1. navigate to the specific path 2. click on 'try it out ' 3.
maybe stream handler is colliding each other.
if the first dimension of `x` is `n`, "row" `i` will get one value if `0 <= i < (n&-4)`, but a possibly different value for `(n&-4) <= i < n`.
see above (should be reproducible with any frontend that uses xrt to talk to cloud tpus though)
1. model 1:- 2 phofimage 2. model 2:- 3 phofimage models: models.zip phofhyperlink both run in cpu mode; but fails in gpu !!
1. run this shell command: vim -nu none + 'setl cul ' + 'set ft=vim|syn enable|/readline#yank ' + 'setl <(curl -s phofurl 2. press `c-n` to complete the text `th`, and maintain the key pressed for a few seconds.
i get an autograph error when running the following code (see the full stacktrace below): phofcode the error is `valueerror: failed to parse source code of <function <lambda> at
when i using `:set dir =$temp//`
first run this script to define and save the sequential model: phofcode then run the following script to restore the model and train it on the imdb dataset: phofcode
as a result, graph objects cannot be constructed.
i may get a truncated undo file due to disk full.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no custom code, but tensorflow is called through keras-retinanet - os platform and distribution (e.g., linux ubuntu 16.04): gentoo (base system release 2.6) - tensorflow installed from (source or binary): binary from pypi - tensorflow version: 1.13.1 python version: 3.6.5 cuda/cudnn version: / 7.4.2.24 gpu model and memory: nvidia geforce gtx 1080 ti (11 gb memory) nvidia driver version: 418.43 i am reporting this here and not at the keras-retinanet project because it seems to be a tensorflow internal issue.
app crash when manually adding row in zset.
`get_output_details()` returns unique list of outputs.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): pip tensorflow version (use command below): unknown python version: 3.6.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: cuda 10.0, gpu model and memory: geforce gtx 1070 8gb
- windows 10 education - tensorflow installed from (source or binary): pip install - tensorflow version: - python version: 3.7.3 cuda/cudnn version: cuda 10.0 / cudnn 7.3.1 gpu model and memory: nvidia geforce gtx 1080 / 16gb
it is expected the post-training quantisation method to accept networks with multiple placeholders.
behavior for different initializers should be consistent.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
since the `url` configuration parameter is supposed to be ignored, the page should load with the first url listed by the `urls` configuration parameter.
--- the issue can also be reproduced with: +'setl fdm=marker|sp b.txt|mksession!
- running in docker container - `tf.git_version`: - `tf.version`: 1.13.1 - `tf.keras.__version__`: 2.2.4-tf
if including tracebacks, please include the full traceback.
phofcode exception is: > typeerror: input 'y' of 'sub' op has type float32 that does not match type float64 of argument 'x'.
for example when keras is downloading imagenet weights for a model it outputs this: phofcode
if i try sroll left window, is scroled again right window.
where did it come from?
during the training of a simple "toy-example" classificator using tensorflow 2.0 and tfds, i 'm trying to use the keras callback in order to log the results and to be able to use tensorboard.
either (q)uitting or (a)borting will open an empty popup window with cursor positioned inside window (see first screenshot below).
(init_gpu()) use the following command to run script phofcode results after 10 epoches phofcode
when using dataset with estimator, the memory foot print of ram keeps raising when estimator 's train and evaluate apis are called in loop.
but for some reason, they are not correctly set again if the buffer has been restored after sourcing a session file created by `:mksession`.
tensorflow: 1.12.0-rc0 traceback (most recent call last): file line 10, in <module> model.add(cudnnlstm(64, input_shape=(10, 39), return_sequences=true)) file line 165, in add layer(x) file line 532, in __call__ return super(rnn, self).__call__(inputs,
when creating a custom optimizer, the optimizer keeps throwing `object has no attribute _create_slots` error.
...and then executing that text, via buffer `a`: :%y a|@a now run a shell command :r !echo hello
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
finish the 100 epochs of training and save the model *.h5
even though i am using gpu for running my code, it produces an invalid argument error when i put -1 as an input of tf.gather.
large logs and files should be attached.
the reason of the crash seems different in tensorflow 1.12.0 from one hand, and in tf 1.13.1 and tf 2.0.0 from another hand.
the id should be a valid html id, like on the operation section itself: phofcode
popup window is not considered to be a window?
1. strapi new heroku-dashboard phofcode
i am trying to convert a model.h5 keras model using tflite.
in future, it will be treated as np.float64 == np.dtype(float).type.
4. gvim will produce `e305: no swap file found for _vimrc`
with same command, can get the right result and plot the bounding box.
not configuring workspace android builds.
it should first walk the chain of identity nodes.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): phofurl tensorflow version (use command below):r1.12 python version:3.6 bazel version (if compiling from source):n.a gcc/compiler version (if compiling from source):n.a cuda/cudnn version:n.a gpu model and memory:n.a you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)"
n/a - arch linux - redis-server 4.0.
notice the function fires, but it does nothing.
each time evaluate is called with the same data, it should take roughly the same time.
i discovered that it halts after tf_closesession and even keyboardinterrupt can 't stop the script.
large logs and files should be attached.
i use the below code to generate .pb file, the key is that padding parameter is same, and dilation_rate is 2. phofcode i use the following code to load and check the .pb file, but padding parameter is not same, but valid.
run this python script, then type in:` run -t 10` will get an error message... phofcode
... steps to reproduce the behavior: 1. i pasted the yaml to phofurl 2. see the example value section 3. date example is displayed incorrectly as empty js object
now create an equivalent test file for gcc 4.8 called `test_zero_out-4.8.py` ensure gcc 4.8 is installed run: this will compile run same code with gcc 4.8. should succeed print "success"
while using eager execution keras convlstm2d seems like not initialized.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): pip install .whl - tensorflow version (use command below): 1.12.0 python version: 2.7.14 bazel version (if compiling from source): gcc/compiler version (if compiling from source): c++ (gcc) 4.8.5 (red hat 4.8.5-28) cuda/cudnn version: cuda: 9.0 cudnn: 7.1.3 gpu model and memory: gtx1050ti/4gb tf_env.txt phofhyperlink
i need a interface from the new api like this: # rnn cell model.add(simplernn( # for batch_input_shape, if using tensorflow as the backend, we have to put none for the batch_size.
class def on_epoch_end(self, epoch, logs=none): print( ' learning rate epoch %d %.5f ' %(epoch + 1, function decaying learning rate.
you may want to, for simplicity, use eager execution do this.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution : linux ubuntu 18.04: - tensorflow installed from (source or binary): source - tensorflow version (use command below): 1.11 python version: 3.6 bazel version (if compiling from source): 0.16.1 gcc/compiler version (if compiling from source): 7.3 cuda/cudnn version: 10.0 gpu model and memory: v100, 16gb
3. in the parameters section, click `add item` button 5 times.
if you are getting `unable to load dll 'tensorflow': the specified module could not be found.`, copy your own tensorflow.dll of version 1.40 cpu for windows to output directory: > et461 or netcoreapp2.2
but problem is that `argc` and `argv` as well and this is not normal!
get following error that shows my values: > traceback (most recent call last): > file line 184, in <module> > main() > file line 179, main file line 102, local_predict predictions = model.predict(instances, file line 268, predict preprocessed, stats=stats,
`./algo` 2. pick scaleway as the provider
i commented under issue #26255 but the original poster closed the issue as his problem was solved by updating to tensorflow 2. i am opening a new issue because updating to the pre-release is not an option and i have no way to even trap this error to try handle it, plus it is an unknown error code so no hint as how proceed.
python import numpy as np import tensorflow as tf from tensorflow import keras x_train = np.random.randn(1000, 8) y_train = np.random.rand(1000, 1) class def __init__(self, output_dim,
i have created `.tflite` with single `add` op.
model.summary() prints 'multiple ' as layer output shapes
fp-16 multi-gpu training with cpu as local parameter server is converging in single process mode, but diverging loss value (nan) adding another loopback ps process with grpc.
please report this autograph team.
- vim version: 8.1 - os: windows 10 - terminal: cm
however, if you try to use any `tf.math` ops, you get `typeerror: failed to convert object of type <class to tensor`.
internalerror: cudnn launch failure : input shape
all the code and llogs are here: phofurl
consistent behavior at creation-time of model and after re-loading from a save file.
2. edit `filename` 3. add the following autocmd: phofcode 3. edit another file: `:e <another_file>` 4. switch to the other buffer: `:bprevious` 5. observe the output.
>typeerror: values must be a list.
if that is case, you should define them in a .py source file.
this is not suitable for some use cases.
i 'm assuming that decreasing time is due autotune running.
please use the new featurecolumn apis instead.
note: i put the code into colab and was able to verify the bug.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): custom code - os platform and distribution (e.g., linux ubuntu 16.04): osx mojave - mobile device (e.g.
specifying an optimizer as variable instead of with a string and default parameters should work.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
1. eager mode has no problem yielding examples if just loop over dataset and print them out.
w op_requires failed at conv_ops.cc:470 : resource exhausted: e0206 2719 classifier.go:148] failed classify face: t [[{{node hint: if you want see a list of allocated tensors happens, add runoptions for current allocation info.
sample code: main.cc phofcode compilation script: phofcode $$$ valgrind ./bin/main.out phofcode $$$ valgrind --leak-check=full ./bin/main.out phofcode
provide a reproducible test case that is the bare minimum necessary to generate the problem.
instructions for updating: colocations handled automatically by placer.
it also happens if you split horizontally then vertically.
large logs and files should be attached.
these figures are * best * case, because them caches be nice and toasty.
memory usage on the same order as it 's on disc usage.
this behavior occurs if the `tf.keras.model` is built with model subclassing.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): - mobile device (e.g.
phofcode now when i do prediction using: phofcode i get following error: > invalidargumenterror: in[0] is not a matrix.
tensorflow.keras .fit training time is way faster than tensorflow gradient tape
i see same peak usage in nvidia-smi for batch sizes 16, 32, 64 and then it jumps up suddenly.
2. open up any text or type multiple words of gibberish 3. in between words, use `yw` to see the cursor no move, then `yb` to see the cursor move backwards.
if including tracebacks, please include the full traceback.
w0617 deprecation.py:323] from (from is deprecated and will be removed in a future version.
ui doesn 't send any http request
additionally, there is a `top_kv2` function in `gen_nn_ops.py` that not exported use.
when passing numpy arrays as inputs to `model.fit` this works as expected, but when using a custom instance `steps_per_epoch` does not have any effect.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - tensorflow version (use command below): pip install - python version: 3.6
> observing memory usage, it always use more than 4100mb memory because my gpu only has 5g memory.
attached with this, is the models and error logs of the models.
i can reproduce what i think is the same error using: phofcode
i install tensorflow as i tell and i try : import tensorflow
this behavior can only be observed on the gpu.
`embedding_matrix[2]` should equal to those two equals on tf 1.13.1 version
the empty string assigned to `b` is wrongly recognized (see the screenshot below).
also, this happens on mkl builds as well as non-mkl builds of tf 1.13.1
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
6. click "authorize" in dialog and note the url used in attempting to fetch a token.
phofcode extending this to support full functionality should not be difficult.
please use below link if you like download waterloo exploratory dataset : phofurl
it was developed based on tf 1.3 and performs well.
it hangs after generating this information: i this tensorflow binary is optimized with intel(r) mkl-dnn to use the following cpu instructions in performance critical operations: avx512f fma to enable them in non-mkl-dnn operations, rebuild tensorflow with the appropriate compiler flags.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i use python3.5 predict the same one sample speed: phofcode ` i use java1.8 predict the same one sample speed: phofcode
as far as i can tell this behavior started after patch
i have asked questions on stackoverflow but not got answer what i want phofurl
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): (this is the tf 2.0-preview) python version: 3.6.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: on a related note, is part of the keras api ( here phofhyperlink ), but absent from `tf.keras.utils`.
large logs and files should be attached.
- tensorflow version: 2.0.0-alpha0 - python version: 3.6 - environment: google collaboratory with tpu
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): pip install tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: cuda 10.0, gpu model and memory: geforce gtx 1070 8gb
see the code and logs below.
maybe other options are possible then too.
i noticed this on ubuntu 18.04 when i upgraded from tf2.0-beta to tf2.0.0-rc0.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): mac os - mobile device (e.g.
code runs without causing an error
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): - mobile device (e.g.
for img, path in tqdm(image_dataset): batch_features = batch_features = -1, for bf, p in zip(batch_features, path): path_of_feature = np.save(path_of_feature, bf.numpy())
# he finds the unique values in each column and uses that # array a vocabulary vocabulary vocabulary)) for feature_name in numeric_columns: example dict(dftrain.head(1)) class_fc ( 'first ', 'second ', 'third '))) print( 'feature value: print( 'one-hot encoded: ', # use entire batch since this is such a small dataset.
resolver = strategy = with strategy.scope(): model = ..... ## your tf.keras model model.compile(loss = custom_loss,optimizer = 'custom_optimizer) for i in range(num_its): data,labels next(generator_fn()) model.fit(data,labels)
add any other context about the problem here.
code was removed near these lines in trt_convert.py when the code was restructured in r1.14 branch: phofurl the original code in v1.13.1 tagged version looped through all of signature definitions: phofurl
`tfrecordoptions` 's `compression_type` argument expects an int (defined in while `compression_type` must be a string.
if use_deconv: deconv_layer = kernel_size, padding='same', strides=(2
the error message is the following: phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.12 python version: n/a bazel version (if compiling from source): bazel 0.20.0 gcc/compiler version (if compiling from source): 7.2.0 cuda/cudnn version: n/a gpu model and memory: cloud tpu
actually completed instantly (because of only single match)
python stopped working f attempting to fetch value instead of handling error internal: failed to get device attribute 13 for device 0: cuda_error_unknown: unknown error
calling `matchdelete(id, winnr)` while window `winnr` is focused, everything works fine.
possible deadlock or session can wait for iterator to free resources.
this is because my mapping searches for a pattern describing a tag then it checks the syntax under the cursor; if the latter is not `helpbar` nor `helphypertextjump`, then mapping searches again for pattern, until it finds match with right syntax.
the prescribed `steps_per_epoch` remains the same in the progress bar for all epochs.
returns nan when used with float16.
phofcode this produces the following error log: typeerror traceback (most recent call last) in <module> 1 dict_data = { 'feature1 ': data} ----> 2 seq_layer(dict_data) in __call__(self, inputs, *args,
even with `model.save(filepath)` keras function i face same problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10, 64 bits - mobile device (e.g.
- os platform and distribution (e.g., linux ubuntu 16.04): windows 7 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0.0-beta0 - python version: 3.6.8 cuda/cudnn version: 10.0 gpu model and memory: geforce gtx 1080 8gb
copied from phofurl since this might be a `tpustrategy` issue, since things work with `mirroredstrategy`.
fix is easy: if isinstance(output_shape, (list, tuple, np.ndarray)):
if including tracebacks, please include the full traceback.
i am suspecting overhead is due to tensor allocations, because throughput changes almost linearly with number tensors (when n sufficiently large).
--config=verbs # build libverbs support.
tensorflow conv2d with nchw with mkl slower than nhwc wihout mkl on cpu platform.
large logs and files should be attached.
the following snippet: phofcode causes the exception: phofcode for comparison, the equivalent operation with `tf.gather` phofhyperlink works correctly: phofcode
must have rank at least 3" more: i have been trying to find some way to squash down my data via cnns.
w defaultlogger tensor datatype is determined build time tensors marked as input or output.
then i create another session with `inter=1, intra=8` run my model in later session.
moreover, i compared its performance with scikit-learn implementation.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
screenshot of issue phofimage - vim version: - os: mageia 7.0 rc - terminal: alacritty, wterm, and perhaps others for which vim can 't guess a ttymouse value
the code below should run instead of giving an error.
- linux ubuntu, cpu - installed packages using anaconda with python 3.6 - tensorflow version 1.12.0 - keras version 2.2.4
when a tab is closed, the tab line is not correctly updated.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
2. timeline visualisation in chrome trace format shows a peak allocation on gpu_0_bfc as ~418mb size of 227. this way too low.
i expect it to build the required configuration and software
this affects both python2 and python3.
cwtest` `:messages` to see far more than just one error; if you check `:cc`, you 'll see we are not stuck on the first location, but have traversed the entire list.
so if, on an avx512 build, i do bazel test --config=opt --cache_test_results=no -- test passes and if i do bazel test --config=opt --cache_test_results=no -- test fails, as it should.
just run label_image tf example and with this: `valgrind --tool=massif --pages-as-heap=yes -m mobilenet_v2 _1.0_224.tflite -i grace_hopper.bmp; grep mem_heap_b massif.out | sed -e | sort -g | tail -n 1`
`tensorarray` objects passed as accumulators to `dataset.reduce` lose inferred shapes.
- tensorflow installed from: nightly aar tensorflow-gpu installed from: nightly aar python version: 3.6.7 cuda/cudnn version: 10.0 gpu model and memory: gtx 1060 6gb gpudelegate works fine if i try to run models like mobilenetv2, without any change in code.
i hope you can help me.
- it would be nice to be able to release memory being used by specific models (that are no longer necessary) rather than resetting the runtime every time i run out of memory (which is often).
if you consider this improper use instead, please let me know.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): 18.04 - mobile device (e.g.
there should be no error, as in fit/evaluate methods work fine.
program crashes with error pasted at the bottom of this issue
large logs and files should be attached.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.13.1 / 1.14.1-dev / 2.0.0-nightly python version: 3.7.3 cuda/cudnn version: gpu model and memory: original tfrecords: phofcode cached tfrecords: console $ ll cache/ total drwxr-xr-x 4 lukasgeiger staff 128b may 20 09:45 .
a new line appears in the buffer consisting of the word `hello`
large logs and files should be attached.
the predicted output tensor for a given input tensor should always produce the same result regardless of the size of the batch the sample is found in.
if the entry size larger than 2gb, we should read it from parts not in a whole
i was following this tutorial: introduction to loopback - using it with sqlite3 phofhyperlink these are the steps in short: phofcode now when i start lb with: `$ npm start` lb is showing an error.
max_steps=100) ##### <<<<<---- this kills notebook kernel
the number of outputs determines the number of featuremaps/filters and is at the last position (x,x,x,here) while for conv2d_transpose layers it is (x,x,here,x) as the following code example will show.
a tab character is sometimes wrongly highlighted by the syntax group `helpnote`.
the model works really well (~99% accuracy on test images).
i was trying to prevent the command line window from opening without the use of `<nop>` mappings.
this time progress bar stopped at but notice that function 's results are same as above: phofcode
i don 't have same issue on linux.
phofcode to run this code in current nightly, remove `callback1` and focus on output of first epoch.
this has only effect overloading results `summary`and `plot_model`with not so relevant information instead having concise insight graph.
phofcode this code works completely fine if the first line is replaced with `import keras`.
screen shot at 1 02 32 pm phofimage
i 'm upgrading from `tf.py_func` in tf 1.14 to `tf.py_function` tf 2.0 and it was possible to map the `tf.py_func` that called the augmentation onto the dataset.
behaviour of += operator on a tensor is different when called via tf.function and when called directly
314.2kib 314.2kib 192.0kib 0, 0.
if including tracebacks, please include the full traceback.
i can create a repro if needed.
i have a plugin which provides a custom `*` normal command.
on gpu adamax from appears to apply nans to variables.
`tf.tranpose()` should be fast without the need to resort to reshaping tricks (which will not always apply anyway).
(functionality used from example here phofhyperlink ) phofcode
`reshape` conversion recomputes the reshape size whenever `resizeinputtensor` is called.
the nodes in the with block are executed after the dependant nodes
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): phofcode - os platform and distribution (e.g., linux ubuntu 16.04): phofcode - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): v1.13.1 python version: 3.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory:
please see the short example test case here: phofurl (alternatively, please see the same notebook rendered on a different site here: phofurl in particular, see: cell 6 (expected output plotted by hand) vs. cell 7 (model behavior right after being created, matches expected) vs. cell 10 (model behavior after being saved and re-loaded, does not match expected) cell 4 (model definition, in particular, see first few lines with lambda layer parameters defined in a loop)
the iterator / the dataset should be converted in its graph representation.
to everything i need to but am unable to visit phofurl it simply states it either took long respond and a couple times go that it refused the connection.
the script should print `x == y?
* reproductions should be small, self-contained, correct examples c phofurl occasionally, this won 't be possible, and that 's fine c we still appreciate you raising issue.
1. install the documentation plugin 2. create the extensions path file 3. copy over the node_modules version of the file 4. modify extensions version with some change 5. start/restart server and regenerate documentation 6. restart server if needed 7. load documentation
the model to save its weights properly
1. run `vim --clean -s reproduce.vim` phofcode
the weights are saved separately rather than altogether due to #33947 , using the `model.save_weights()`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.14 - mobile device (e.g.
4. you should get an empty response.
linux smp wed may 8 utc 2019 x86_64 x86_64 x86_64 gnu/linux] - terminal: [e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
then waveglow model convergence quickly after 50k-80k steps.
restoring the model should succeed.
the dataset stops after returning all the records in the sqlite database.
tensorboard crashed when keras finish training the model and do a record in tensorboard
the model should be trained normally.
0x50(%r15) if we trace through the above.
large logs and files should be attached.</em> phofcode everything is up-to-date for tensorflow: $ sudo -h pip3 install --upgrade tensorflow requirement already up-to-date: tensorflow in (1.12.0) requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in (from tensorflow) (1.16.1) requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in (from tensorflow) (0.6.1) requirement already satisfied, skipping upgrade: gast>=0.2.0 (from tensorflow) (0.2.0) satisfied, skipping upgrade: six>=1.10.0 (from tensorflow) (1.12.0) (1.0.6) protobuf>=3.6.1 (3.6.1) (1.12.0) astor>=0.6.0 (0.7.1) numpy>=1.13.3 (1.15.4) termcolor>=1.1.0 (1.1.0) (1.0.5) wheel>=0.26 (0.30.0) h5py (2.8.0) setuptools (39.0.1) markdown>=2.6.8 (3.0.1) werkzeug>=0.11.10 (0.14.1) ``
* writing to tensorboard (*console parameter = false*) tensorboard event file which contains the distribution and histograms of gradients derived from the total loss that has been accumulated over the last epoch.
i use generalized "row" for a slice of a tensor with a given fixed first index, e.g., `x[i,,]` or `pred[i,]`.
i made a dummy network to try and isolate the issue.
here 's a single shell command reproducing the error: $ vim -nu none --cmd 'set fdm=marker ' +"put!
i 'm trying to run tflite model ' phofhyperlink ' with gpu delegate on android devices.
similar behavior can be observed with `d/u` and `f/b` keys.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
if that takes longer to do, at least exposing the error message to the client, rather than crashing would be desired.
last commit: 4a42fbe move to the arm deployment schema (#1107) python 2.7.12 runtime variables: algo_provider "local" algo_ondemand_cellular "false" algo_ondemand_wifi "false" "_null" algo_local_dns "false" algo_ssh_tunneling "false" algo_windows wireguard_enabled "true" dns_encryption "true" ``
i am loading 8k image and it takes around two seconds running through the function.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): (this is the tf 2.0-preview) python version: 3.6.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: here is a gist with the full session output: phofurl
1. go to admin 2. change to random language 3. see that link to
22. proper name a group.
i believe that from user point of view it would consistent if both examples gives exactly the same output- there is no difference in architecture.
here are the results when providing image filled with 0.1 value: cpu host tf: cpu host tflite: (6.2% difference) android cpu: android gpudelegate: test_model.tar.gz phofhyperlink also, tflite converted models seems to always run on cpu despite using gpudelegate, for example this model runs for hundreds of milliseconds no matter whether cpu or gpudelegate is used, while stock mobilenet_v1 (no quantization) runs ~80ms per inference on this device when using gpudelegate and on cpu.
however, it is correctly saved when using
if including tracebacks, please include the full traceback.
its size changes one line.
i deployed one chief, one ps and one worker with protocol 'grpc+verbs '.
1. run mobilenet model classification in golang env 2. input tensor shape tried other batch sizes as well 1 and 3 but no luck.
wrote delegates as described in phofurl and converted model this way: #31015 (comment).
the specified values are correct on the `bufenter` event.
<< " "; return false; }`
this is not an issue with any other imports (like `import logger`).
#4094 is another cindent comma-separated list indentation issue.
userwarning: converting sparse indexedslices dense tensor of unknown shape.
the code below raises error: > tried to convert 'tensor' to a tensor and failed.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
* i'm seeing this when `filetype` is `vim` or nothing.
python import tensorflow as tf tf.zeros([10], dtype=tf.bool) ``
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): google colab - tensorflow installed from (source or binary): binary i suppose - tensorflow version (use command below): 1.13.0rc1 python version: 3.6
instructions for updating: the old _featurecolumn apis are being deprecated.
num_tpus = 4 tpu_cluster_resolver = tpu_strategy = run_config = model_dir=output_dir, num_shards=8 * num_tpus, keep_checkpoint_max=5, ) xlnet_config xlnet_model estimator use_tpu=true, model_fn=xlnet_model, config=run_config, params=none, train_input_fn is_training=true, drop_remainder=true)
random graph operations added during mapping will not be reproducible."
1. run `vim --clean` 2. run `kill -sigtstp pid-of-vim` 3. vim did't quit alternate screen,there 's a mess
provide a reproducible test case that is the bare minimum necessary to generate the problem.
wget phofurl unzip less less
set up to install to local 18.04 vm.
here is the result with cpu: cpu phofimage
- have i written custom code (as opposed to using a stock example script provided in tensorflow): nil - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
the problem doesn't come from how i load data, since no problem if i test for loop over my data.
data transferring overlaps with computing in order to speed up training.
my use case is the build the graph using python interface for tf and export that graph as a pb file.
i pursue following four steps: first building dataset pipeline and defining input function: phofcode then, in step 2, i define feature column with a single key, and shape 784: phofcode step 3, instantiated estimator as follows: phofcode and finally, step 4 using estimator by calling `.train()` method: phofcode
a clear and concise description of what you expected to happen.
when attempting to train a sequential model on the mnist dataset, the model remains at 11% accuracy.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:n/a - tensorflow installed from (source or binary):source tensorflow version (use command below):v1.11.0 python version: anaconda python 3.6.6 bazel version (if compiling from source):0.17.2 gcc/compiler version (if compiling from source): gcc 7.3.0 cuda/cudnn version: cuda 9.2, cudnn 7.3 gpu model and memory: gefroce 1080ti * 4 (4-way gpu), 11,178 mib each.
it should be possible to free the function calling `import tensorflow`.
as i fit a custom model using the keras api.
when passing model_dir with a local file system, it saved successfully.
cause: converting grl.call <__main__.grl valueerror: unable locate grl.call <__main__.grl note functions defined environments, like interactive python shell do expose their code.
the bug hidden when all dimensions are 256 or lower, because of special way phofhyperlink that python treats integer objects between -5 and 256. the code below causes this happen reliably on my machine.
calling `.map` on a `tf.data.dataset` should leave available imports intact.
however, on the first bit of code phofhyperlink i tried i was told that it 'detected 0 issues that require attention', and the report it gave was essentially blank.
what does it mean for this error?
now code : python import tensorflow as tf import tensorflow.keras as k import tensorflow.keras.layers as kl import tensorflow.keras.metrics as km from fashion_mnist tfcfg = tf.configproto() = true sess = tf.session(config=tfcfg) (x_train, y_train), (x_test, y_test) = x_train x_train.reshape((-1, 28, 28, 1)) x_test x_test.reshape((-1, 28, 28, 1)) y_train 10) y_test 10) model k.sequential([ kl.conv2d(32, 3, 1, input_shape=[28, 1]), kl.batchnormalization(), kl.leakyrelu(), kl.conv2d(64, 3, 1), kl.batchnormalization(), kl.leakyrelu(), kl.conv2d(128, 3, 1), kl.batchnormalization(), kl.leakyrelu(), kl.leakyrelu(), kl.flatten(), kl.dense(512), kl.batchnormalization(), kl.dense(10) ]) class def __init__(self, name=none, dtype=none,
4. see error "unhandled promise rejection in browser console"
run `vim --clean`, and create a popup window with `normal`: phofcode vim opens a popup window and uses the highlight group `pmenu` instead of `normal`.
if including tracebacks, please include the full traceback.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
steps to reproduce the behavior: 1. connect to algo vpn server 2. go to phofurl
1, 'number of classes to display per detection box. ')
2. add to your plugin configuration.
node-device colocations active during op creation: with no device assignments were active during op creation.
in a deeprl setting, using a dqn implementation in tf2.
do not see any inference speedup with xla enabled through
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): tensorflow-gpu (1.12.0) python version: python 3.5.2 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.0 gpu model and memory: 1080ti 12g you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
the terminal in normal mode works as expected
i have a keras model that has an embedding column as a feature.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 18.04): - mobile device (e.g.
detailed steps to reproduce the behavior: i used this (with `--cmd 'let g:nothrow = 1 '` to control that it succeeds without throwing) to refine my code: function!
<c-u>")` won 't do anything.
after adding the wrapper, the program reports an error shown in traceback below.
it works correctly for patterns that match longer strings.
i was switching in the app between cpu and nnapi and checked the execution time.
wireguard should generate public key and algo should move on to the next task
the crash happens with or without this line.
`:q` to quit the just now created tab
no screenshot - vim - vi improved 8.1 (2018 may 18, compiled mar 29 2019 - arch linux, up to date - linux consol
opening camera preview: cameradevice-jv-0: stream configuration failed due to: endconfigure:372: camera 0: unsupported set of inputs/outputs provided cameracapturesession: session 0: failed to create capture session; configuration failed `
provide a reproducible test case that is the bare minimum necessary to generate the problem.
extra unsubscribe statements appear in the console when editing the code when the old component is unmounted.
i want to measure tensorflow inference time with different models on my 4-core armv8 cpu.
# can you tell me about how can i do that?
i am trying to use post-training quantisation with a calibration data-set to capture the activation ranges.
if you run code i provided here for you, you will get random values like one that comes in following every time you run code: , dtype=float32), 6] this problem does not happen on a cpu, or if variable type is set to integer.
so this works as expected c i.e.
update password and click `save` 6. view `strapi_administrator` table in postgres db (you should see admin password now set as plaintext in db, and now you cannot log back using new password set)
large logs and files should be attached.
however, when tried to load the frozen graph using the call got following error: file line 430, in import_graph_def raise valueerror(str(e)) valueerror: input 0 of node was passed float from incompatible with expected resource.
this is the error output: valueerror: weights for model sequential_3 have not yet been created.
args parser.parse_args() arguments n_gpus args.n_gpu num_parallel_processes args.n_cpu train_file args.train_file vali_file args.vali_file parameters dropout 0.5 num_classes 3 -1, 0, 1 image_size 40 num_channel 1 learning_rate 0.0001 epochs 3 shuffle_buffer 50000 number samples from which it will sample batch_size 100 input_size (image_size, image_size, num_channel) checkpoint_steps 2000 rseed int( random.random() * (2
(these c++/python style 0-based indices.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 (1809) - tensorflow installed from (source or binary): binary (pip install tensorflow-gpu) - tensorflow version (use command below): 1.10.0 python version: 3.6.8 cuda/cudnn version: 9.0 gpu model and memory: gtx 960 2gb >python predictor_lstm_all.py using tensorflow backend.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): pip tensorflow version (use command below): tensorflow-gpu 2.0.0a0 python version: 3.6.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10 gpu model and memory: geforce 1070
1. put downloaded cifar-10-batches-py to ~/.keras/datasets/ 2. code: import tensorflow as tf (_, _) =
the issue is that it is running in cpu memory rather than gpu memory.
there is some way to release unused compiled model
- have i written custom code (as opposed to using a stock example script provided in tensorflow): nope - os platform and distribution (e.g., linux ubuntu 16.04): android pie (pixel 2) and debian (coral edgetpu dev board) - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no - os platform and distribution (e.g., linux ubuntu 16.04):linux - mobile device (e.g.
after training in my c program a checkpoint file is created from which a model 's weights can be restored for more training or prediction within my c program.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04):ubuntu 16.04 - mobile device (e.g.
multiplying two tf.float64s together is giving me a substantial error over multiplying two np.float64s together.
1. go to strapi admin panel 2. click on users 3. debug network you will see count api error
hence, i 'm suspecting that there may be a race condition, wherein dummy is getting invoked even before gradients are properly updated in backprop phase.
however, when i add like tutorial phofhyperlink , it occurs this error: unicodedecodeerror traceback (most recent call last) in <module>() 20 convert_start = time.time() 21 # hyps, src_frames, en_frames, scores = 'src_frames ', 'encoder_frames ', 'scores '], wav=read_data) ---> 22 hyps, scores = 'scores '], wav=read_data) 23 convert_end = time.time() 24 diff convert_end - convert_start in run(self, fetch_keys,
smaller size of output graph
get the faster rcnn + resnet50 pretrained model from ` phofurl and run with the above config.
parser.add_argument( '--n_cpu ', type=int, help= 'number of processes used for reading parsing data for model input. '
this causes when &column >= 160. found errors in test_helpgrep(): function line 42: expected ['col', [['leaf', 1067], ['row', [['leaf', 1066], ['leaf', 1065]]]]] but got ['row', [['col', [[ 'leaf', 1067], ['leaf', 1066]]], ['leaf', 1065]]] function line 42: expected ['col', [['leaf', 1075], ['row', [['leaf', 1074], ['leaf', 1073]]]]] but got ['row', [['col', [[ 'leaf', 1075], 1074]]], 1073]]] makefile:75: recipe for target 'test_quickfix' failed make[1]:
- tensorflow installed from (source or binary): tensorflow installed from binary.
during handling of the above exception, another exception occurred: traceback (most recent call last): file "crash.py", line 62, in <module> adversarial_images =
we noticed this because our layer serialization failed phofhyperlink after this commit.
memory has not been freed or re-used.
the output is `helpbar`, which is correct.
when passing an empty tensor as the second argument to the `tf.cholesky_solve` function a segfault is encountered.
- with meteor: phofurl - with rollup: phofurl
the text inside the quotes gets selected
`:cdo` aborts on a failure and stops traversing the list.
phofcode if we can use some cache storages just like leveldb which will provide high performance and will avoid memory leak.
if including tracebacks, please include the full traceback.
* format-on-save triggered by `:w` command * fires `bufwritepre` autocmd * this is handled synchronously within `govim` which makes a call to `gopls` for any edits that need to be applied * still within autocmd handler, changes are applied, one by one first change in our repro happens to be a call `append()` this results, as expected, in callback our `listener_add` function with single change `govim` is in process handling this change when we see _another_ identical callback; i.e.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the result on the gpu is significantly different from cpu.
for the object detection app i had to add the gpu delegates, change some code, and use a self-converted ssd model.
i can 't use `textlinedataset` as my csv file contain multi line values in quotation marks.
gpu volatile-utilization cpu utilization on other hand were behaving oddly.
also, i am not sure if the hard-coded model can be trusted or not.
1) create date type field 2) display field in list 3) add an item without filling the date field 4) open item list date phofimage
1. if accuracy is about the same as mae then what does it mean?
i expect it to work, since the model otherwise works fine, and is saved correctly.
changing x to line below and running same code generates similar results.
with quantization=true, phofcode the output of the tflite model seems too different with that of the original tf function.
-0.5 0.25] stateful: [ 0.875 -0.4375 delta: [-0.125 0.0625 bwd:: non_stateful: [1. stateful: 0.875 0.25 -0.5 ] delta: [-0.125 0.25 ]
the rnn layer should check the type of the rnncell and, if it is a subclass of `dropoutrnncellmixin`, reset the dropout masks after each call.
on the the same machine using a virtualenv to force the use of non optimized tensorflow as follows: phofcode run 1: phofcode run 2: phofcode run 3: phofcode
by default, axis is 0 which will mask from the first dimension.
... steps to reproduce the behavior: 1. click get /hhutvnqgqcun or get /kayqpvejwlyy to expand the operation 2. wait
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary, pip install tensorflow==1.12.0 tensorflow version (use command below): 1.12.0 python version: 3.6.4 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: no usage gpu model and memory: no usage machine ram and #cores:
save model should produce a correct savedmodel in non-eager mode so that it can be loaded later.
on the windows-based machine, the same code is run to compile model but instead of training model again - weights are loaded using `model.load_weights()`.
when trying to restore my model in python, trained weights can not be loaded neither by: phofcode nor with phofcode the model loads successfully with: phofcode but this is an untrained model.
if including tracebacks, please include the full traceback.
however, this is output of model.fit(train_dataset, epochs=12, callbacks=callbacks) my windows machines.
error while deploying on do and vultr a clear and concise description of what the bug is.
2. move cursor over 3. type * 4. observe the "press enter to continue" prompt.
gif phofimage in the screenshot, initially, the status line of the second window displays `active`, even though the latter is not focused.
i have tried to find out what was changed since and couldn't find anything related to the issue.
- reference version: '1.10.0 ')` we are experiencing the issue on our models, that used to be trained using tensorflow 1.10, where there was no issue, but since we upgraded to 1.13, it appeared.
18/100 4:04 w allocator (gpu_0_bfc) ran out trying allocate 220.71mib.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- os platform and distribution (e.g., linux ubuntu 16.04): linux - mobile device (e.g.
i 'm trying to convert the keras 's mobilenet model with float16 precision for gpu inference.
it should reduce lr when val_loss doesn 't improve.
')[-1] == '.jpeg' or _.split('.
this occurs only when the focus is on another window.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
filereadable() should return _true_ for path relative to the users home directory.
this code is taken straight from the tutorial phofhyperlink phofcode
however, there 's a huge problem when a layer adds some update operation (like the batchnormalization layer).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): true - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - mobile device (e.g.
14. the button removed from h4 tag in source code benefits users.
this will crash following message: phofcode
when `~` appears somewhere in the middle of the string, it won 't be expanded as opposed to `%`.
i have tried pickle and npy without success.
i am no expert on dynamic libraries on windows, but i have read that the `.dll` and the `.exe` have separate heaps.
reference: phofurl according to reference, it draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.
prints out the result of sess.run and exports the 'model ' phofcode this is the complete output: phofcode the model is then served using docker via the command: phofcode this is output of running docker command: phofcode however when this is run: this error thrown:
2. make the content type public.
when use the official on right get correct results.
apply delegate for gpu next operations are not supported by gpu delegate: average_pool_2d: expected 1 input tensor(s), but node has 0 runtime input(s).
info:tensorflow:finished evaluation at info:tensorflow:saving dict global step 1: acc = 0.0, global_step = 1, loss = 0.0 info:tensorflow:saving 'checkpoint_path ' summary global step 1: info:tensorflow:loss final step: none.`
the floating point input data are multiplied by the scale value whereas they should be divided by it.
a non-compiled predict of a single image using restnet50 takes 5ms, a compiled version takes ~140ms.
if including tracebacks, please include the full traceback.
python 3.6.8 dec 24 2018, [msc v.1916 64 bit (amd64)] on win32 type "help", "copyright", "credits" or "license()" for more information.
- os & version: windows 10 1806 - redis-server version 9.3.817
phofcode also see colab notebook here: phofurl
textprop_matchadd phofimage - seen in terminal vim on arch linux, and gvim on windows 10
# otherwise, model.evaluate() will get error.
however, when selection is set to exclusive, double clicking on a word and then moving the cursor backwards leaves the word that was double-clicked unselected.
- os platform: win 10 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.13.1 - python version: 3.6.7 cuda/cudnn version: cuda cudnn 7.5 gpu model and memory: rtx 2070 8gb
if including tracebacks, please include the full traceback.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): google colab - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): none - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
if custom layers are used weights of the model cannot be saved using model.save_weights() or exported to the savedmethod.
@dataset.py:45] instances loaded 100%|| @timer.py:48] load groundtruth boxes for valminusminival2014 finished, time:8.2563sec.
python import tensorflow as tf import tensorflow.keras as keras import types import numpy as np json os sys hostname_idx = { "master": 0, "slave1": 1 } worker_index = = = json.dumps({ 'cluster ': { 'worker ': ["master:1531", "slave1:1531" ] }, 'task ': { 'type ': 'worker ', 'index ': worker_index} }) mirrored_strategy options_distribute tf.data.options() true def sliding_window(values, window_size=2, stride=1, start=0): i start r none if i+window_size <= len(values): yield values[i:i+window_size] while true: i i+stride if (i+window_size-1) < len(values): yield values[i:i+window_size] else: break num_timesteps 100 num_timesteps_to_predict 1000 num_signals 2 num_inputs batch_size 2
they fail with `e149: sorry, no help for expr-=~?`.
i would like to save the model.
in fortran code, an empty string represented by two consecutive double quotes seems to be recognized as a beginning of a multi-line string.
parameters in the path should display the same as path parameters placed in the operation.
i 'm running asynchronous distributed training in tensorflow using parameter server strategy.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
only expected following keys: ['dense_73']
this very simple defun'd function should execute quickly, including the graph-building and graph optimization steps.
n/a - vim version - windows 64-bit, compiled with mingw1809, macos 10.14] the problem does not appear with the vanilla downloaded from the web site.
when invoking `keras.model.fit` with two `data.dataset`s, one is for training, and one is validation, so that the training one is infinite with `epochs` and `steps_per_epoch` provided to `fit`, and the validation one is finite without any additional parameters to `fit`, the progress bar shows incorrect number of steps after the first validation.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: 3.6.6 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: na gpu model and memory:
for a reproducible test, i also tried this, to see if the memory leakage is due to something else: phofcode i can still see a huge memory usage when executing this.
it happens for both `vim` and `gvim`.
the following code causes a segfault: phofcode
this all worked fine and yesterday was running my models on my gpu in r with no problems.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): tf.version.version: tf.version.git_version: python version: 3.6.8 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: n/a here is the stacktrace: attributeerror traceback (most recent call last) in <module> 18 return 19 ---> 20 increment() # raises attributeerror: '[...].eagertensor' object has no attribute 'assign_add' in __call__(self, *args,
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os - mobile device (e.g.
the problem is that the model won't learn anything.
model will compile and train without issue.
the fetched result should be the value of the variable.
the model should train on two gpus asynchronously.
when `completeopt` includes `popup` and there is `info` to display: * the default highlight group appears to be `pmenusel` as opposed to the documented `pmenu` * setting an alternative highlight group via `completepopup` has no effect
devices: streamexecutor device (0): <undefined>, <undefined> warning: logging before flag parsing goes to stderr.
however, my methods are currently yielding a
stack-trace of one such exception, which happens when cudnn initialization fails, is provided below.
just the tensorflow tutorial code directly from the tensorflow website.
the inputs in timeline_label are different with information in the graph node.
for object detection app i added gpu delegates, changed ismodelquantized false (and don't use isquantized anymore), and use a self-converted ssd mobilenet model (because is currently written for quantized models using cpu and doesnt have gpu delegates in original code).
training should always end with a new checkpoint.
this bug happens for every script run with different settings.
from python3 train.py --logtostderr --train_dir=training/
if including tracebacks, please include the full traceback.
should work with the default `__call__` method of any subclass of `tf.keras.layers.layer` under both tensorflow 1.14 and 1.15-rc0.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.13.1 python version: 2.7 bazel version (if compiling from source): 0.19.2 gcc/compiler version (if compiling from source): 4.8.5 cuda/cudnn version: cuda 9.0; cudnn 7.3 gpu model and memory: tesla v100, 16g $ tensorflow_model_server --port=8413 --rest_api_port=8414 --model_name=resnet i building single tensorflow model file config: model_name: resnet model_base_path: i adding/updating models.
according to and cudnn release notes, 3d convolution operations on volta architecture should benefit from amp training with the tested versions of tf, cuda and cudnn.
i get a beatiful new vpn droplet
the following test case fails.
if including tracebacks, please include the full traceback.
load_weights should work this problem only happens on two+ layers of nested model with non-trainable weights.
tpu estimator should run without `steps` or `max_steps` as mentioned in the documentation.
import utils file line 6, <module> .
`# save the entire model as a savedmodel.
this confirmed by fact that if you set ` 'winminheight '` to `0`, height last which entered `7` (height old + `1` for status line was above).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes, see example below - os platform and distribution (e.g., linux ubuntu 16.04): windows - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0 alpha 0 gpu python version: 3.5.3 cuda/cudnn version: cuda 10 gpu model and memory: geforce 960m
compile an application without any errors
@evhan (maintainer of the scheme syntax file
trying to load .tflite file on android device.
i 'm porting a ml model ( phofurl ) implemented in pytorch to tensorflow (using the keras layers, knowing that tf 2.0 will come soon).
after a bit of trouble installing cuda/cudnn etc.
it has only 22 port connections open everywhere, 80 port is opened for few ips(mostly debian repo's).
` # multi worker strategy strategy = with strategy.scope(): train_dataset, validation_dataset, test_dataset = load_data() multi_worker_model = alexnet() # fit epochs=epochs, validation_steps=6, callbacks=callbacks, verbose=2) # eval # save model 'model ')) 'model ')) `
provide a reproducible test case that is the bare minimum necessary to generate the problem.
import tensorflow as tf a = batch_size=2) batch_size = tf.shape(a)[0] cell = b = dtype=tf.float32)
the following is the fragment code in estimator_training.py.
i can see the keys.
i have a repro of this and can either provide the precise steps for the repro or happily provide more logging/debug info as required.
assertion errors or other exceptions, however no crashing of python.
`x = (tf.zeros((32, 32, 12)), tf.zeros((32, 32, 12)))` why is it faster for me to use a list comprehension with t.numpy(), appending it to a list, and then run np.array(list), than it is run np.array(x)
the `keras_to_tensorflow.py` file is from here phofhyperlink , the `vgg_blstm_ctc.py` file is from here phofhyperlink , the input model is from here phofhyperlink .i has changed the `keras_to_tensorflow.py` file like this: phofcode
the mnist.py script achieves a test accuracy of <85% after 12 epochs of training when using tensorflow-gpu=1.14.
when trying to build a simple model in eager execution mode using sgd as an optimiser the following exception is thrown: phofcode
this sample code should run.
it takes 'smartcase ' into account by pressing the keys `/ up enter c-o`.
and what i 'm expecting to see is phofcode here i 've chosen a shard with a test that passes on my machine but you can hopefully see the difference.
when i run the following program, which is supposed to be able to differentiate between images of cats and dogs using a convolution model, the program gets stuck at the fit method.
see phofurl for some common reasons and solutions.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
please let me know what could be the route cause of it.
raise an error when fitting tf.keras model when the training dataset size less then batch_size and validation_split is 0.0. if using a batch_size less then dataset size or setting validation_split, the fitting is good.
a future version of pip will drop support for python 2.7. more details about 2 support in pip, can be found at phofurl requirement already up-to-date: virtualenv (16.7.2) bigmac-87:algo-master manou$ python -m virtualenv --python=`which python2` env && > source env/bin/activate && > python -m pip install -u virtualenv && > install -r requirements.txt running with interpreter already using interpreter new executable installing setuptools, pip, wheel... done.
looping over in a continuous loop degrades in performance after a few hundred iterations.
according to the docs (and docker convention), the `latest` docker images should contain the latest
use "id" as a field name.
eager execution isn 't working on a jupyter notebook with gpu.
optimizer: tf.keras.optimizers.adam interestingly, when i try to save my model weights only using model.save_weights where model is a tf.keras.model instance, it works fine, no error.
the fix was to move tensor computation into `call` method, though it was a silent fail and would have been very difficult to troubleshoot if it didn't fail on a nightly.
2. run `vim --clean` (or `gvim --clean`, happens in both) 3. execute `:autocmd bufread * silent!
1. go into vim's `src/` directory and run `vim --clean`.
trying to run simple example on cpu in distributed mode according to the documentation.
moreover, if i use the metric, it seems to burn batch size into model, so it behaves like a stateful model.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0-rc1 python version: 3.6.5 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: na gpu model and memory: na the error message(with eager execution enabled) is: traceback (most recent call last): file "src/t.py", line 11, in <module> )(x) file line 473, in __call__ return super(bidirectional, self).__call__(inputs,
the rnn layer with an rnncell does not reset the states of dropout masks compared to the layer implementations of the cells.
i train a keras model and let it be saved by the after the training is finished, i want to continue it from the saved checkpoint.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the training should finish after the set number of epochs.
when filing the bug, set the verbosity to 10 (on linux, `export autograph_verbosity=10`) and attach full output.
when trying to use tf.nn.embedding_lookup during a typical training workflow, everything seems to work fine.
the other model starts with a rnn layer.
ipod touch, ipad pro, iphone xs, iphone se.
title of preview popup window should be truncated (at the beginning?
- have i written custom code: yes.
run smoothly, `c_api.tf_sessionrun` expected to be thread-safe when used with a separate session and graph for every thread.
the match should be included in the resulting highlight: to reproduce for `v_gn`, search `5` and highlight `34567` from right to left, then `gn`.
the division operation should be used to convert the floating point to quantized fixed point value and the corresponding tests changes respectively.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macosx 10.13.6 - mobile device (e.g.
the model can be learned on colaboratory 's tpu using custom layers.
you could reproduce the error on the following tf-hub colab page: phofurl adding a cell with the `modelcheckpoint` code phofcode and then adding the callback to `fit_generator` function of model: phofcode
after restoring from the checkpoint, i expect the iterator to return the same elements as it did after saving the checkpoint.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: not tested - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.7.3 bazel version (if compiling from source): not compiled from source gcc/compiler version (if compiling from source): not compiled source cuda/cudnn version: using cpu gpu model and memory: using cpu nothing here
i assume phofcode cannot be quantize, and nothing to be quantize actually, since if i add phofcode fully quantize into int8, it tell me split_v is not supported.
profile data of language model: node name | requested bytes | total execution time | accelerator execution time | cpu execution time
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no - os platform and distribution (e.g., linux ubuntu 16.04):linux ubuntu 16.04 - mobile device (e.g.
i have a multi-input network that uses a `tf.bool` `tf.placeholder` to manage how batch normalization is executed in training and validation / testing.
a message should be displayed.
when attempting to run a session via c api on a tensorrt-using graph, it is possible for exceptions to not get caught and abort the whole process via `std::terminate`.
`tf.function` with an input signature should behave like graph mode with constant memory usage and no "warmup" phase.
i think that we only need to define the self.dense(200) only once, but we can reuse it in the def call(self, x)
when run in the debugger, if there is a nan in the loss, the debugger crashes.
- vim version phofcode - os: ubuntu 19.04 - terminal: xterm(330) add any other context about the problem here.
phofcode phofimage phofcode phofimage phofcode phofimage phofcode phofimage
when i run model.evaluate with verbose=true, i get too many "=======" signs.
if a warn or msg could reminder the user - the remote saving failure - would be great.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i attached a spreadsheet file with results of training with different datatypes and my expectations.
this will result in data-type errors in the converted estimator.
however, 1.11 is not supported in gcloud ml-engine.
this does not occur in terminal vim: phofimage
if a model works during model.fit(), i expect the same model to also work for `saved_model.save()`.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
however, for the following code, keras can pass while `tensorflow.keras features an error: phofcode the error is: invalidargumenterror traceback (most recent call last) in <module>() 10 print(a, b) 11 ---> 12 13 14 # notice that add_function created is independent of variables in __call__(self, inputs) 3074 3075 fetched = -> 3076 3077 3078 return in __call__(self, *args,
phofcode please copy to a script and run as: without distributed scope: python3 script.py with distributed scope : python3 script.py distributed
expected the example section to be the same depth as the model section swaggeruiissue1 phofimage swaggeruiissue2 phofimage
when supplying bias tensor `-1`(for making bias optional), it raises runtimeerror when running it in python interpreter as shown the below(or seg fault/asan error in c++ interpreter) supplying zero-valued bias tensor(i.e.
### model description (same for both eager and static) phofcode also, the following variable definitions are shared in both the eager and static implementations phofcode ### eager training i show just a single training update, that 's run in a training loop phofcode ### static graph training first i show the graph definition,then how is used in the loop #### graph def phofcode and this is what 's inside loop (executed in a monitoredsession): ` the model definition is same, loss definition is same, steps and same, only difference eager mode enabled or disabled.
this probably is valid for other parameters like e.g.
too much code to extract to make a barebones example, but i could do it if absolutely necessary.
the results with eager on are: d loss: collapses to zero, that 's wrong since its aim to stay around 0.5 bad d phofimage generated images: wrong, bad reconstructions since d collapsed bad_gen phofimage while when off, discriminator loss looks correct and generated output are one expected: d loss: good d phofimage generated output: good_gen phofimage
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 home - mobile device (e.g.
it uses the tf session to estimate the value.
[0204 @dataset.py:45] instances loaded from 100%|| @timer.py:48] load groundtruth boxes for train2014 finished, loading annotations into memory... done (t=8.37s) creating index... index created!
training samples train_samples train_generator$n # 7000 training no.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - tensorflow version (use command below): 1.12.2 - python version: 3.6.5 image phofimage
i have variable-lenght input (sentences), so i want to use raggedtensors in my `tf.data.datasets`.
then, i run the assign operation in the end() function.
the metal gpu delegate gives the same output as the cpu interpreter.
- os platform and distribution (e.g., linux ubuntu 16.04): - ubuntu 17.10 mobile device (e.g.
i have been using this configuration for about a year but the flashing only started recently.
if including tracebacks, please include the full traceback.
- custom code - windows 10 1803 - tensorflow installed using pip with conda as environment manager.
test.vim phofcode 1. :so % 2. cursorline also is displayed in the main window
provide a reproducible test case that is the bare minimum necessary to generate the problem.
instructions for updating: apply constraint manually following the optimizer update step.
exact same code working with tensorflow 1.12
so i myself cannot dig this anymore without help of tf team members.
http request sent, awaiting response... 200 ok length: unspecified [application/zip] saving to: master.zip.2 master.zip.2 [ <=> ] 1.03m --.-kb/s in 0.08s (12.1 mb/s) - master.zip.2 saved root@:~# apt install unzip reading package lists... done building dependency tree reading state information... done unzip is already the newest version (6.0-21ubuntu1).
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.6.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: here is the stacktrace: phofcode
i assume that the mse should be equivalent to the mae squared but i may be wrong.
w0707 deprecation.py:323] from (from is deprecated and will be removed in a future version.
if i run it by setting tf_config environment variable with only 2 different node or even a single node then it is hanging.
### run the code below in a local machine: phofcode
i wanted to implement an eager mode version of the large margin code from google-research code phofhyperlink .
order) 5. open your browsers dev-console
i have noticed an issue while iterating over a distributeddataset (using a that contains data of type string, with eager execution enabled.
- have i written custom code: minimally reproducible example below uses only stock 1.14.0 code.
'x ' button close 24. try it edit 25. apikey description cell
', loss) 361 return self _train_model(self, input_fn, hooks, saving_listeners) 1137 return hooks, saving_listeners) 1138 else: -> 1139 1140 1141 def input_fn, saving_listeners): input_fn, 1167 1168 estimator_spec = self._call_model_fn( -> 1169 features, labels, modekeys.train, self.config) 1170 global_step_tensor 1171 worker_hooks, _call_model_fn(self, features, labels, mode, config) 1125 1126 logging.info( 'calling model_fn. ')
click update a couple times in this repl phofhyperlink
start with `gvim --clean bug.vim` phofcode the first character is tab.
`'on epoch end: {}'` print statements) appear
the session return an ndarray [[value_estimate]] instead of value_estimate.
this graph is then imported in go environment using go interface for tf.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:na - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.4 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: na gpu model and memory: na include any logs or source code that would be helpful to diagnose the problem.
the if statement is as follows: # compute jaccard for each default box for gt_list in actual_list: gt_label = gt_list[1] gt_box = gt_list[0] for i in range(len(matches)): # i].shape --> (4, ) jacc = jaccard(gt_box[0], i]) jacc_thred = tf.where(0.5 <= jacc, 1, 0) if(jacc_thred == 1): matches[i] 4 # <--
* converting the 80-class model to .tflite using quantization toco works reasonably well (<1% drop top 3 accuracy).
x1 gets dropped from output of saved model.
looks like it is unable to backpropagate correctly to the embedding layer with variable length.
to ask for api token and then location
( i have tried several toco converts on other models, they do not have this behavior. )
i 've installed on disabled ipv6 servers before and did not experience this issue.
the notebook runs faultlessly on the ec2 instances.
num_filters2 32 there 32 these filters.
on google colab, phofcode tensorflow version:
4. type ":verbose set formatoptions?"
4. then go to windows event logs, saw crash log.
foo() let l:var = 0 try " this triggers the problem: "throw 'something ' " problem occurs in the next line: let x = wincol() || l:var catch /something/ endtry endfunction call foo() this surely is valid vimscript.
datasets, info tfds.load( name= 'mnist ', with_info=true, as_supervised=true ) mnist_train, mnist_test datasets[ 'train '], datasets[ 'test '] you can also do get total number of examples in dataset.
i found that test_helpgrep() in fails on my bench.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
t [[{{node w internal: [_derived_]inconsistent output shapes, got [16], but expected [64].
i 'm running on cpu.
so guess certification is installed and added to ssl.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
after of running it shows - 'sequential ' object has no attribute 'total_loss '-
how come the operations are different even when the models are of same version?
simple reflection of props fails in 3.7.1 phofurl
i could not choose a smaller example because the buggy values appear only in part of the second and all of following batch items.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
perhaps the documentation just needs to be fixed to add a `call()` method and an explicit `signature` argument, but it would be nicer if it could work without having to do that.
the bottom window is squashed to `0` lines.
to perform training i use a standard tensorflow 2.0 loop (with dummy input for simplicity) phofcode the crucial part here is the `something` object which is passed to model.
the normal built-in redirect to the endpoint including the final slash should occur.
steps to reproduce the behavior: start the application
workarounds: - if instead of `inputlayer` you add `input_shape` to `dense` layer, it works.
multi gpu training of "predrnn" keras model using works without any warning related ops compatibility with devices.
detailed steps to reproduce the behavior: 1. run `vim` 2. open netrw for directory with c files 3. type `:mf *.c` 4. nothing is selected
1. query the user model with the below graphql query 2. you should get an empty result
here is an example created to reproduce this behavior (with the workaround) - phofurl
these warnings should not be raised.
the training should not crash because of the accuracy computation.
== uname -a linux 4.14.79+ #1 smp wed dec 19 pst 2018 x86_64 gnu/linux check pips mesh-tensorflow 0.0.5 msgpack-numpy 0.4.3.2 numpy 1.14.6 protobuf 3.6.1 tensorflow 1.12.0 tensorflow-hub 0.2.0 tensorflow-metadata 0.9.0 tensorflow-probability 0.5.0 check for virtualenv false tensorflow import tf.version = 1.12.0 tf.git_version = tf.compiler_version = sanity check: array([1], dtype=int32) env ld_library_path dyld_library_path is unset nvidia-smi nvidia-smi has failed because it couldn 't communicate with the nvidia driver.
devices: streamexecutor device (0): <undefined>, <undefined> found device 0 with properties: name: geforce rtx 2080 ti major: 7 minor: 5 memoryclockrate(ghz): 1.635 pcibusid: libcudart.so.10.0 libcublas.so.10.0 libcufft.so.10.0 libcurand.so.10.0 libcusolver.so.10.0 libcusparse.so.10.0 libcudnn.so.7 adding visible gpu devices: 0 libcudart.so.10.0 device interconnect streamexecutor with strength 1 edge matrix: 0: n created tensorflow with 7784 mb memory) -> physical gpu (device: 0, name: geforce rtx 2080 ti, pci bus id: compute capability: 7.5) libcublas.so.10.0 libcudnn.so.7 e could not create cudnn handle: e could not create cudnn handle: w unknown: failed to get convolution algorithm.
traceback (most recent call last): file line 427, import_graph_def graph._c_graph, serialized, options) # pylint: disable=protected-access node expects be colocated with unknown node during handling of above exception, another exception occurred: traceback (most recent call last): file line 71, <module> model = file line 142, load_model return compile) file line 86, load model = 506, load_internal export_dir) 102, __init__ super(kerasobjectloader, self).__init__(*args,
with access to at least 2 gpus, begin training the model with `initial_run=true`.
1. open console 2. run phofcode 3. choose custom installation 4. provide configuration 5. see this error
this greatly slows down resnet50 phofhyperlink , from 709 to 551 images/sec.
in an untouched tensorflow repo with r1.12 checkout and bazel 0.18.1 installed, run `bazel build
algo vpn server will be configured for all users listed in the config.cfg file.
our `listener_add` callback function, with note at stage we have _not_ returned from original callback (the logs show be case) we see same callback 5 times in total.
either tf.cast() gives me a tensor that tells me it 's on the cpu, or else tf.cast() should work correctly.
-using tf 2.0.0b-gpu on google colab.
phofcode since i can iterate on the dataset using a for loop phofcode i do expect i can treat the dataset as an iterator, thus extracting the next element by calling `next(dataset)`.
if including tracebacks, please include the full traceback.
i also tried to make it save a mounted google drive with no more success.
python import math import numpy as np import tensorflow as tf error = true n_features = 100 batch = 2 """ model """ x = dtype=tf.float32) w tf.variable([1.0] * n_features) b tf.variable(1.0) z tf.reduce_sum(w * x, axis=1, keepdims=true) + b """ loss is incorrect if error is true """ if error: y_ tf.sigmoid(z) else: y_ 1.0 / (1.0 + math.e
(issues related to the runtime files should be reported to their maintainer, check the file header.)
1. run `gvim --u none` 2. copy the text in the paste to the system clipboard phofurl 3. run the following ex commands: phofcode the last ex command will cause a crash
any help in this regard would be much appreciated.
2. then i save model by 3. finally, i load model by while loading i get the error message : > unknown layer: sequencefeatures
i faced issue in memory allocation.
- os platform and distribution: ubuntu 18.04 - mobile device: xiaome 8, android 9 - tflite installed from (source or binary): nightly aar tflite-gpu installed from (source or binary): nightly aar gpu model and memory: --------- beginning of crash e/androidruntime: fatal exception: inference process: pid: 13748 internal error: failed to apply delegate: tflitegpudelegate prepare: shader compilation failed: error: 0:6: 'data ' : syntax error: syntax error internal error: no main() function!
this is only resolved when the inputs are scaled by 255.
large logs and files should be attached.
workers will need to restart training if any fails.
the result of `:hi` command has no space between highlight name and "xxx" if the length of the highlight name >= window width - 1.
- vim 8.1, patches 1-2003 - os: ubuntu 18.04.3 lts - terminal: [konsole and gui] i have tried the following as workarounds: - call `:redraw` at the end of startupcurrentlinebug create a hidden window with `popup_create ( ... , hidden: 1 )` and `call popup_show()` at the end of startupcurrentlinebug none of the attempted workarounds worked.
$ vim --version vim - vi improved 8.1 (2018 may 18, compiled jul 29 2019 included patches: 1-1776 compiled by arch linux huge version with gtk3 gui.
small batch size should also give me the correct result since my original un-optimized graph can handle small batch size.
i can say my tf.gather is in tower/cond/while i think tower is the problem, since i used tf.gather in cond/while.
sum was added to the blacklist in the justification is "the dice loss function used in u-net uses sums that overflow fp16"
1. if we do not use then it will not throw exception 1. if we do not use `return_state`, then it will not throw exception the problem is at: phofurl which `state` returned from `k.rnn` is a `tuple` instead of a `list`, so that ` [output] + states` will fail and throw exception.
only expected following keys: [ 'output_1 ', 'output_2 '] ``
i expect the `tensorboard` callback to gracefully handle these issues, perhaps display a warning, but do not force a recompile or a kernel restart.
only do things necessary for prediction while doing prediction.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.14 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0.0-beta0 (issue also happens on tf 1.12 and 1.13) python version: python 3.6.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: n/a
i changed the mnist url from https to http.
the ability of the svd to handle ill-conditioned matrices is important for our application.
i expect a function to be trace just once for a given set of inputs.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): `yes` - os platform and distribution (e.g., linux ubuntu 16.04): `linux archlinux #1 smp preempt tue nov 13 utc 2018 x86_64 gnu/linux` - mobile device (e.g.
assuming code snippet is pasted in a file called `test.py`, run: phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes and no - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
the program exits with the following error: phofcode
number of eligible gpus (core count >= 8, compute capability >= 0.0): 0 (note: was not compiled with cuda support) starting new session optimization results for grappler item: graph_to_optimize constant folding: graph size after: 388 nodes (-14), 510 edges (-14), time = constant folding: graph size after: 388 nodes (0), 510 edges (0), time = info: initialized lite runtime.
phofcode i get the following output: phofcode when i run on the cpu, problem goes away.
external command should keep current directory, as in linux do.
while the existing issue 1161 seems related, i 'm failing with the current-latest algo (commit db34d55).
phofcode i 'm not using any common ops (k.argmax, kround, etc.)
phofcode produces the following three warnings: warning: logging before flag parsing goes to stderr.
calling `model.predict(x, steps=n)` on both an uncompiled (1) and compiled (2)`tf.keras.model` should produce a list of numpy arrays corresponding to the outputs of the model without any errors.
see the attached gist phofhyperlink
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes, i converted the speechcommands example phofhyperlink to use the tensorflowliteswift api instead of the c wrapper in the original example.
python #!/usr/bin/env python3 import sys import tensorflow as tf # otherwise nothing works, and it really sucks, but is declared in the docs multi_worker_strategy = def main(): batch_size = 12 features_shape = 372, 558, 3 labels = 10 sample def with_shape(t, shape): t tf.squeeze(t) t.set_shape(shape) return t ds_train s: (s, tf.ones((labels,)))) s, l: (with_shape(s, (batch_size,) + features_shape), with_shape(l, (batch_size, labels)))) ds_val s: (s, tf.ones((labels,)))) lambda s, l: (with_shape(s, (batch_size,) + features_shape), with_shape(l, (batch_size, labels)))) with model weights=none, classes=labels) + features_shape) model.summary() optimizer cross_entropy loss=cross_entropy, metrics=["accuracy"]) model.fit(ds_train, validation_data=ds_val, epochs=1, steps_per_epoch=100) if __name__ == "__main__": sys.exit(main()) ``
- os platform and distribution: linux ubuntu 16.04 - tensorflow installed from (source or binary): built from source, with xla and mkl support, use nvcc as compiler.
calling tf.cast() has an error saying it 's on the cpu.
traceback (most recent call last): file "t.py", line 16, in <module> model(data) file line 620, in __call__ outputs = self.call(inputs, *args,
any other function that expects a `compression_type` should also respect the same api.
1. implementation of the spectral normalization function to be passed to kernel_regularizer argument of the tf.layers module (e.g.
i want to use toco to transform the pb file to the tflite file
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): arch linux 5.0.10 - mobile device (e.g.
each of these features has their own time stamps (e.g.
provided value: 0.0 requested dtype: int32).
a new version should have similar or better performance than the previous version.
the expected behavior is that an epoch will completely go through the dataset, and it will report a reasonable validation loss, and it will take some time to go through the entire dataset.
1. change amountlimit to 10 2. go to graphql phofurl 3. make a query without limit 4. it returned me 100 records,
in one of my research, i need to assign values to large sparse matrices.
we have handled couple of cases in issue #11431 and #14017 and it happens because of numpy 's different behavior on intel vs z (pr #12963) .
large logs and files should be attached.
since it 's stochastic, it might be run without error in single run.
convolutional layer in the tensorflow model should remain the same even after tfliteconverter.
for exampleon previous versions, using eager execution this worked: `image = random_translation, 'nearest ') ` but after this change i get a `valueerror: the truth value of an array with more than one element is ambiguous.
- vim version (bug is present also in vim 8.0) - os: debian stretch - terminal: xfce4-terminal
1. huge gpu consumption when running a simple matrix multiplication.
i think it is reproducable with any python code that load and serve a neural network.
8.1.892] - os: opensuse 15 - terminal: gui this was initially reported at phofurl
2. method #2: directly restore from checkpoint using metagraphdef and readbinaryproto.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
large logs and files should be attached.
as it is, it just welcomes bugs.
after attempting to train with multiple print statements, i found that the place where code seg faults is in train function: phofcode specifically on this line, where self.sess.run is called: phofcode
the fact that an exception is underway should not influence the parsing result of (valid) vimscript.
- tensorflow installed from (source or binary): source.
phofcode - os: windows 10 17134 - terminal: gui,cm
converter = print("converter = " + str(converter)) tflite_model = converter.convert() #no save the tflight model to file
the networks should be trainable individually and in the concatenated version, no matter if we specify metrics or not.
*/ @@ -5540,6 +5541,7 @@ find_help_tags( t if (strcmp(arg + 5, expr_table[i]) == 0) t { t tstrcpy(d, arg); + t targ_is_literal = 1; t tbreak; t } } -5716,7 find_help_tags( *matches (char_u
- what is the top-level directory of the model you are using: models->research ->deeplab - have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - tensorflow installed from (source or binary): source tensorflow version (use command below):1.13 bazel version (if compiling from source): cuda/cudnn version: na gpu model and memory: no gpus used exact command to reproduce: tflite_convert --output_file=test.lite --inference_type=float --mean_values=128 --std_dev_values=128
with mirroredstrategy the same accuracy is reached as training on one gpu in shorter time (ideally in half the time).
after migrating to tf 2.0 i tried to run the exact same model and got much worse results
tf.tuple should support index with tensor in graph mode.
i would expect that two models trained to similar accuracy on the same architecture should both quantize reasonably well, despite the number of prediction target classes in the fully-connected layers.
`estimator.train` should succeed with multiple gpus and multiple applications of a keras layer.
for example, a python function that is actually a wrapper around an r function.
on a tpu (colab) running model.evaluate on a tf.data.dataset build with tfrecords throw : compilation failure: dynamic spatial reduce window is not supported: %reduce-window.21 = %reshape.12, f32[] %constant.16), window={size=1x3x3x1 stride=1x2x2x1}, to_apply=%max_f32.17, ttpu compilation failed the fit method work perfectly with the same dataset.
it 's possible that this is also a feature request and not a bug, i am not quite sure.
- os platform and distribution (e.g., linux ubuntu 16.04): debian testing - tensorflow installed from (source or binary): binary - tensorflow version (use command below): - python version: 3.7 error log: traceback (most recent call last): file "main.py", line 52, in <module> apply_grads([true, false], grads, vars) file "main.py", line 23, in apply_grads grads_per_model[i], vars[i]) file line 414, in __call__ return
- os & version: windows 10 1809 - redis-server version: current redislap - redis desktop manager:
and make `tensorflow.keras` correct also.
i am a complete novice at this so any help will be greatly appreciated.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): 18.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): python version: python 3.7.3 cuda/cudnn version: cuda 10, cudnn 7.4.2.24 gpu model and memory: 4 x nvidia v100 phofcode
in this latter case, we have a different filter for each batch element, and so cannot apply the normal tf.conv functions without a map.
gradienttape.gradient() returns gradient for only the last variable in var_list, which is a strange behavior different from tf.gradient()
here is full traceback: attributeerror traceback (most recent call last) in <module> 6 modelsavefile = 7 ----> 8 base_model = 9 10 in load_model(filepath, custom_objects, compile) 228 raise valueerror( 'no found in config file. ')
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no - os platform and distribution (e.g., linux ubuntu - mobile device (e.g.
t [node: iteratorfromstringhandle = output_types=[dt_int32], phofhyperlink ]] <dtype: 'int32 '>
the code am modifying is same as this code phofhyperlink , and what am trying to add attentioncellwrapper between 126 127 lines.
can anyone please give any clue that what should i do to improve the performance of my gcp instance.
no error during the training with the filtered dataset in the `input_fn` of the estimator
num_train_examples num_test_examples %d, num_test_examples %d" %(num_train_examples, num_test_examples)) buffer_size num_train_examples batch_size_per_replica 64 batch_size batch_size_per_replica * epochs 20 train_dataset train_dataset eval_dataset with strategy.scope(): ### compile ## seperate activation layer so that batchnormalization can be added later on.
sub operator should be running on the gpu as per the documentation provided.
this means subsequent runs of the same code with an incomplete cache will fail with: phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.13.1 python version: 3.7.1 bazel version (if compiling from source): 0.20.0 gcc/compiler version (if compiling from source): 7.3.0 cuda/cudnn version: 10.0 gpu model and memory: tesla k80
tf.layers.conv2d) phofcode 2. function is passed to a layer in [tf.layers] module `conv_layer = filters=128, ` 3. run regularizer after each update phofcode
file downloads correctly observed: file is corrupted as described in the linked issue
- custom and stock code - tensorflow installed from binary: - tf_env.txt phofhyperlink - cuda/cudnn version: nvcc: nvidia (r) cuda compiler driver copyright (c) nvidia corporation built on cuda compilation tools, release 10.0, gpu model and memory: name: nvidia geforce gtx 1060 with max-q design memory: 8124 mb perf_test_tf_2_0.log phofhyperlink
returns: image(s) with same type and shape as `images`, translated by given vector(s).
tensorflow crashes when checking the input_signature of a tf.function decorator when using multiple gpus in a mirroredstrategy.
there should only be this many equal signs: not
when ` 'cursorline '` is set, sometimes, the syntax highlighting takes a lot of time, which adds a lot of latency.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): pipenv tensorflow version (use command below): 1.10.0) and 1.11.0) python version: 3.5.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory:
when i run the code, i get the following error: phofcode according to the documentation ( phofurl "the input pipeline generated by your input_fn is run on cpu."
> > ## errors > **fetch error** networkerror when attempting to fetch resource.
w0619 some requested devices in `tf.distribute.strategy` are not visible to tensorflow: w0619 estimator.py:1811] using temporary folder as model directory: w0619 `eval_strategy` is not passed in.
i define the custom ops like below: phofcode it seems like the custom ops not work ok in eager mode
4. if remove the `redraw`, the two popup windows are both closed.
so, crossshardoptimizer is failing badly.
calling tf.function from tf.py_function in dataset.map hangs the program.
the code should run successfully.
repl: phofurl (obviously there are more suitable/effective patterns that could be used to implement this trivial example, but it's intent is only to distill what i encountered in a more complicated scenario.)
however, after assertop invoked once, following function calls are not be called.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
just calling `import tensorflow` launched many warning messages.
this looks similar to phofurl
note: while the example code is python, this problem also exists once the graph is deserialised and appears to be language independent.
add any other context about the problem here.
iterating on a batched dataset made from tensor slices of ragged tensors produces `tf.raggedtensor` that seem to be improperly formatted.
- tensorflow 1.12.0 (from pypi) - python 3.6 - cuda/cudnn/gpu not used full traceback: keyerror traceback (most recent call last) in <module>() 1 ----> 2 clear_devices=true) in clear_devices, import_scope,
should be able to load regardless of the size of variable
if including tracebacks, please include the full traceback.
that the quoted text ` 's '` is highlighted as a string and the remainder of the line is highlighted correctly.
right now, it 's not, even when i use in the dataset 's `shuffle()` method.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
phofcode gives the error: phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
i expect it to not crash when running
python import tensorflow as tf from tensorflow import keras import numpy as np x = 1) y = x
17. form elements must have labels.
--> 620 raise runtimeerror( 'there was no new checkpoint after the training. ')
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - tensorflow installed from (source or binary): source - tensorflow version (use command below): python version: 3.6.6 bazel version (if compiling from source): 0.18.0 gcc/compiler version (if compiling from source): 8.2.1
tf version 1.12. it probably relates to other versions too.
that is, both the above processes are expected to run naturally: print the contant and string and exited.
0429 23940 deprecation.py:323] from > to_float (from is deprecated and will be removed in a future version.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):custom code - os platform and distribution (e.g., linux ubuntu x64 - mobile device (e.g.
on this example the effect is small but i 'm trying to train a transformer model with `tf.function` and it takes very long for the training to reach full speed.
there should be a way to statically unroll a loop when using `tf.function` for performance reasons.
as you can guess, it would be expected to run without failure.
when i try to use a layer as an activation function in a `dense` layer, i get an `attributeerror: 'tensor ' object has no attribute 'numpy '`.
detailed steps to reproduce the behavior: 1. change to a directory with subdirectories that have files with content to search through, but not too big because you don 't want to wait forever.
i also understand this error is connected to control flow operations (linked the batch normalization method creating operations like `switch` and `merge`).
throw a warning when silently rounding.
" below, {} are used to indicate a variable gvim.exe -i none -n -u none --noplugin -u none {file} # longer than two pages / or ? "
this is a problem because many plugins which know how to do the work they * expect * to be slow asynchronously, but still use `expand(":p")` a number of times synchronously to begin with, expecting it to be extremely fast.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i tested on (the direct parent of this patch).
applying to a dataset that just contains one tensor (in contrast to a tupled pair of tensors) leads to an incorrect error (from the example below): valueerror: tensor 's shape (2,) is not compatible with supplied shape (none, 2) the supplied shape seems to be incorrectly derived.
when using tf.keras models and layers to build a simple lstm and train on the penn treebank dataset a long list of errors are thrown of the form: w op_requires failed at variable_ops.cc:104 : already exists: resource however, when just importing keras and building the same lstm, no errors occur.
2. turn on the wysiwyg editor for that new `text` field.
but once encounter, it 's >90% keep be there until reopening gvim the only thing contains the related text is mswin.vim and its dependency.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
below you can find the minimum code needed to reproduce the error.
the values for `loss` and `mean_squared_error` should agree and both use the masking.
minimal example: python class mymodel(tf.keras.model): def __init__(self,
i get an exception when computing the union of two string sets and trying to convert the output to a dense tensor (`typeerror: cannot convert 0 to eagertensor of dtype string`).
i was hoping for a more explicit error message.
cpu gives the following (using random colours): phofimage gpu delegate gives the following (using random colours): phofimage basically, all pixels here are (wrongly) identified to be of same class (at index 0) because scores for class 0 are always highest.
when creating tf.keras.model with functional api, dilated convolution of tf.keras.layers can 't estimate the output shape.
i think this should be the expected behavior: phofcode
specifically, when i process first image takes 1400ms, second image 1150ms, third image 900ms... at 7th our 8th image it levels off at 350ms per processing.
phofcode the issue occurs the second time around in the loop when the `exp_buff` variable is rewritten with `iter(dataset)`.
i would like to be able to run symbolic tensors through a while loop without disabling eager execution (basically, i would like eager execution not to take away practical functionalities which are very useful in designing models without having to put up small hacks of the framework, which are bound decrease readability and stability).
the `worker_fn` will be used if an "evaluator" task exists cluster.
but as the model is very small it has to run perfectly.
- have i written custom code : yes - os platform and distribution : linux ubuntu 18.04.2 lts - tensorflow installed from : binary - tensorflow version : 1.12.0 python version: 2.7.15 cuda/cudnn version: 10.1/7 gpu model and memory: geforce gtx 1080ti 11gb i am trying the tf estimator api for monitoring validation metrics during training.
1. go to '...' 2. click on '....' 3. scroll down to '....' 4. see error
duplicateflagerror: the flag 'log_dir' is defined twice.
- vim version : - os: centos 7 - terminal: gnome terminal
problem arises once header field is reset to empty.
the above error does not occur in tf 1.12, perhaps due to `datasetv2` changes with `optimizedataset`.
a cross-device loop must have a pivot predicate: while/while_context`
warning: the tensorflow contrib module will not be included in tensorflow 2.0. for more information, please see: * phofurl * phofurl if you depend on functionality not listed there, please file an issue.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux opensuse leap 42.3 - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):y - os platform and distribution (e.g., linux ubuntu 16.04):linux - mobile device (e.g.
similar value when running same op multiple times.
the model after compilation failed to generate the prediction.
the file prime.c below (but any c file will work): 3. begin visually selecting wildly 4. you should see colors occasionally jumping with wrong syntax highlighting.
large logs and files should be attached.
cannot create a `savedmodel` with a `densefeatures` layer, i get an exception: `attributeerror: 'str ' object has no attribute 'shape '` (see full stacktrace below).
if including tracebacks, please include the full traceback.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): pip install tensorflow==1.13.1 tensorflow version (use command below): 1.13.1 python version: 3.6.4 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory:
621 622 if runtimeerror: there was no new checkpoint after training.
`vi test.py` phofcode 3. move cusor to last line 4. press `<f5>` 5. you will find cusor move to first line automatically
import tensorflow as tf from tensorflow.keras import layers from time import time class sampling(layers.layer): # uses (z_mean, z_log_var) sample z, vector encoding a digit.
failednode133 is my outputnode sigmiod/softmax ,i had tried ,both are failed
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - tensorflow installed from (source or binary): pip package tensorflow==2.0.0-beta1 - tensorflow version (use command below): 2.0.0-beta1 python version: 3.7.3 cuda/cudnn version: gpu model and memory: titan xp 11gb
also, it prints out duplicate logging message with my own formatter and something internally defined format(i guess) as below.
then, file `b` doesn 't save its changes.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os 10.13.6 - tensorflow installed from (source or binary): from pip i think, i don't remember and i don't know the difference - tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6.7
phofcode the snippet is taken from here: phofurl
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04):linux ubuntu 16.04 - mobile device (e.g.
here is a colab demonstrating the issue.
then, the output of `tf.nn.conv2d` is different when batch_sizes are 64 and 128.
1. write this in `/tmp/vimrc`: let a =<< trim end x end echo a 2. start vim like this: $ vim -nu /tmp/vimrc the command raises the error `e990`, because ` x` has been joined with `end` (the backslash is removed in the process): e990: missing end marker 'endx' --- here is a different issue which also involves a line beginning with backslash.
to quote comments from the method: phofcode the problem is that if `inputlayer` was added manually with a specified `input_shape`, then it is an error not to serialize it -- because then the following layers do not know what input shape is.
what i noticed is that the model only converges when using the keras fit function.
again, it makes no sense.
large logs and files should be attached.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): conda gpu version tensorflow version (use command below): 1.10 python version: 3.5 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: 10 gpu model and memory: quadro gp100 16gb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
... steps to reproduce the behavior: 1. click on 'authorize ' 2. enter value 3. press 'authorize ' 4. refresh the page 5. click 'authorize ' again
random output should be different on each call in both graph and eager modes.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): osx 10.13.1 - mobile device (e.g.
tf.estimator using mirrored strategy takes forever to start training.
i use tensorflow tutorial 's code and just restore weight on different session.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux 16.04 - mobile device (e.g.
i then tried to run some code that i was working on before (and which used to run fine) and got the following error: using tensorflow backend.
this is because of an allocator stats check introduced between releases 1.13.1 and 1.14 in the gpu common runtime function phofhyperlink .
no errors, and a correctly saved model :-)
phofcode is there an issue with the latest gpu build?
numpy returns something meaningless in this case, so is not a very good role model!
- vim version - os: ubuntu 18.10 - terminal: gui, roxterm
the problem arose when upgrading from tensorflow 1.8 to 1.11 and from the error it seems to be caused by a when isolating the issue to reproduce it, it seems that `max_pool`, `dataset` and `squeeze` are involved to raise the error.
test.py: python import tensorflow as tf do_eval_dist = true def model_fn(features, labels, mode): layer = tf.layers.dense(2) logits = layer(features) if mode == predictions = {"logits": logits} return predictions=predictions) loss labels=labels, [-1])) if mode == class_targets axis=-1, [-1]) preds axis=-1, name= 'preds '), [-1]) return loss=loss, eval_metric_ops={ 'cohen_k ': preds, 2), }) if mode == train_op return loss=loss, train_op=train_op) def input_fn(): features labels 0.
memory leak when model has flatten or bn layer.
{cmd}` to work with the following shell configuration: phofcode i 've also the tried the following and more: phofcode
### approach 2 - convert tflite also, tried following: ` tflite_convert --input_arrays=input --output_arrays=` but, i am not sure what last layer is, as is fairly convoluted.
now the behavior changes in 1.14.0, these variables are a single tensor (not a list any more) if there is one single element.
it was working with tf version in conclusion transformer demo not working anymore, there any way to get installed in colab version?
w op_requires failed at conv_ops.cc:470 : resource exhausted: e0206 2719 classifier.go:148] failed classify face: t [[{{node hint: if you want see list of allocated tensors happens, add runoptions current allocation info.
npm install strapi@beta -g strapi new my-project --quickstart
i want to predict actions for various input images in reinforcement learning
i want to make the if statement in my code work.
and i wrote a muli-gpu version to do the stuff.
i expect to see calculated shape with none only in first dimension
hi, how can i access validation data within a custom callback ?
1. run this shell command: $ vim -nu none +"set co=80|setl fml=0 fdm=marker" {{{1 ', 'x {{{1 \']" + 'norm!
there should be no error, and ops should be assigned to their appropriate devices (cpu vs gpu).
printed node names as well.
the metric should be calculated successfully in both distributed and non-distributed evaluations.
... steps to reproduce the behavior: 1. launch swagger ui 2. perform an api invocation 3. scroll down to api response 4. notice that the first line of the response is having the line "can \'t parse json.
and the tf.identity should just return a scalar float.
large logs and files should be attached.
2. click [get] /test bar.
2. set langmap `:set langmap=jkljkl;hjklhjkl` 3. start terminal `:terminal` 4. from the bottom window you can use `<c-w>l` to switch to the terminal 5. if you now try to use `<c-w>k` move down it will actually try move up instead of down only if you press `<c-w>j` will you move down.
so only way to ensure that data will be reshuffled at each epoch is to use then `model.fit(dataset, steps_per_epoch=..., epochs=n_epochs)`.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): pip nightly tensorflow version (use command below): python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution: linux ubuntu 16.04) - mobile device if the issue happens on mobile device: lg v30 android 8.0.0 - tensorflow installed from (source or binary): source tensorflow version (use command below): 0.0.0-gpu-experimental python version: 3.6.5
keras duplicates shared variables in the variable list: phofcode the first print: phofcode the second prints: phofcode
additionally all images works fine without tensorflow for example: used scipy read image and try print shapes of all images which no channels are 3 4. the image which i used is one images form waterloo exploratory dataset.
lines 1235, 1326, 1459, 2535 of nn_ops.py includes the following error check: if isinstance(output_shape, (list, np.ndarray)): however, if output_shape is a tuple instead of a list, then the error check is skipped.
1. write a script similar to the one written above, which basically creates an estimator warm-started from the latest checkpoint in your training job, calls the input function to get the actual labels, then tries to compare that to prediction output from an estimator.
reverting to tf 1.13.1 with eager execution enabled shows the same results.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux centos 7.3 - mobile device (e.g.
1. run `vim --clean -c "set 2.
phofcode i have find a bug when use exponentialmovingaverage with codes work fine without using a distribution strategy.but when using a distribution strategy (such as mirroredstrategy), an error is reported: phofcode by debugging i can see that it works fine when ema.apply() is executed on the first gpu.but when executed on the second gpu, it will report an error.this is because has been created building the model in _model_fn().so on the second gpu, tf tries to create variable it doesn 't exist at all, 'replica ' should appear at end of variable name.
if i increase the weighting by a certain factor, the overall loss of the model should increase by the same factor.
often red highlights will appear.
however, i got much different result.
if including tracebacks, please include the full traceback.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
ideally, saving this would work just fine and not crash on the type annotation
i will provide a small snippet of code to reproduce the error, if it is not well known
following are code changes for this: phofcode and custom_op_cpp() function is defined as follows: phofcode custom c++ op is defined as: phofcode this works as expected, see same gradients for each run (1 iteration), since i 'm loading same dataset each time.
i want to run the object detection app on gpu by using gpu experimental with a self-converted ssd model.
i would expect the output of `:echo` to have been preserved even after focusing a different program window.
i should be able to pass a variable size image to the model.
- vim version - os: [windows 10] - terminal: gui phofurl the issue is in `s:netrwmarkfiles` function where `fnameescape` function is used.
1. run `vim --clean popup.vim -c "source popup.vim"` phofcode 2. run `vim --clean popup.vim -c "source popup.vim"` phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow):na - os platform and distribution (e.g., linux ubuntu 16.04): centos 7.5 - mobile device (e.g.
no error occurs and training continues.
due to certain circumstances i must use the c api.
1. add content data 2. delete content data with "recycle" button.
... steps to reproduce the behavior: 1. go to phofurl 2. paste in the swagger/openapi definition yaml above.
as per the comment in the function will use the underlying logits for better numerical stability if the last op is a softmax: phofurl but since keras adds an identity op to output of every layer, last node of a layer will always be `identity` so condition in above if statement will never fail for keras models and underlying logits are never used: phofurl using non-keras-layers like `tf.nn.softmax` will work, however.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - tensorflow installed from (source or binary): from official tensorflow gpu-powered docker - tensorflow version (use command below): 1.13.1 python version: 3.5.2, 2.7 cuda/cudnn version: 10.1 gpu model and memory: nvidia geforce 2080 ti and nvidia tesla t4
the values of non-zero elements in sparse weight matrices with fixed indices should be trainable.
reproducible prediction from same input instance, independent of row number or input length.
- i have written custom code in microsoft visual studio pro 2017 - c# (using tensorflowsharp v1.11 wrapper classes).
if including tracebacks, please include the full traceback.
this makes it impossible to use in a sequential model.
savedmodel should produce same outputs as the original model.
i use vim config below to map <f5> to run python, python2 is ok, but cursor always go first line after running python3.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): mac os x mojave - mobile device (e.g.
3. click the "try it out" button.
the same model definition has been reused but it has been used to first define a static graph and then train model: it works perfectly.
i would hope that the above 4 issues would be pointed out by the upgrade script as needing my attention, even if it cannot (for some reason, although i imagine the one on line 63 can be done with a string replacement) fix them automatically.
memory leak is very serious
keras should match up targets with the correct output when casting, according to the loss dictionary defined in `compile`.
if including tracebacks, please include the full traceback.
i would expect both version to be equally fast, or the newer version to be faster.
showing the file name in the title was added in commit 90f3e7a.
(gdb) info register r15 r15 (gdb) x/2 0xee2c0bc0 x/2 0x7fffee2c0bc0 ... the 0x0 that ended up r8.
- i also tried running algo with sudo, got same error - tried troubleshooting phofhyperlink , though it didn 't seem relevant, got same error task [common : check the system]
when i put the below into my vimrc , py3 import os;import it show below error .
- os platform and distribution (e.g., linux ubuntu 16.04): osx mojave 10.14.1 - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no (used script provided in tensorflow) - os platform and distribution (e.g., linux ubuntu 16.04): google colab - tensorflow installed from (source or binary): binary - tensorflow version (use command below): r1.12 python version: 3.6
provide a reproducible test case that is the bare minimum necessary to generate the problem.
(no new ops are being added in graph and this was made sure by using finalize() method of graph within session.)
the implementation of the tab-local `'cmdheight'` values seems to be incomplete with regards to the offered vimscript api functions: * `:help 'cmdheight'` should clearly signify the scope as _tab-local_; the _global (but ...)_ is confusing.
when using the @tf.function decorator a much greater accuracy is achieved for the same model type, number of epochs, number of batches, and dataset compared to a training run not using @tf.function.
dataset should iterate as it does in eager mode without `tf.function` decorator.
in eager mode, training a linearregressor estimator calling `model.train(..)` fails with the following error: phofcode
`wincol()`, or custom function); simple expression (`42`, `g:var`) works * if `l:var` is put in parentheses `(l:var)`, it works _(that 's my preferred workaround for now)_; putting entire right-hand side parentheses `(wincol() || l:var)` does _not_ help * if (superfluous) `l:` sigil is dropped, it works * if `l:var` has different scope (`g:var`, `s:var`), it works so it looks like rather obscure problem parser.
is using function compositions using `functools` package an issue in tf2.0 ?
i don 't have this issue on tpu if the tf.data.dataset is not built from tfrecords
its not accurate at all?
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: nexus 5 emulator, api version 23 - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: python 2.7.15rc1 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: n/a
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): pip binary tensorflow version (use command below): 1.10.0 python version: 3.6 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: na gpu model and memory:
(note changes to local addressing bash user@user:~$ netstat -nr kernel ip routing table destination gateway genmask flags mss window irtt iface 0.0.0.0 0.0.0.0 ug 0 0 0 enp0s31f6 0.0.0.0 u 0 enp0s31f6 0.0.0.0 u enp0s31f6 user@user:~$ sudo systemctl start wg-quick@wg0 user@user:~$ netstat -nr kernel ip routing table destination gateway genmask flags mss window irtt iface ug enp0s31f6 u wg0 u ``
provide a reproducible test case that is the bare minimum necessary to generate the problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)"
as its inference was pretty slow on cpu, i checked using tensorflow 's profiler advise and found out the main time consumption was from batch normalization(took 140 ms in total).
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.0 / 7.4.1 gpu model and memory: nvidia titan rtx you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
the error looks like a collision/off-by-one error of some sort.
if maintainers agree with my diagnosis, i'll happily provide a pull request fix it.
python seed is 37 seed is none assertionerror traceback (most recent call last) in <module>() 15 seed_assert(none) 16 ---> 17 ds.map(seed_assert) 8 frames in seed_assert(elt) 4 seed = 5 print("seed is {}".format(seed)) ----> 6 assert seed not none, "random seed not set.
use the tf_gpu_allocator environment variable to select the cuda malloc allocator: `$ export then, in a python shell, try to use the gpu: $ python >>> import tensorflow as tf >>> ``
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04):linux ubuntu 16.04 - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): osx and linux - mobile device (e.g.
'--n_gpu ', type=int, help= 'number gpus used for training model. '
there should be red border to indicate error on `test` input.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
however it seems that when applying those changes (within the autocmd), multiple `listener_add` created callbacks are made for the same event.
any code that attempts to compile a keras model produces this error.
sentencepiece phofhyperlink is a library that provides extremely fast and efficient utilities for text encoding during training.
model saved as tf.keras .hdf5 converted to tensorflow model using and when using freeze_graph to get a single .pb file running into error
now i want to integrate a simple demo of tflite (similar to benchmark) my codes, which is built by cmake.
without the distributed strategy, i can readily pass a batch size of 300 to a given gpu, within the distributed strategy, this drops to 200 on 2 gpus, and 70 on 4 gpus.
the second issue probably has something in common with the first, so i 'm posting code to reproduce first one and logs.
- os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - dataflow runtime - tensorflow installed from: binary - tensorflow version: 1.13.1 tensorflow transform: 0.12.0 python version: 2.7.x detailed logs: captured from stackdriver dataflow logs.
calling by itself does * not * result in the exception.
phofcode provide a reproducible test case that is the bare minimum necessary to generate the problem.
i have a 1d convolution layer whose weights are using weight normalization (salimans & kingma, 2016).
phofcode when passing model_dir with a s3 path, it failed with exception entitytoolarge.
tensorboard do the record normally
1. download `zalgo.txt` phofhyperlink 2. run `vim --clean --cmd "set spell" zalgo.txt` 3. vim crashes with:
my dotfiles : phofurl phofhyperlink one help file to reproduce the bug : phofurl phofhyperlink i also post the bug in vim-airline git repository, here : phofurl phofhyperlink sorry for my horrible english.
iphone 8, pixel 2, samsung galaxy) if issue happens on mobile device: ipad mini 2 real device and iphone 6s simulator - tensorflow installed from (source or binary): source tensorflow version (use command below): latest commit is from march 22. python version: n/a for this problem bazel version (if compiling from source): 0.23.2 gcc/compiler version (if compiling from source): $ gcc --version configured with: apple llvm version 10.0.0 target: cuda/cudnn version: n/a gpu model and memory: n/a if it would help to provide my xcode project, i'm happy to provide it.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary, pip3 install tensorflow-gpu tensorflow version (use command below): 1.13.1 python version: 3.6 bazel version (if compiling from source): no gcc/compiler version (if compiling from source): no cuda/cudnn version: gpu model and memory: gtx 1060m, 6gb are weights empty before training?
( in my case, coming from flow_from_directory), steps_per_epoch is overridden by len(train_generator) in this file training_generator.py phofhyperlink this doesn 't happen in keras.
check that you see no error (in your console or top of document.
should you find it reasonable i 've extended the solution to full refactoring of the mkl primitive factories
1. environment is `development`, database is `sqlite 3` 2. enable email confirmation 3. register and open confirmation url in email 4. call login api 5. got `your account email is not confirmed`
when changing enum to id 2, sending a request for parameter id 2
provide a reproducible test case that is the bare minimum necessary to generate the problem.
internalerror: cudnn launch failure : input shape provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes.
/pet/{petid} 2. click "try it out" button 3. specify pet id 4. click on "execute" button 4. see error - pet id was removed, petid field is red.
the function should work in the same way as without the decorator, but it doesn 't.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
phofcode any help will be appreciated!
1. create a new project 2.
if including tracebacks, please include the full traceback.
basic testing/benchmarking script: python from __future__ import absolute_import from __future__ import division from __future__ import print_function import numpy as np tensorflow as tf def get_data(dense_shape, mean_edges=100, seed=123): """generate random sparse matrix data."""
i expected the `training` argument to be set to `true` (or `1`) automatically when i called the `fit()` method.
`tf_upgrade_v2` fails if the file contains f-strings
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - tensorflow installed from (source or binary): source - tensorflow version (use command below): master (1/31/19) python version: 3.5.2 bazel version (if compiling from source): 0.19.2 gcc/compiler version (if compiling from source): c++ (ubuntu 5.4.0 cuda/cudnn version: cuda gpu model and memory: quadro rtx 6000, 24gib
additionally, the diff between old file and new file showed no changes!
as of some recent patch, it is opening a new one.
it 's as if `cno <plug>(up) <up>` mapping had been ignored.
however, input `dtype` cannot be specified in this way and when you pass `tf.int32` on input, using an `inputlayer` is required - in functional api `tf.keras.layers.input` _is_ serialized model (and, funnily, as a `inputlayer`).
running the single-operation model given below using the metal gpu delegate gives a different output than when executing on cpu or with the full tensorflow interpreter.
1. install strapi using --quickstart defaults 2. create a new content type with a string field that has the "unique" config setting set 3. create an item with "test" in the unique field from 2 4. repeat step 2 with "test" in the unqiue form field 4. both items are added
tflite archive for mobilenet v1 has been downloaded from link from article, mentioned above phofhyperlink code: phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 64bit - mobile device (e.g.
this is the offending code.
`tf.nn.conv2d_transpose` with the same inputs is expected to produce the same outputs on different calls on gpu.
when a is fixed to a python scalar (say pi/3), rotation works fine (cf.
able to correctly link to the framework.
this would be the best solution for me but i 'm not sure how convert into a singe depthwise_conv_2d v2(dilation=2).
factorizing a small 4x4 (dense) matrix with walsmatrixfactorizaion results in the following error: > invalidargumenterror (see above for traceback): input matrix is not invertible.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): w op_requires failed at spacetobatch_op.cc:219 : invalid argument: padded_shape[0]=741 is not divisible by block_shape[0]=2 traceback (most recent call last): file line 3296, in run_code exec(code_obj, self.user_global_ns, self.user_ns) file line 1, in <module> file line 197, in runfile global_vars, local_vars) # execute the script file line 18, execfile ", file, 'exec '), glob, loc) 22, <module> res = conv(test_in_2) 712, __call__ outputs = self.call(inputs, *args,
also do not necessarily see any networking issues.
this phofhyperlink script with a small preprocessing change: phofcode i also removed the printing of predictions, as the script is built for a classifier rather than a detector.
patches are pending for these issues (#21676) but they are not yet merged.
try 'bce ' or \'mse \'") kl_divergence(x, x_pred): self.c += (1/1440) # todo use correct scalar self.c min(self.c, 35) todo make variable kl -0.5 tf.reduce_mean(1 + z_logvar - z_mu
also note that such an `something` dictionary entirely realistic if what you do nlp you need carry your vocabulary around.
- have i written custom code: yes - os platform and distribution: ubuntu 16.04 - tensorflow installed: binary - tensorflow version: python version: 3.6.6 phofcode (if this api is not ready yet, please ignore this issue.
1.why the input shape and output shape is different from the model retrained using object detection api ?
provide a reproducible test case that is the bare minimum necessary to generate the problem.
but here comes the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
in eager mode, when a function is compiled with `xla.compile`, any constant arguments are auto-cast to tensors, but these tensors don 't appear to reflect the fact that they are constant.
after the model has been loaded from a checkpoint, the following messages appear: phofcode
by * unexpected * i mean the math result is wrong, on my machine at least.
though the author there was using tensorflow 1 and reported that data input as numpy arrays did not cause problems, so this seemed different enough for its own issue.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes (custom code) - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
please update sample code accordingly (may pass in allow_pickle=true explicitly to np.load).
when i move to a batch size of 16, the size is larger than the int32 index can handle.
followed all the steps in the guide, haven 't previously created a vpn, after choosing 1 (digital ocean), it skipped crucial next steps
global_step)` may not be recording summaries unless `global_step % n == 0`, however, it is running the code within its block anyways.
both the "predict" signature, and the "serving_default" signature definitions should be in the saved_model.
i 'm running the example code here: phofurl this runs successfully but if i try to make a prediction with the model by running phofcode i get the following error: phofcode
steps to reproduce the behavior: 1. set use_existing_eip to "true" 2. deploy to aws ec2 3. run through all prompts as normal.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i'd expect that assignment to a local variable doesn't change the ability to compute gradients!
see the code below for an example.
then do `git clean -ndfx`.
pr to fix this issue: #27661 thanks!
could provide pb file a small inference script if needed.
since tf.keras using tf as backend operation, weights updates and gradient calculation should be identical.
- it still try to download dataset online, while my network connection is restricted.
after successfully training and exporting the trained wide and deep model from here: phofurl tried to freeze the exported model using freeze_graph.py.
code for chief: phofcode code for worker: phofcode ip_address1 is always the address of the chief system and ip_address2 is the adddress of worker, and it is swapped when swapping windows and unix systems rules.
when using phofcode with phofcode during forwarding validation data it still makes use of random which changes the state for the next train operation and so the training procedure is not deterministic.
i tested 3 situations: 1) create two variables on gpu and fetch them in one `sess.run` call: memory increases by 1057mib is not released after closing session resetting graph) 2) create two variables on gpu fetch them one by one with `sess.run` call: increases by 528mib is not released after closing session resetting graph) 3) create two variables on pu fetch them: isnt increased released after closing session.
: it should work without the requirement of external push
tf2.0 documentation phofhyperlink if i run the code using only python py_file.py then it is running fine.
this is different from tf 1.13.1 where it works as expected: phofcode missing spec causes some problems, e.g.
one of the debug tips from `:help modifyotherkeys` is to enter insert mode, press <kbd>ctrl+v</kbd> and then the key sequence that's problematic.
i do not understand it but there are instructions for updating given in warning, see code below.
(for me, i'm running in a qemu virtual machine, and i see this 100% usage both inside the vm from `vim`, and on the host from `qemu`.)
if including tracebacks, please include the full traceback.
subsequent calls to `tensorarray.concat` returns a fully unknown shape.
`adam`) in `model.compile(...)`, everything works as expected.
alternatively, it can be found in this txt file: run_dummy.txt phofhyperlink phofcode
18. name as 'close '.
- the value of a string argument is accessible in the scenario described above.
the model is implemented as an estimator.
vim versions prior to patch work fine.
i also did same benchmark with tflite, inference time of quantized models is much lower as i expect.
in the doc, read() shoud return the contents of the file as string
i expect that the latest post training quantization should remain as fast as the one in version 1.10.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
properly-shaped randomly-generated values, since learning performance is not at stake here) and uses exclusively layers taken from `tensorflow.keras` (no custom bits), whose code present below.
`tf.cast` should not change the bit representation of values.
i checked it with 'diff ' command between generated files.
if i removed 'tf.function' decorator, code worked fine with a warning: > warning:tensorflow:using mirroredstrategy eagerly has significant overhead currently.
the gradients should be calculated well regardless of how many times we call the layer within a phofcode .
when i set partitioner to none, the training process quickly converged to an expected test auc 0.87. however, when i either used fixed_size_partitioner or with modified min_slice_size, the test auc stayed below 0.64. different learning rates and batch sizes didn 't help.
i intentionally use the training data to evaluate the model, # !!!
the dilated convolution can estimate the output shape as convolution op does.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): i have custom code to build an image classifier network with the tf.keras api - os platform and distribution (e.g., linux ubuntu 16.04):linux ubuntu 18.04 (linux mint) - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.10 - mobile device (e.g.
unexpected and undocumented runtime exception/error when handling malformed data.
using @tf.function and not using @tf.function) for the same model, number of batches and epochs on the same data.
i am also not mixing and matching keras and tf.keras layers.
2-4x slower relative to running inference without mkl
if it 's working as expected, then i think the documentation really needs to be expanded, with detailed examples and clear guidelines.
run this code, ``a_s`` and ``b_s`` are set to always multiply to 1000. i see performance going from 0.9 seconds up to 45 seconds on this test, even for the same values.
patch diff --git a/src/ex_cmds.c b/src/ex_cmds.c index --- a/src/ex_cmds.c +++ b/src/ex_cmds.c @@ -5527,6 +5527,7 @@ find_help_tags( t t t "s/ 1", "s/ 2", "s/ 3", "s/ 9"}; static char *(expr_table[]) = {"!=?
i guess it's something related to implementation of `@tf.function`, probably `tf.function` creates some temp file and updates them.
in this particular example, i was trying to use the `infeedenqueuetuple` op (with the inputs being placeholders that i'm feeding).
a code below works well on python 3.6 with tf 1.12. on python 3.7 with tf 1.13, however, the code crashes.
large logs and files should be attached.
25/100 3:45 26/100 3:42 27/100 3:38 28/100 3:35 29/100 3:32 30/100 3:29 31/100 3:26 32/100 3:24 33/100 3:21 34/100 3:18 35/100 3:15 36/100 3:13 37/100 3:10 38/100 3:08 39/100 3:05 40/100 3:03 41/100 3:00 42/100 2:57 43/100 2:55 44/100 2:52 45/100 2:49 46/100 2:46 47/100 2:43 48/100 2:40 49/100 2:38 50/100 2:35 51/100 2:32 52/100 2:29 53/100 2:26 54/100 2:23 85.66mib.
in git bash mintty (windows 10), `exitval` is -1 as in linux.
... steps to reproduce the behavior: 1. scroll down to 'schemas ' 2. click on 'person ' to expand it 3. scroll up to the top of the page 4. see error
instead the following error is emitted: phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary):binary tensorflow version (use command below):1.13.1 python version:3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version:10.0 gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
the `worker_fn` will be used if an "evaluator" task exists cluster.
the output of the `tf.sigmoid` function seems abnormal when the input has nans.
* the graph construction above sets the expected shapes of the placeholders as polymorphic `[none]`.
but why do still get ssl error?
in particular, the toco converter relocates the mul operation from the bn layer to before the reshape layer, at which point layers do not have compatible dimensions.
detailed steps to reproduce the behavior: 1. run `vim --clean` (or `gvim --clean`, etc.)
(service: amazonec2; status code: 400; error code: unsupported;}`.
it is worth noting that if i do call model before first training loop, gradients are ok. next loop they become none.
reproducing this exactly may require running through sagemaker.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 / colab - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
program exits with the following error message: w op_requires failed at retval_op.cc:70 : internal: error evaluating input 0 as a compile-time constant.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): from manjaro repositories tensorflow version (use command below): 1.11 python version: 3.6/3.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: / 7.3.0-1 gpu model and memory: 1080ti 11gb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)"
> warning: logging before flag parsing goes to stderr.
i am not sure will it work further, however at the moment i've the following error on cli just after running the `pip -q install -r requirements.txt` command: phofcode
phofcode calling `run()` after setting `num_layers` to 1 or 2 works, but fails if `num_layers >= 3`.
or i can see an error while
personally i think cache should just be removed as it won't save significant amount of computation in any sane code and has unbounded memory requirements in broken code, but if there are good reasons retain it we can just key on values of tuple components, rather than their object ids.
i am running a tpu allocated by `ctpu up` in tensorflow 2.0 (i 'm aware this isn 't fully supported atm).
it should load the model correctly even it the restore from the savedmodel is performed inside a distribution strategy context.
the error `e486: pattern not found: <plug>(up)` is raised, the cursor has jumped over the second occurrence of `test`, and the search register has been populated with `<plug>(up)`.
error that gives us no information what is actually wrong: retval[0] does not have value
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): `binary` tensorflow version (use command below): 1.12.0` python version: `3.6.7` bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: `none` gpu model and memory: `none` event file from filewriter: [log.zip]( phofurl
import conv_utils 9, .. backend as k 89, .tensorflow_backend * 5, tensorflow as tf 24, tensorflow.python pywrap_tensorflow # pylint: disable=unused-import 52, * 15, node_def_pb2 15, attr_value_pb2 15, tensor_pb2 15, resource_handle_pb2 22, serialized_pb=_b( ' tensorflow "r x01( tbn typeerror: __init__() got an unexpected keyword argument 'serialized_options '
the code can be find in the following notebook: phofurl phofcode `model = baseline_model()` phofcode `estimator_train_model = phofcode
which is what happens when i run this similar shell command: $ vim -nu none + 'sil ", 3)|setl list|bo vnew|b x ' x
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
after reading source code for `reshape.cc` it seems like even if batch size is indicated as -1, during conversion batch size explicitly computed and stored in model, which main cause of this bug.
long phofimage confirmed on multiple: vim - vi improved 8.0 (2016 sep 12, compiled feb 22 2019 included patches: 1-503, macvim custom version 8.1.950 (155) nvim v0.3.4 - macos 10.14.5 - macos terminal version 2.9.5 (421.2)
when i try to import `tf.keras` or `tf.python.keras`, i get `attributeerror: module 'tensorflow' has no attribute 'keras'`.
5. initial request works fine.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.6.8 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: here is the output i get (i added comments marked with `# <=`): metagraphdef with tag-set: 'serve ' contains the following signaturedefs: the given savedmodel signaturedef contains the following input(s): the given savedmodel signaturedef contains following output(s): tensor_info: dtype: dt_invalid # <= what is this?
`:badd` doesn't even load a buffer, it just adds file name to buffer list.
... steps to reproduce the behavior: 1. go to the swagger editor 2. copy/paste the yaml definition 3. select the `--` choice in dropdown for `type` parameter.
<img width="602" alt="screen shot at 17 13 30" src=" phofurl
vim freezes in the insert mode when i hit an enter.
643 if 644 raise is not supported when eager execution " 645 "is enabled.
i 've tried several solutions which are mentioned lower.
logs with errors for both 1.14 and 1.13 below.
3. press `<c-w>_` to maximize the upper window.
but i 'm sure there must be a way stop leaking logging macros.
- vim version: - os: [5.1.7-arch1-1-arch
when verbose=0, it should be silent, but it prints out multiple lines during validation.
but when running tasks, i encountered the following error: caused by: internal error: failed to apply delegate: next operations are not supported by gpu delegate: mean: operation is not supported.
i want to use the tensorboard callback in keras to show the embeddings in the projector view
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - tensorflow installed from (source or binary): binary (pip) - tensorflow version (use command below): 1.12.0 python version: python 3.6.4 :: anaconda, inc. include any logs or source code that would be helpful to diagnose the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.10.0 python version: 3.6.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 9.0/7 gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
i0705 querying tensorflow master (grpc://a.b.c.d:8470) for tpu system metadata.
if including tracebacks, please include the full traceback.
if including tracebacks, please include full traceback.
after further investigation setting the phofhyperlink to either 0 or 1 does not help.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
in meantime, i suggest adding a step to whatever process generates & commits these files that validates results, to avoid this recurring again (cf #4313).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): docker image running under host on ubuntu 16.04 - mobile device (e.g.
only way i was able to test was by also creating the python bindings for the op
current behavior on google cloud using `deep learning image: tensorflow 1.14.0 m33` is non deterministic.
` internal error: failed to apply delegate: warning: op code #38 cannot be handled by this delegate.
creating a boolean constant prints a deprecation warning: > w0828 deprecation.py:323] from _eagertensorbase.cpu (from is deprecated and will be removed in a future version.
it doesn 't appear to be training at all.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - tensorflow installed from (source or binary): - tensorflow version (use command below): phofcode - python version: 3.7.1 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: geforce rtx 2080 ti i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 avx512f fma i cpu frequency: hz i xla service executing computa tions on platform host.
model inference time is the same for gpu and cpu (~5500 ms)
detailed steps to reproduce the behavior: 1. run `gvim --clean` 2. create a second line with the text `ab*cd` 3. position the cursor on the first text 4. set `nomagic` via command `:set nomagic` 5. set `@/` register to search text `ab*cd` via command `let @/="ab*cd"` 6. press `n`
info:tensorflow:loss = step = 0 info:tensorflow:saving checkpoints for 1 into tmp/model.ckpt.
code should run without error
models created will be collected by gc, and memory usage go down to initial level
it should work, but doesn't.
when moving from wifi to lte either by disabling wifi on the iphone (xs ios 12.2) or moving out of wifi range, the vpn tunnel shows connected, but will not pass any traffic.
phofcode output phofcode indeed, i think only variable scope will add a prefix to the variable name, but not name scope.
this does not occur every run, in-fact it might occur 1/10 and only after 1-5 minutes running.
i am using tensoirflow object detection api in c++ , trained model with version 1.8.0, now i am running it on a laptop with no gpu and in microsoft visual studio 2017. the readtensorfromimagefile function takes alot of time and i would like it if it can work faster.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
if including tracebacks, please include the full traceback.
run each cell from beginning of notebook so you load data and create generators before you get to example under heading 1.6. then try to run example.
following is what i am getting from the given code: phofcode
in summary, i created a custom model in model subclassing style, which contained only 1 custom layer.
after step 2, the letter `c` should be in the buffer and insert mode should still be active.
tensorflow allocates more memory than specified.
7. observe that with no change to code but using beta0 instead of alpha0 validation accuracy goes down from > 95% to < 90%, with beta1 even to < 80%
this forces me to name `input` layer same name as key of dictionary given as input.
base_data_path = #load the previously saved h5 model # try to reload the saved h5 file # recreate the exact same model, including its weights and optimizer model_file = 'my_model.h5 ') model = # show model architecture model.summary() #create directory to save savedmodel saved_model_dir = !mkdir -#p saved_model_dir ` provide a reproducible test case that is bare minimum necessary to generate problem.
given a batched ragged tensor `x`, manually building a new one using `tf.raggedtensor.
`strapi build` then `strapi start` 6. in the strapi dashboard roles & permissions > facebook enter the client id, secret and redirect url to front-end app, in this case with create react app is ` phofurl 7. configure facebook app so that ` phofurl is specified as a valid oauth redirect url.
if input shape is specified when models are created, they pile up in memory, without being destroyed.
the local variable `packed` should have a value set.
base profile phofimage regression profile phofimage for reference this profile was captured during training on mobilenet but we have also observed this on other convolutional architectures.
i split the train set images into 5 shards, and in the .config file, i changed `min_dimension` and `max_dimension` to 300 and 512, respectively, in hopes of faster training.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):
inventoryitem and type to become individual definitions
i 'd like cusor keeping it 's space after running
if including tracebacks, please include the full traceback.
i checked a sha256 checksum of the data in memory, and they differ from the checksum of the written data on s3.
all future ec2 modules then must use the temporary session_token.
the relations is then (of course) lost between the entities.
no error: should return the warped image
when i press i in the list of files, the file details appear, the list stays in place, in the same order
4. the ui will hang on 'loading... ' image.
according to the supported ops phofhyperlink , the following operators could / should have been somewhat simplified into trtengineops: `const`, `mul`, `conv2d`, `fusedbatchnorm`, `maxpool`, `identity`, `biasadd`, `concatv2`, `reshape`, `sigmoid`, `exp`, `add`, and `sub`.
* writing to tensorboard (*console parameter = false*) train on 60000 samples epoch 1/5 i profiler session started.
i implemented a spectral normalization regularizer that can be passed as the [kernel_regularizer] argument in the [tf.layers] module.
when using `tf.function`, the cpu memory usage increase forever.
model-broken.tflite.zip phofhyperlink this basic model fails to run on gpu (with the error specified above), but will run fine on cpu.
run this shell command: vim -nu none --cmd 'au bufwinenter,winenter * setl stl=active ' --cmd 'au bufwinleave,winleave * setl stl=not active ' -o /tmp/file{1..2} the status line in the right window displays `active`.
i've trained a model using keras (in google colab), then i've converted the keras h5 file to tflite using `tfliteconverter` (also in google colab) by using this code: phofcode after that, i've run the tflite model (in google colab also): phofcode the tflite model can be found here phofhyperlink however, when i run the tflite model in an android app (using the same input data) y get different outputs: (this code is based on this example phofhyperlink from tensorflow) phofcode the output values i get in android code are not same as outputs obtained with python code.
i 'm trying to process pixel images real-time on a low-powered pc.
when calling `tf.make_ndarray()` on a tensor (`<class dtype `uint8`, shape `(?, 1000, 1500)`), i get the following error: `file line 563, in makendarray shape = [d.size for d in tensor.tensor_shape.dim] attributeerror: 'tensor ' object has no attribute 'tensor_shape '` i call the function inside a function given to `tf.dataset.map()`, which is not executing eagerly.
3. then try to view the value of random key, and application exited without any hanging nor warning.
recover from swap file failed on windows ?
internal error: failed to apply delegate: op_context.input->type == ktfliteuint8 || op_context.input->type == ktfliteint8 was not true.node number 1 (dequantize) failed to prepare.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.12 python version: using c++ bazel version (if compiling from source): bazel release 0.17.2 gcc/compiler version (if compiling from source): gcc (ubuntu 7.3.0 cuda/cudnn version: not using gpu gpu model and memory: not using gpu include any logs or source code that would be helpful to diagnose the problem.
i have test code here: phofurl `python benchmark_model.py --dim_length 128 --bz 8` will work `python benchmark_model.py --dim_length 128 --bz 16` will give error
--config=mkl # build mkl support.
you will find that loss and val_loss are terribly wrong.
there are a different blog post and resources on how to perform backpropagation for batch norm.
sorry, and patches welcome (that 's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
i am having difficulty creating this new layer to add to the model, as it seems that fundamentally, i don 't have the ability iterate over the batch size, which is unknown until runtime.
unfortunately, after adding some new data, i ran into the 2 gb graph memory limitation and was forced to switch to using `tfrecord` and `tfrecorddataset`.
when calling a tf function multiple times with the same arguments, the function keeps getting traced and more and more new concrete functions get generated.
you only need try `pickle.dump` the concrete function.
instructions for updating: colocations handled automatically by placer.
tensor memory allocation fails for the same batch size (determined by n_acton_samples) as i scale the number of gpus in my strategy.
large logs and files should be attached.
a workflow that uses python files will not encounter this issue since all the gpu memory is released automatically once python interpreter finishes.
i tried to build fully quantized autoencoder model using tf2.0 beta and keras on colaboratory phofhyperlink but when i run tfliteconverter.convert method, the jupyter kernel crashes and the session is restarted.
the status line in the right window displays `not active`, because: - for the window to exist, vim had to focus it at least temporarily - it is not focused anymore, therefore it was left - when it was left, `winleave` was triggered - when `winleave` was triggered, the autocmd `au bufwinleave,winleave * setl stl=not active` run when the autocmd run, value of ` 'statusline '` option c local to window c set to `not active`
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): google colab - tensorflow installed from (source or binary): pip - tensorflow version (use command below): 2.0.0b python version: 3.6.7 gpu model and memory: tesla t4 this will raise an error that only sequential or functional models can be saved phofcode
this came from a tf2.0 port of the pytorch dynamic graph example here phofhyperlink .
i can if deemed necessary.
request is sent with user-agent equal to specified parameter
i have already test with ubu14+ tensorflow1.4, or ubu16 with tensorflow1.2/1.4, both of them are failed, the faild log as below, can you give me some help.
however, what i find weird, and why i suspect it might be a tensorflow issue, is that it only occurs when input dataset size is large enough.
i get runtime of around 290 ms for the unconverted model, while having runtimes of about 420 ms for the quantized tflite, the optimized and quantized tflite and the plain tflite model.
`output_names` is not restored and outputs are given new auto-generated names: phofcode model fails to load when using dictionaries for losses: py import tensorflow as tf i = x = tf.keras.layers.dense(1, name='my-output')(i) m = tf.keras.model(i, x) 'mse'}) assert m.output_names[0] == 'my-output' m.save('my-saved-model') m2 = # valueerror: unknown entries in loss dictionary: ['my-output'].
large logs and files should be attached.
steps to reproduce the behavior: 1.
i am getting the following error: input to reshape is a tensor with 24576 values, but the requested shape has 1536
graph mode in tf2.0 should be at least approximately as fast as graph mode in tf 1.x.
sqrt(2) should be computed the same way every time.
instead as many steps are performed as needed for a full epoch.
the model should be loaded correctly and then run the inference.
here 's the main part of the code: python def model_fn(features, labels, mode, params): # model layers # ... rmse = predictions) if mode == predictions = { 'new_states ': predictions } return predictions=predictions) loss = predictions) metrics = { 'rmse ': rmse} if mode == return loss=loss, eval_metric_ops=metrics) optimizer train_op optimizer.minimize(loss, return loss=loss, train_op=train_op) ``
tflite converted models seems to always run on cpu despite using gpudelegate, for example the simplest model from phofurl runs for hundreds of milliseconds per inference no matter whether cpu or gpudelegate is used, while stock mobilenet_v1 (no quantization, no manual tflite conversion, downloaded tflite model) runs ~80ms per inference on this device when using gpudelegate and on cpu.
- vim version: - os: arch linux - terminal: gnome terminal vim - vi improved 8.1 (2018 may 18, compiled apr 26 2019 included patches: 1-1186 compiled by arch linux huge version with gtk3 gui.
t [[{{node t w unknown: failed get convolution algorithm.
tf.placeholder and tf.sparse.placeholder have the same behavior.
def body(loopvar): y = x * w loss = tf.reduce_mean((y - y_)
for a low loss of 0.03, the true positives should be close to 50k.
it should import fine, i've copied code directly from the official tensorflow keras getting started guides.
(aside: im surprised not to be able to find time function that yields millisecond or microsecond precision.)
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the model is compiled and the correct model output is produced with model.summary().
during real training it happens quite fast.
i have reduced it a minimal example that simply feeds model zeros below.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
if i pause for more than approx 350ms, then time it takes process next (with *same session* being reused whole time), jumps back up 1400ms.
at the beginning of the program, the memory is only 190mb.
when called, this function creates a concrete function to call for the specific types and shapes of the arguments, by calling and subsequently, it reuses the same concrete function when it encounters the same types and shapes again.
import tensorflow as tf import numpy as np from scipy import misc import glob os #command to extract all file names num_parallel_calls=4 def parse_fn(filename): """decode the jpeg image from the filename and convert to 0, 1]."""
`y` and `weights` entry may be none.
i expect `return variable` to return a reference, not a copy of the variable.
above one batch is defined the entire dataset.
- os platform and distribution: linux version 4.14.79+ - tensorflow installed from (source or binary): the platform is provided by google - tensorflow version: 1.14.0-rc1 (also tested on my own machine with tf 1.13.1) - python version: 3.6.8
i don't have full code and data to share since my model and data are proprietary.
when `tf.reduce_mean` is called on the output of a `tensorarray.concat` which had a fully dynamic shape (including rank), it produces a `tensor` of scalar shape, but whose value is a 1-element vector.
the directory tree: phofcode `test.sh` has one function, `tags` is generated with `ctags`.
install the latest tensorflow with `pip install tensorflow` into a conda environment running python 3.7 with cuda 10.1 installed.
output has correct values without gpu delegate, but contains only zeros (0.0f) when using gpu delegate
either target a non-quantized output format, or change the input to contain min/max information, or --default_ranges_min= and --default_ranges_max= if you do not care about accuracy of results.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
when i tried to use `model.summary()`, the output shapes were printed as
1 has in that issue.
building a new tensorrt engine trtengineop_0 batch size 224 w op_requires failed lookup_table_op.cc:809 : failed precondition: table initialized.
when run `converter.convert()`, the following error will be raised: phofcode when adding relu op after the bn, it will be converted without the error massage.
the problem is that this speed increase only increases if i push images to tensorflow in rapid succession.
find the reason is that the function will be traced twice after add line `df = the print will run twice.
from __future__ import absolute_import, division, print_function, unicode_literals import tensorflow as tf import os import time numpy as np glob matplotlib as mpl mpl.use( 'agg ') matplotlib.pyplot as plt pil imageio # from ipython display (train_images, _), (test_images, _) = train_images = 28, 28, 1).astype( 'float32 ') test_images = 28, 28, 1).astype( 'float32 ') # normalizing the images to the range of [0., 1.]
sometimes, the local value of a window-local option is not copied from the last window where the buffer was displayed.
different seeds to produce different performance/parameters, specially if the dataset is small or if the optimizer only takes a step for a small batch size.
i n future, it will be treated as `np.float64 == np.dtype(float).type`.
but i dont know why this bug appears again beta version.
this is somehow related to fact that this `tf.nn.conv2d` call wrapped in a `tf.data.dataset.map` call.
according to the documentation phofhyperlink it should be possible to pass a "callable that takes no arguments and returns the actual value to use" as learning_rate to and this "can be useful for changing these values across different invocations of optimizer functions".
here with a softmax: phofcode
i tried to create a minimal example, so i 'm not even training the model here before evaluating some data.
the `saved_model_main_op` collection and it 's related op should be preserved after conversion.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): windows 10, also tested on ubuntu 18.04 on same machine - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): arch linux - mobile device (e.g.
have tried restarting the kernel multiple times.
large logs and files should be attached.
variables copied using should have the same shape of original variables in order to make possible restoring from a checkpoint.
if including tracebacks, please include the full traceback.
expect fit() time to remain constant even with multiple re-compiles
to keep the reproducible example small, i decreased the size of dataset and batch size, which shows trend of increasing memory.
i believe it 's only supposed to be in big and huge builds.
phofcode the code causing this is located at: phofurl
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.12.0 python version: 2.7 bazel version (if compiling from source): 0.21.0 gcc/compiler version (if compiling from source): 4.8.5 cuda/cudnn version: 9.0/7 gpu model and memory: titan xp 12gb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
run `vim --clean tmp.vim -c "source tmp.vim"` phofcode
theoretically (x @ y) and (y.t @ x.t).t should evaluate to same value and their value should be same as numpy version.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.12.6 - mobile device (e.g.
trying to convert a sparsetensor of type string into the corresponding dense tensor using tf.sparse.to_dense throughs an exception: typeerror: expected string passed to parameter 'default_value' of op 'sparsetodense', got 0 of type 'int' instead.
... steps to reproduce the behavior: 1. go to ` phofurl 2. paste in the above yaml file.
please use the new featurecolumn apis instead.
the model.call() processes the inputs using the standardize_input_data() method to ensure the input data is as expected.
otherwise, document should provide ordering of executing all hooks.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
tf.function does not change the behavior of any operations on tensors.
i am attempting to build a custom tensorflow layer to perform k-means clustering across channels of a given image.
if a tf.keras.models.model uses customized loss added to the model by model.add_loss(loss), and a tf.data.dataset dataset which yields tuples (x, y, sample_weight) is used in model.fit(dataset), a typeerror will be raised.
i am generating a dataset using `from_dataset` that involves copying and decrypting data at read time, which takes some time to process.
no cpu idle time in prediction.
the ec2 instances have ubuntu 18.04 and either 1 or 4 tesla v100 gpus.
tf.saved_model.load cannot load compiled keras model, saved as savedmodel in graph mode.
we have tried appending some nodes on top of deeplab gpu converted model.
resnet50 inference accuracy should look something close to (top1, top5) = (0.7315, 0.9109).
note: valgrind is not actually necessary, but it occurs more frequently with valgrind, possibly due to the slowdown it causes.
however, as said in those are "inlined routines, since these are performance critical" thus cannot be moved a `.cc` file.
what follows is a description of the sequence of events.
conversion was attempted with: phofcode
but it seems from the error that it is related to graphs.
this will download and prepare training data, which takes a long time.
phofcode this issue is critical for us.
our `tf.data` pipeline looks roughly like: phofcode tuning parameters after upgrading doesn't recover any performance, nor does using
10. proper name be provided for group.
large logs and files should be attached.
a 200 with no response body should yield some sort of warning, because the schema should at least be `{}` a 404 from the server should yield a big red error somewhere.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
if including tracebacks, please include the full traceback.
i would expect both, the model trained with keras fit and the model trained with a custom training loop, to converge.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
the full code can be found here: phofurl currently, the code in repository works fine because the new interpreter() and interpreter.run() are called in the same thread.
what may be the problem with mirrored strategy?
i get an error when running the code on tpu because xla doesn 't support candidate samplers.
it would be nice if the object detection example code worked without having to remove my existing tensorflow install.
<img width="661" alt="screenshot at 00 34 11" src=" phofurl - vim version phofcode - os: macos 10.15.1 - terminal: iterm2
when using reshape layer of keras and -1 is used to infer the shape, the output shape is incorrect.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
start from vertical column 4.
7 edit newly created user.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary / pip tensorflow version (use command below): 1.12 python version: 3.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: either there is a problem with tf.load_op_library in tf 1.12 contrary to tf 1.8, or there is a mistake in my process to generate the import library from the pyd file (the process is normally used for dll, not pyd).
provide a reproducible test case that is the bare minimum necessary to generate the problem.
12 gb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" the `wr`s (verbs work request) that trigger this error has lkey set to 0 and non-zero buffer length, indicating that it 's trying to send unregistered memory region
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 x64 - mobile device (e.g.
as described in tf.slice phofhyperlink , i tried to use `tf.tensor.getitem` to perform a slice in a more pythonic way, but ran into memory leaks.
phofcode results infinite flood: phofcode
large logs and files should be attached.
], dtype=float32), 6] i suppose this is a bug in the caching process involved if the code is run on a gpu.
when i use tensorflow in cpu mode, such error message doesn 't occur.
this issue can be solved by rewriting `u = -u;` to `u = 0 - u;` as follows.
this would make keras a lot easier to use together with `tf.data` because it gets rid of the need for defining a exact number of steps.
warning:tensorflow:from (from is deprecated and will be removed in a future version.
try to provide a reproducible test case that is the bare minimum necessary to generate problem.
the model can be loaded correctly.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
certs to be installed in correct location.
5.0.1] add any other context about the problem here.
it returns : [[[nan nan] [nan nan] [nan nan] [nan nan]]]
i would expect to see the date that is provided in openapi definition.
only one process can profile for a distributed job?
my code is designed to determine gpus info and their indexes before import tensorflow by using nvidia-smi but tensorflow device indexes mismatch nvidia-smi indexes.
however, after refreshing the connection.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): not for the original issue, then i tried some workarounds - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.13.6 - tensorflow installed from (source or binary): pip install tensorflow - tensorflow version (use command below): 1.12.0, 1.13.1 and python version: 3.6.7 we are using cpu here.
should use quantized version of model?
- the mnist images are well classified.
the behavior of `tf.nn.relu` when fed with `nan`-valued inputs is inconsistent: - if the input is a constant tensor, `relu` returns `nan`.
more complicated examples like phofurl fail with the same error message
it crashes in values.py, line 1220, phofcode so what's happening here appears to be that tf tries to divide the int32 tensor by a float with no cast and blows up.
training finished, prediction works fine.
phofurl download files and edit model location in run.py
if including tracebacks, please include the full traceback.
i think it started with the update from fedora 29 to 30. but it seems they use the same vim version.
.tflite model converted with post-training quantization works well on cpu.
on the "images" tab in tensorboard i don 't see all bounding boxes which are in the tfrecord file.
i am therefore in the process of changing all keras imports in my projects to tensorflow.keras imports.
i would expect a minor release (10.0 to 10.1) which has been out for almost 2 years to work correctly.
the below code (using zero l2 regularization) fails with phofcode
the calculation order of `json_encode()` should be faster than o(n2).
it also doesn 't matter whether sign is actually visible - this still happens with `set signcolumn=no`.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
indices1 = tf.constant([[4], [3], [1], [7]]) updates1 = tf.constant([9, 10, 11, 12]) indices2 = tf.constant([[4], [3], [1], [7], [7]]) updates2 = tf.constant([9, 10, 11, 12, 12]) indices3 tf.constant([[4], [3], [1], [7], [7], [7]]) updates3 tf.constant([9, 10, 11, 12, 12, 12]) shape tf.constant([8]) scatter1 tf.scatter_nd(indices1, updates1, shape) scatter2 tf.scatter_nd(indices2, updates2, shape) scatter3 tf.scatter_nd(indices3, updates3, shape) print(scatter1) print(scatter2) print(scatter3)
when i run tf code on my rtx 2070 i have an issue about: e failed to synchronize the stop event: an illegal memory access was encountered e internal: error destroying cuda event in context an illegal memory access was encountered e internal: error destroying cuda event in context an illegal memory access was encountered f check failed: status == cudnn_status_success (7 vs. 0)failed to set cudnn stream.` and... sometimes that crach windows.
traceback (most recent call last): file "test.py", line 33, in <module> outputs, state = inputs, dtype=tf.float32) file line 324, new_func return func(*args,
when i run it from the nightly build, i'm able to get a model with 1 trt engine node.
if including tracebacks, please include full traceback.
... steps to reproduce the behavior: 1. go to editor.swagger.io 2. add a dictionary with examples to a component schema 3. look at render 4. see no examples in the schemas section.
phofcode on iterm2 none for now
provide a reproducible test case that is the bare minimum necessary to generate the problem.
steps to reproduce the behavior: 1. use image freebsd-11-2-x64-zfs for digitalocean provider in config.cfg 2. try deploying to do.
running tfbdg with a filter vs running without a filter produces the same code errors when dealing with catching outofrangeerrors.
when run on tf 1.12 , log.txt is created and the logging is being recorded correctly.
phofcode the code is similar to the following official examples: linear_regression.py phofhyperlink and linear_regression.py (eager) phofhyperlink .
upsampling using results in unnatural smearing of the right and bottom edges of the image.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu - tensorflow installed from (source or binary): source - tensorflow version (use command below): tested on 1.12 and 1.13 python version: 3.6 cuda/cudnn version: 9 gpu model and memory: verified on 1080ti and titan v include any logs or source code that would be helpful to diagnose the problem.
the first algo instance running ubuntu 19.04, and the second one running ubuntu 18.04. instead you end up with two instances running 19.04.
class def __init__(self): def begin(self): self.mask_var = [] ### collect the masked variables for v in tf.global_variables(): if v.name.find("mask:0") != -1: self.mask_var.append(v) self.check_mask = def end(self, session): # assign 0 to the variable # get [0, ..., 0] ### however, if restoring checkpoint, self.mask_var[0] is not [0, ..., 0]
- os platform and distribution (e.g., linux ubuntu 16.04): windows 10 and google colab - mobile device (e.g.
the issue is completely ameliorated when passing predict tensor produced from in this case no additional dataset operations appear be created by keras `predict` loop.
both ops should appear in the loaded module.
the code for tflite conversion is following: phofcode the code for optimizing for inference was following terminal call: `python -m --input alex_frozen.pb --output alex_frozen_optimized.pb --output_names=output` all my models, including plain, unconverted model is uploaded here testing: models.zip phofhyperlink
use the following command to compile (without running of `./configure`): phofcode use the following command for benchmarking: phofcode | tf version | 1-gpu | 2-gpu | 4-gpu 8-gpu |:-:|:-:|:-:|:-:|:-:|
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: hw p9, mi8, oneplus 5, vivox9, huawei honor v9.... - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 2.7/3.7(both tryed) you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
i can 't reproduce this locally, but can probably add some diagnostic lines for the travis ci run if someone could suggest the best way forward.
however, when we tried running the code, it returned a typeerror.
similarly, if mobilenetv2_ade20k_train phofhyperlink is used, two also give very different results: cpu gives following (using random colours): phofimage gpu delegate gives following (using random colours): phofimage here's tflite model phofhyperlink used.
wireguard android app does not connect.
file line 1497, in load_weights self.layers) file line 751, in layer, weight_values, original_keras_version, original_backend) file line 377, weights = 365, convert_nested_model 377, weights = 353, convert_nested_model 459, weights[0] = np.transpose(weights[0], (3, 2, 0, 1)) 598, transpose return _wrapfunc(a, 'transpose ', axes) 51, _wrapfunc return getattr(obj, method)(*args,
large logs and files should be attached.
colab link to produce an error.
trying to install algo on centos 7.x by following instructions for centos 6.x on phofurl so far the givens steps work also for centos 7.x except that python 2.7 comes by default with 7.x., so you do not have to software collections (scl) package.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
--> 430 executor.run_local() 431 return 432 run_local(self) 618 if not metrics: 619 # this is unexpected.
large logs and files should be attached.
phofurl 1. create a store 2. subscribe to it in component a 3. create an element with ``out:fade`` in component a 4. destroy component a and trigger a store update before the animation has completed result: component a never got destroyed.
- stock tensorflow example phofurl - os platform and distribution (e.g., linux ubuntu 16.04): windows 10, 1809 - tensorflow installed from (source or binary): pip install tensorflow-gpu - tensorflow version (use command below): python version: 3.6 cuda/cudnn version: 10.0 and cudnn64_7.dll gpu model and memory: nvidia 1080 attached is the full output of the label_wav.py command, including exception details, and a list of all modules loaded into the python process at the time of failure.
calling `model.predict()` with a `tf.data.dataset` as input is only possible when compiling the model first and also providing labels.
for gpu 0 the behavior is normal, but for gpu 1 the output becomes zero.
case, should define them a .py file.
mnist from `tensorflow_datasets` uses 21mb split into 10 files.
it accomplishes this by providing custom ops written in c++, instead of userland code.
an increase of about 40 percent.
this will train without problem in eager mode, but the addition of the `@tf.function` decorator causes error.
i should be able to provide a `tf.data.dataset` as input, that provides a single tensor or a list of tensors.
these figures are simply wrong (see below).
vim --clean -s <(cat << 'eof ' func!
therefore, calling `estimator.train()` in a loop will lead to unbounded memory growth, and eventually, an oom error.
large logs and files should be attached.
detailed steps to reproduce the behavior: 1. open a file with url in it.
* main.cc int main(int argc, char
if i push the file to /system/lib64 path in device it works
3. scroll down to 'results. '
`strapi new api.domain.com` 3. choose your installation type quickstart (recommended) 4. quickstart 4. see error
tflite to actually use gpu when running conv2d networks
play [ask user for the input]
the `{cmd}` is executed in the configured shell.
detailed steps to reproduce the behavior: run `vim --clean test.py` then phofcode the first 'added ' parameter is zero, despite lines being added.
the same code works fine when running with either python 3.7 (instead of python 3.6) or compiling with gcc 4.8 (instead of gcc 5).
the converted tflite model puts these two operations together into a fullyconnected op, and somehow its bias `matmul_6_bias` is incorrectly shaped as single-element vector.
this does not seem be an issue with ftrl optimizer itself, since have tried several of them, all showing significant skewing of predicted results towards one or other end, if label dataset not balanced.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): macos10.14.3 - mobile device (e.g.
computing gradients for some graphs involving `raggedtensor`s causes an error
phofcode i ran algo on my mac (10.14) using digital ocean & ubuntu 19.04. i was sure to install the dependencies & opted to retain the keys during initial run.
since `:source` doesn 't accept a range, i started using the following command to execute a selection: phofcode when a popup window was created through executing the content of a register, and the window is closed with the mouse, vim throws an `e342: out of memory!` error.
import tensorflow as tf import zipfile import filecmp normal_fd = open("normal.zip", "wb") normal2_fd = open("normal2.zip", "wb") open("gfile.zip", "w").close() # "touch" file so that it exists, issue #32090 gfile_fd = "w+b") # need +, issue #32122 for fd in (normal_fd, normal2_fd, gfile_fd): with zipfile.zipfile(file=fd, "w") as zipfd: with zipfd.open("test.txt", "w") as fid: fid.write("hello, world!
...which was originally created as op defined at: file "benchmark_model.py", line 108, in <module> file line 96, in define_model conv1 = file line 457, in __call__ output = self.call(inputs,
unfortunately, i 'm not able to consistently reproduce it, but it has happened many times, and on two different machines.
2. add new policies `isexecuted.js` to path phofcode 3. update phofcode 2. add new "book" in admin panel.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary):binary tensorflow version (use command below): 2.0.0-beta0 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.0/ 7.5 gpu model and memory: 11gb gtx 1080ti include any logs or source code that would be helpful to diagnose the problem.
the loss and accuracy of model trained by tf.gradienttape should be similar to the one trained by keras fit with the same settings
unexpected and undocumented runtime exception/error when handling malformed data.
... steps to reproduce the behavior: 1. configure swagger ui as specified above.
below error when deploying algo on vultr task [common : check the system]
when i load multiple inception v3 graphs into memory and afterwards unload all of them i get a memory leak.
the following previous layers were accessed without issue: [ 'input_9 ', 'conv2d_29 ', 'conv2d_30 ']` this issue does not occur in a tensorflow 1.x environment, only tf 2.0
i can reproduce this using `govim`; i haven 't managed to find a smaller repro.
results running on google colabs shown below (cpu only), similar performance difference on my local machine, on which i used the specified python version (windows 10, 64-bit, i5-7200u cpu) - phofurl 1.13.1 ... epoch 6/6 - 0s 238us/sample - loss: 2.0.0-alpha ... epoch 6/6 - 0s 542us/sample loss: 0.2409 as you can see, the (mse) loss is several orders of magnitudes worse for 2.0.0-alpha.
the code that is running can be found here: phofurl the commands that run this code are listed above
when analyzing a tensor in eager-execution mode in the "watches" of the pycharm debugger, the deprecated object tensor._shape is automatically evaluated.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): source tensorflow version (use command below): python version: 3.7 bazel version (if compiling from source): 0.21.0 gcc/compiler version (if compiling from source): cuda/cudnn version: 9.0 / 7 gpu model and memory: 8x gtx 1080 ti 12gb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
expected to see some improvement.
the import library is missing.
-> 2382 2383 2384 if y is not none: in names, shapes, check_batch_axis, exception_prefix) 321 'expected to see ' + str(len(names)) + ' array(s), ' 322 'but instead got the following list of ' + --> 323 str(len(data)) + arrays: str(data)[:200] '...') 324 elif len(names) > 1: 325 raise valueerror( valueerror: error when checking model input: list of numpy arrays that you are passing to your model is not size model expected.
a minimal code is like: phofcode
i 'm looking to run `gvim` via `xvfb` in a docker container to give extra coverage for phofurl however i get the error message: phofcode
until all layers support masking, it's not practical to only support masking via an upstream layer.
in tf 1.13.1, using a dataset api pipeline that parses a sequenceexample phofhyperlink with phofhyperlink exported as a savedmodel with `strip_default_attrs` set to `true` (which is the default behavior for estimators) fails with the following error: phofcode see the full stacktrace below.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - tensorflow installed from (source or binary): - tensorflow version (use command below): phofcode - python version: 3.7.1 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: not relevent gpu model and memory: not relevent include any logs or source code that would be helpful to diagnose the problem.
however there was significant time difference for average running time for 'depthwiseconv2d', when we benchmarked the two tflite models with 'tflite android benchmark tool'.
expected a "typeerror" or an empty list as a result.
hence, this measurement does not serve my purpose.
it should only summary one scalar.
i write a custom loss function (dice coefficient) with following code: phofcode when i run code like this: phofcode i got the following error python traceback (most recent call last): file line 84, in <module> epochs=50) file line 21, in train callbacks=callbacks) file line 643, in fit file line 681, in fit 294, model_iteration batch_outs = f(actual_inputs) 813, execution_function return [out.numpy() for out 416, __call__ self._initialize(args, kwds, 359, _initialize *args,
if including tracebacks, please include the full traceback.
phofurl seems to have "missing selinux" at the end of the log file and i had some hangs on that step too.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
wanted to do one full pass over data, so that should be: # samples / (steps_per_epoch * batch_size), which would be ~1,593 steps.
the net loads and runs fine if i do not attempt to apply the gpu delegate before the attempting to load it.
notice the human readable file sizes.
the code should just run and update the parameters per usual.
i have created two conda environments - one for tf1.14 and the other one for tf2.0 - so all hardware and drivers are absolutely the same.
[y/n]: n no rocm support enabled for tensorflow.
also execution time seems to increase quite a lot.
large logs files should be attached.
steps to reproduce the behavior: 1. run ./algo update-users behind strict firewall
i would expect c api for `tf_sessionrun` and similar to return an error and caller to have an option to handle the error in some way that does not involve crashing.
add any other context about the problem here.
if using normal `map` and `batch`, one get phofcode
when using @tf.function decorator with tf.gradienttape to compute updates on a model, i find that it takes 110 seconds per epoch.
`history = model.fit_generator( train_generator, steps_per_epoch=8, epochs=15, verbose=1)` if train_generator is a instance of sequence.
try the code yourself here: phofurl
i before removing unused ops: 221 operators, 366 arrays (0 quantized) before general graph transformations: 221 operators, 366 arrays (0 quantized) f check failed: start_array.data_type == arraydatatype::kint32 range op inputs must be int32.
running this works as expected.
probably something in quantization process is holding the data.
<pre> # -*- coding: utf-8 -*- # @author : lin lan (ryan.linlan@gmail.com) from __future__ import absolute_import from __future__ import division from __future__ import print_function import tensorflow as tf class foo(tf.keras.model): def __init__(self): super(foo, self).__init__() self.dense = self.embeddings = 5)), dtype=tf.float32) @tf.function def call(self, inputs): embeddings = tf.nn.embedding_lookup( self.embeddings, inputs) return self._inner(embeddings) # @tf.function def _inner(self, embeddings): batch = tf.shape(embeddings)[0] ta size=batch) for i in tf.range(batch): this :]) ta ta.write(i, this) return ta.stack() foo foo() res foo([0, 2, 4, 6, 8]) </pre>
when adding a custom op in c++, the shape function passes a null inferencecontext pointer.
1. create new content type 2. name the content type row
import tensorflow as tf import numpy as np class def __init__(self,
just to test after installation, i tried importing tensorflow in python, but got an unexpected error (see code section) .
i'm using the nightly docker tf image to perform tf-trt.
when using the function using autograph the following error occurs: `valueerror: ( 'input has undefined rank: ', tensorshape(none))`.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
this python script is a reduced reproduction: phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.13.1 python version: 3.7.1 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory:
from the keras documentation, it should work.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux centos 6.5 - tensorflow installed from (source or binary): anaconda3 - tensorflow version (use command below): 1.10.0 python version: 3.6.6
and it leads to this error w0617 tpu_strategy_util.py:56] tpu system %s has already been initialized.
if including tracebacks, please include the full traceback.
this way the validation dataset could be used without `.repeat()` and the evaluation would be performed on same set of examples.
* have i written custom code: yes * os platform and distribution: debian 9.9 * tensorflow installed from: pip * tensorflow version: 1.13.1 python version: 3.7.3 gpu model and memory: n/a - tested in cpu mode traceback: traceback (most recent call last): file line 35, in <module> file line 1188, in train_on_batch outputs = self.train_function(ins) # pylint: disable=not-callable file line 3076, in __call__ file line 1439, __call__ run_metadata_ptr) 528, __exit__ can not squeeze dim[0], expected a dimension of 1, got 32 t [[{{node ``
so i converted my model to .tflite as shown under "running on mobile with tensorflow lite" when i try to run it on my phone, the app just crashes.
3. if server needs to restart (unattended-reboot) or is rebooted, dnscrypt-proxy fails to resolve.
the expected behavior is that the `tf.graph()` instance used by an invocation of `train()` is garbage collected by the python gc after `train()` terminates.
when holding all values memory (as previously was case), filter rather fast irrelevant values are skipped unnoticeable.
then i tried the tensorflow custom estimator and reproduced the problem.
seamless conversion to tflite and mlmodel files.
the two following layers give different number of variable in `model.variables`.
notably, using abseil 's app.flags will not cause the exception to occur with the same source code.
code annotated with `@tf.function` behaves differently depending on whether functions are called and captured as variables, or called and passed directly to other functions.
mentioned test scenario should not hang.
but for adversarial training, i need to call model inside of loss function.
a value == `virtcol("$") - 1` i guess?
get davis dataset and put it in respective folders
is saved as padding is same.
operations like addition are commutative.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): 18.04 - mobile device (e.g.
however no error is raised, being unclear behavior - are new weights (in case of increased size) initialised and trained?
3. now expand `a` in models list 4. scroll top and you'll see 4 time error: `resolver error - cannot read property '0' of undefined`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): centos - mobile device (e.g.
the expected behavior is after toco, the .tflite still has one conv2d layer and one fully connected layer.
in the meantime i'm modifying the file involved in the training.
only modified the line 47 in interpreter.cc where i set usennapi(true); - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04.1 lts - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0.0-alpha0 python version: 3.7 the automatically generated saved_model signature relies on the name of outputs.
the speed fast and stable.
if including tracebacks, please include the full traceback.
at the end of the file it return none (end of iteration).
the send/recv tensors `tensors` gives proper answer on all workers.
steps to reproduce the behavior: 1.
workaround: get the actual scalar float in the ndarray if it is not a float phofurl phofcode
windows with normal buffer types are affected too it 's not only help windows.
wrong url to vultr config documentation when you try to launch a vultr server, also found the same incorrect url on the docs
i was expecting similar behavior for both and
this causes reported validation loss/metrics to be artificially high since we are training on samples that should have been withheld.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
validation samples valid_samples validation_generator$n # 800 validation fit to history fit_generator( train_generator, steps_per_epoch as.integer(train_samples / batch_size), epochs epochs, validation_data validation_generator, validation_steps as.integer(valid_samples / (batch_size/9)) )` nb.
import tensorflow.keras,code can not work.
color in popup window seems not right.
2. load tensorflow model into golang env.
(env) root@vpn:~/algo-master# ./algo play [ask user for the input]
if including tracebacks, please include the full traceback.
when looking at the log it appears that the function producing the `typeerror` is so, `sequencefeatures` does expect `sparsetensor`, only to convert it to dense `tensor`, and fails if given `tensor` that is already dense.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): anaconda on windows 10 - tensorflow installed from (source or binary): install by pip in anaconda environment - tensorflow version (use command below): tensorflow-gpu 2.0 alpha python version: 3.6.8 cuda/cudnn version: cuda toolkit 10.0; cudnn 7.5 gpu model and memory: gtx 980 ti 6gb
the model has parameters and model depth is 88.
toco converter is throwing an error when trying to convert resizenearestneighbour op.
i am trying to train a face detector by using the object detection api.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
the depthwise separable convolution, unlike the normal convolution when used with floating point 16 is slower than floating point 32, but more importantly when it is used with batch norm and an optmizer (sgd, adam, etc.
previously, performance has been okay: phofcode but starting from tf 1.11 and up (as of writing newest is 1.13.1), performance has been much worse: phofcode using `tf.train.profilerhook`, i have generated the following performance profile for tensorflow 1.13.1: profile phofimage .
here is my minimum snippet of code to reproduce this error.
from ._conv import register_converters as _register_converters` `>>> mnist = one_hot=true)` `warning: logging before flag parsing goes to stderr.
after kicked off training, noticed that gpu memory usage was about 95%.
if including tracebacks, please include the full traceback.
the output is like: phofcode
please goto training_loops.ipynb and open google colab from there.
ui sends the http request and display the results
requires information about why the above error is getting displayed for some of bmp images.
depthwiseconv2d needs the dilation_rate argument.
should not give any error
however, can 't see information when also add flag "--use_nnapi=true".
... 1. create a path with a parameter in it, such as `/api/collection/{id}`.
this allows for some of the layers to work but not all, so it might be a simple calculation i need to figure out what additional padding to use but i cant figure out what this is.
i am trying to parallel download some images.
better logging is a much more efficient way to root cause and resolve issues.
method 1: plain python + tf 2.0 phofcode _throws_
shortly after pasting the api, the error appears, as opposed to install successfully, like so many times before.
this is probably because cudnn failed initialize, try looking see if a warning log message was printed above.
simple program wroks with cpu, but crashes when nnapi is turned on, with words: e/tflite: op code 67 is currently not delegated to nnapi e/tflite: returning error since tflite returned failure nnapi_delegate.cc:739.
a valueerror is generated cause a perreplica object cannot be converted to a tensor (see the log below).
the keras model is a slightly modified version of inception_v3 from the keras model library.
* converting both models to .pb format works just fine (identical performance in inference).
phofcode succeeds, but: phofcode fails with: make[2]: entering directory cd ..; old_po_file_input=yes old_po_file_output=yes xgettext --default-domain=vim --add-comments --keyword=_ --keyword=n_ --keyword=ngettext:1,2 *.c if_perl.xs gvimext/gvimext.cpp globals.h if_py_both.h vim.h po/gvim.desktop.in po/vim.desktop.in gcc -std=gnu99 -c -i.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): - mobile device (e.g.
i have to scroll waaay up to see the previous output.
i expect to be able to reuse the input bytebuffer with updated contents across multiple inference calls
this happens despite: - no oom errors on startup - using a bare-bones example cnn - trying with and without limiting the available memory with option allow_growth=false)` - restarting machine reinstalling latest nvidia drivers reinstalling cuda trying cuda 9 (tf 1.12.0, conda env) and cuda 10 (tf 1.13.0rc2, manual installation) i 'm beginning to suspect that this is hardware issue but gpu has never had any problems and can still run high intensity games for hours no problems.
instructions for updating: call initializer instance dtype argument instead of passing it constructor w0327 `eval_fn` not passed in.
<< endl; t treturn -1; t} tf_status* status_1 = tf_newstatus(); status_1); tif (tf_getcode(status_1) != tf_ok) { t tcout "load lib wrong" endl; t} tstatus = tif (!status.ok()) t{ t tcout "create graph wrong: " status.tostring() endl; t treturn -1; t}
if i change the weights then restore without reinitializing the model, it will properly restore.
* download the vim source code for at least version 8.1 patch 1502 (i 'm using 1524) * go to the `vim/src` directory * run `make distclean` * run `cflags= '-o2 -pipe ' ./configure --with-features=tiny` run `make -j` run `./vim --version` (or open vim and type :ver), and notice that at the bottom it shows `linking:` and `-lcanberra` appears.
1. open the `post test` tab.
i would expect it to work in both modes similar.
an exception should be thrown when the condition in phofcode is not met.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 14.04, os x 10.14 - mobile device (e.g.
detailed steps to reproduce the behavior: 1. set terminal width to 17 (`:echo &columns` should be 17).
if applicable, copy/paste the text or add screenshots to help explain your problem.
however, it actually allocates ~52% memory as in the screenshot.
this can cause problems if the path is a file, as you would assume that the path is a directory after calling `mkdir()`.
the error `tensor objects are only iterable when eager execution is enabled` is confusing me, since i have tried enable eager execution and results are same.
if including tracebacks, please include the full traceback.
large logs and files should be attached.
depending on the batch size, during training there is eventually (after 1000 or so steps, sometimes a lot more steps with smaller batches) a cuda-related crash, with one of a number of error messages.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): archlinux - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12 python version: 3.6 cuda/cudnn version: cuda 9.0, cudnn 7.1 gpu model and memory: nvidia 1080ti
import tensorflow as tf tf.random.set_seed(0) inputs = dtype= 'int32 ') x = output_dim=3, mask_zero=true)(inputs) outputs = tf.keras.layers.lstm(2, model = tf.keras.model(inputs, outputs) padded_inputs tf.constant([[1,3,0,0]]) it prints the following: tf.tensor( [0.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): 2.0.0-alpha0 python version: 3.7.2 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: not relevant gpu model and memory: not relevant
phofcode and it cost 200s for every 100 steps ,the same time as adamoptimizer
this did not appear make any difference.
i can modify the other samples in batch to influence output of x. this should not be case as all samples should be processed independently.
they were missing from loss dictionary passed to `compile`), this may match outputs to wrong targets.
pip install intel-tensorflow tar xvzf testcase_2367.tar.gz # attached cd testcase_2367 python testcase_2367.py
for example, the script below: phofcode produces errors as follows: phofcode both ktfliteuint8 and ktfliteint8 version of the pack operator is already implemented in tflite (see pack.cc phofhyperlink ), so it should be straightforward to support pack as well in the tflite converter
i have about 2 gb total of tags files, and so tags scanning takes upwards of 5 seconds.
i just modify some codes in lingvo to use tf-debug.
please consider using the `keras.utils.sequence` class.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code: no, i used tensorflow 's object detection api 's officially released model `ssd-mobilenet-v1-fpn`.
during eager execution, python lists are wrapped by tf 2.0 as `<class i see a weird issue with this data structure when using function composition to build a custom keras layer.
12. merge both link button discussed above in single element for benefits screen reader users.
import tensorflow as tf import os def cond(size, i): t return tf.less(i,size) def body(size, i): t b=2*7.5+c t with tf.variable_scope("a", reuse=tf.auto_reuse): t t a = t a = tf.scatter_update(a,i,b) with t t return (size, i+1) with tf.session() as sess: c=tf.constant(4.0) i = tf.constant(0) size = tf.constant(6) _,i tf.while_loop(cond, body, [size, i]) a init sess.run(init) print(sess.run([a,i]))
the code below phofcode produces a unmatched dataset in order.
realize that it might be more efficient to code convolution by adding a custom op but want to avoid that in prototyping stage as much as possible.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): one line modification to official colab checkpoints.ipynb phofhyperlink - tensorflow version: 2.0.0-alpha0
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 on docker (18.03) - tensorflow installed from (source or binary): pip - tensorflow version (use command below): 1.13.1 python version: 3.6 cuda/cudnn version: no gpu gpu model and memory: no gpu celery 4.2.2, keras 2.2.4
detailed steps to reproduce the behavior: 1. edit `filename.xml` 2.
it is actually good when you remove the reactive declaration.
but in some terminals (like xterm and st), it 's possible to restore them by setting ` 't_ti '` and ` 't_te '` appropriately.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: i'm not completely sure this is a bug/oversight, or if `tf.module` was never intended to be compatible with keras layers.
steps to reproduce the behavior: 1. follow steps on github page for setting up ec2 instance with algo 2. follow steps installed algo on localhost 3. execute algo
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - mobile device (e.g.
detailed steps to reproduce the behavior: phofcode
large logs and files should be attached.
it shouldn't allow to authorize an empty textbox
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04.4 lts on windows linux subsystem - mobile device (e.g.
phofcode so slow it results in 'redrawtime ' exceeded, syntax highlighting disabled
a keras custom callback typically has an access to the "model" object.
w op_requires failed at conv_ops.cc:470 : resource exhausted: oom when allocating tensor with type float on by allocator cpu e0206 2719 classifier.go:148] failed classify face: allocating tensor with type float on by allocator cpu t [[{{node hint: if you want see a list of allocated tensors happens, add runoptions for current allocation info.
maybe it is also my fault but do not see an error in memory management.
@dataset.py:45] instances loaded 100%|| @timer.py:48] load groundtruth boxes minival2014 finished, time:0.0225sec.
calculated input gradient shape: (32, 1024)` neither `tf.cond` or python's if statement work without decorator.
this behavior might be the result of the op being called on the gpu, as only cpu call is expected to throw error, as per comment here: phofurl setting cuda_visible_devices to be empty forces op to run on cpu and test passes (the invalid argument error is successfully thrown), but passing `use_gpu=false` as an option to session wrapper does not have this effect.
have localized it running load_model from keras, before reaching that point have imported tensorflow and verified that gpu is available.
info:tensorflow:saving checkpoints for 0 into tmp/model.ckpt.
- have i written custom code: <img width="311" alt="screen shot at 22 56 17" src=" phofurl
regarding the code snippet above, we need `inputs` to be a list rather than a single operand and `.addinput()` should be `.addinputlist()`.
i 've tried deleting the model the end of each iteration also using and allow_growth with no avail.
i 'm trying to reproduce the results from the tutorial example about "text classification with an rnn" provided by tensorflow at: phofurl phofhyperlink however, this warning message constantly appears that shows "skipping optimization due to error while loading function libraries: invalid argument: ... " i tried other optimizers, and lstm or gru architectures but nothing changes!
if line numbers are on, selection stops earlier still.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): google colab - mobile device (e.g.
if including tracebacks, please include the full traceback.
function has not returned from first call before second call made by vim.
when using tf.data.dataset.shuffle and iterating through the dataset multiple times the shuffled order is always the same.
the monitored session hangs in there fetching the `reduced_weight`.
a clear and concise description of what the bug is.
` the model here is still which have been proved working device.
i trained a fasterrcnn for point detection using tensorflow and its python api.
-> 2441 sample_weight = 2442 elif isinstance(x, iterator_ops.iterator): 2443 graph mode iterator.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no - os platform and distribution (e.g., linux ubuntu 16.04):windows 10 x64 - tensorflow installed from (source or binary):source - tensorflow version (use command below):1.14.0-rc1 python version:not used bazel version (if compiling from source):not used gcc/compiler version (if compiling from source):visual c++ 14 or higher.
edit - _refer to latest gist below_
this should be hosted at phofurl making the same call i see something like this - <img width="383" alt="screen shot at 1 48 06 pm" src=" phofurl
however, in training mode (where the dropouts are activated) gru and lstm implementation in tensorflow 2.0 seems to be re-using the same dropout masks, leading to deterministic behavior.
error: pack (stack_3) axis attribute is out of bounds: 1 w failed to run optimizer arithmeticoptimizer, stage strided_slice_3.
large logs and files should be attached.
1. type `alt + f11` to set mintty screen size to full 1. run `vim -nu none` 1. type `:call popup_create( 'foo ', { 'line ': &lines-5, 'col ': &columns-5, 'pos ': 'botright '})` 1. type `alt + f10` to set mintty screen size to default type `alt + f11` set mintty screen size full sigabrt occurs.
hang on "saving wireguard private keys"
all changes to reflect the correct line contents.
should return an `autocastvariable` variable instead `tf.variable` so that the `dtype` is preserved.
i expect to find some value for `dataset.prefetch()` that reads all or enough data to memory to allow for fast training without stalling.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - tensorflow installed from (source or binary): pip - tensorflow version (use command below): python version: 3 convertererror: toco failed.
ops.name_scope(name, "translate"): return transform( images, ``
keras should work as described in the api without calling numpy behind the scenes in graph mode and crashing
`cumsum` should be as accurate as the `float32` precision.
the kernel tensor for conv2d and conv2d_transpose layers (in contrib.layers) behave differently.
in fact, @fchollet said this is by design: phofurl but since i don 't want migrate full keras-based solution, how can i handle updates?
the end result is build fails (error code > 0) and strapi starts itself (but sqlite is used and postgres isn 't altered by strapi in any way).
official benchmark tool test and android integration test (benchmark version- 1.14, and android(tflite gpu delegate-nightly)
below is a minimum working example to reproduce the problem: phofcode
this is code copied over from the effective tensorflow 2 documentation on tensorflow.org.
finish conversion without any errors.
this might be me not understanding intended behavior of `map` function, but making it so that file reads can happen correctly in a map function would be useful in cases like this.
drwxr-xr-x@ 113 lukasgeiger staff 3.5k may 20 10:14 .. -rw-r--r-- 1 lukasgeiger staff 45m may 20 09:45 -rw-r--r-- 1 lukasgeiger staff 3.2m may 20 09:45 mnist.index ``
expected server to deploy with no errors and ability to install on clients with no issues.
--- this additional autocmd can be used as a workaround: au cmdwinleave * au cursormoved * ++once wincmd _ example: vim -nu none + 'au cmdwinleave au cursormoved ++once wincmd _ ' + 'au winenter wincmd _ ' + 'bo sp ' + 'set lines=10 ' " press `q:` :set lines=30 :q --- frequently encounter this issue when use tmux and close a pane changes geometry terminal from vim 's point view.
1. create a user model with overlapping properties with users-permission-user 2. install graphql plugin 3. start server 4. see error
additionally, when i add additional non-tensorflow code inside of my mapped function, that code only gets executed first time it is called.
phofcode in my code, the _input_min_range_ param is redundant.
2. type `hello vim world` into the buffer.
is there any other solution for this issue or i have to keep loading model in worker who uses it for prediction?
to not throw the error as was the case for tensorflow 1.8.
these key properties and methods should in dataset of tf2.0
instructions for updating: if using keras pass *_constraint arguments to layers.
expected result: "statuslinetermnc xxx term=reverse ctermfg=0 ctermbg=2 guifg=bg guibg=lightgreen"
now the concatenate function works properly when using a sequential model.
when trained during eager execution, using phofcode the model is trained without problems, and i expect using `model.fit_generator()` would allow this behavior too.
`vim.eval` shouldn 't change the value of dictionaries it evaluates.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): - mobile device (e.g.
maybe the input parse method distribute strategy is not fully tested.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
steps to reproduce the behavior: 1. go to console window 2. subscribe channel 3. receive very long message (6,000~) 4. can't drag text or scroll horizontal
bazel build:- phofcode tf-lite benchmark:- adb shell --num_threads=1 adb: no version information available (required by adb) starting!
why isn 't it working?
when using `tf.keras` with `eager` execution enabled, i don't have the control over learning rate of the optimizer i.e.
expand the tree node to see that the duplicate keys are shown
ubuntu 18.04 server vm running on proxmox on homelab all instances of my ip address have been replaced with (.env) algo git:(master) sudo ./algo [warning]: could not match supplied host pattern, ignoring: vpn-host play [localhost]
phofcode `:source` this script and `:call
below is some detail about distributed training: model : `neumf` dataset : `ml-1m` num_worker : `2` num_gpu_each_worker: `4` strategy : reduce_alg replace `pscpu/pscpu` with `nccl/xring` nccl version the distributed training run 120 steps then tensorflow quit when use default all reduce alg `pscpu/pscpu` of `mirrorstrategy`.
as above, my physical network is ipv4-only.
beta1 used to throw an exception in this case.
i am using win 10 and tf-nighlty.
vim stops scrolling the pum immediately.
output screenshot phofimage - vim version: > :version > vim - vi improved 8.1 (2018 may 18, compiled sep 3 2019 > parches incluidos: 1-1968 > compilado por arch linux versin "enorme" sin interfaz grica (gui).
i expect to be able to delete the iterator object within the tpu strategy scope.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - mobile device (e.g.
was on my latest xiaomi trying to edit phofurl as soon as i type a character the editor deletes it.
only layer instances should be tracked, not classes.
reducing the layer definition step by step i noticed that the error should be related to `tf.linalg.expm`.
example of correctly handling 32-bit integers: <pre> *buffer_size = size; *buffer_data = switch (type) { case ktfliteint32: { int32_t *p = * * &gt;(array->data())); for (size_t i 0; i &lt; size; i+=4, ++p) *p break; } </pre> 2. the flatbuffers buffer must be writable.
why is this so hard?
as it does without the `@tf.function` decorator.
import numpy import tensorflow as tf input = z = z = model = outputs=z) x fill_value=2.34, dtype=numpy.float32) y numpy.full((100,1), fill_value=1.23, dtype=numpy.float32) while true: model.fit(x=x, y=y)
when running without dilation im able to handle the variable sized input.
gcc/compiler version (if compiling from source): 4.8.2 cuda/cudnn version: no gpu model and memory: no you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
cuda-memcheck caught a bunch of these: ========= invalid __global__ read of size 4 ========= at in fusion_514 ========= by thread (959,0,0) in block (10,0,0) ========= address is out of bounds device frame:fusion_514 (fusion_514 : 0x880) saved host backtrace up to driver entry point at kernel launch time phofhyperlink
gpudelegate = new gpudelegate(); tflite = new
and with `verbose=1` that thing is so huge, it 's useless.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): debian gnu/linux 9.8 (stretch) (gnu/linux x86_64) - mobile device (e.g.
`toco --output_format=tflite --inference_type=float --std_dev_values=1 --mean_values=0` failed traceback (most recent call last): file line 11, in <module> sys.exit(main()) file line 320, in main app.run(main=run_main, argv=sys.argv[:1]) file line 125, in run _sys.exit(main(argv)) file line 316, in run_main 121, _convert_model output_data = converter.convert() 309, convert 225, toco_convert 107, toco_convert_protos (stdout, stderr)) runtimeerror: toco failed see console for info.
when i toggle capslock, the cursor is temporarily drawn inside the status line.
the iterator should cleanup incomplete cache files if finished or if the process exits.
please find the colab gist to reproduce the issue " phofurl phofhyperlink
the vim - try it at bash/cli -> nothing paste.
if the following does not immediately produces the bug, then try increasing the batch size from 100 to something greater.
the graph should freeze no matter the phofcode parameter.
add a few files to array.
* observe that "second" is now displayed in dropdown and example detail section.
the benchmark tests were carried out using the following tools and devices:- bazel version:build label: 0.23.1 tensorflow version: 1.13.0 android device: oneplus 3 however using tf-12.0 with bazel 0.16.0, the 'benchmark_model' built failed for 'official model'; but worked with replicated model.
large logs and files should be attached.
for reference see the image below <img width="1440" alt="screenshot at 6 34 05 pm" src=" phofurl > right: image of original pbtxt file of mobile-net architecture > left: image of pbtxt file obtained by using above script.
code should run with no error
i've changed the code little bit, mainly in the parts where i have to load training data.
however, the error also occurs when using eager execution (as tested by using a for-loop on the dataset instead of `.map()`).
when calling a custom op from a python function with tf.function, i got a segmentation fault.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
related to #28628. i am currently not sure how to reproduce a custom build with the same git_version as of nightly.
i don't have context know how this should be approached anyway.
though i am happy to provide more information and would be eager to get suggestions on how properly debug this problem.
according to the source of netrw (netrw.vim: 2220 phofhyperlink ), which eventually change the windows format path to unix-like form just before the execution of scp command.
the actual behavior is that update operations (of moving mean and variance) are not called when training model.
these issues in past seem somewhat relevant, but they all refer to much older source code for tensorflow, and this a change since r1.12.
obviously the expected behavior is for all the files to be saved.
loss: 'mae' metrics: ['mse','acc'] optimizer: kr.optimizers.adam
parser.add_argument( '--job-dir ', type=str, help= 'gcs location to write checkpoints and export models ' parser.add_argument( '--train_file ', type=str, help= 'gcs location of train data .tfrecord file location. '
the value estimate became an array instead of scalar float after 8000+ steps
2. the tf.keras fit_generator() path stands out as being different from rest.
operation expansion in less than 1 sec
on a range dataset, window followed by flat_map works as expected, but on a dataset of tuples (originally encountered using a csv dataset), flat_map fails, complaining that the input argument to the mapped function has multiple components.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04.2 lts (bionic beaver) - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): archlinux - tensorflow installed from (source or binary): binary - tensorflow version (use command below): python version: 3.6 cuda/cudnn version: 10 gpu model and memory: 1080 ti
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: none - tensorflow installed from (source or binary): source tensorflow version (use command below): python version: python 3.6.5 bazel version (if compiling from source): 0.16.1 gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: geforce gtx 1080 ti with 12gb memory `tf.nn.conv2d_transpose` with the same inputs produces the same outputs on different calls on
after having updated to 2019.3 i got a popup asking me for credentials.
vim gives up that undo file and continue.
the focus issue i'm opening today this part phofimage which should not exist in my opinion.
below is a code snippet.
- have i written custom code: yes - os platform: win10 - tensorflow installed: pip install - tensorflow version: and python version: 3.6
1. the script attached is used to train a model using multiple gpus.
by way, i'm not sure this os a specific bug in v1.13.0rc1 and v1.12 or it happens other releases as well since i was using colab.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): debian 10 buster fresh install (testing) - mobile device (e.g.
the segmentation fault originates from usage of `std::function` on the interface boundary, introduced in phofurl
it seems that providing the `contained` option when defining a new syntax with the `syntax` command, does not prevent the corresponding item from being highlighted at the top level as expected.
my iphone should disconnect from the wireguard vpn when i connect to my home wifi.
when using `@tf.function` on a function that involves abstract classes (e.g.
a function that correctly works in eager execution can't be decorated with `@tf.function` if declares a `tf.variable` in the function body.
likely something like this - <img width="774" alt="screen shot at 1 47 28 pm" src=" phofurl 2. download and setup swagger-ui locally following readme instructions for development.
on running the train method, it is throwing error despite initializing all variables before the optimize step.
2. the discrepancy between training with `tf.keras` and `tf.estimator` + `tf.keras` should be minimal or non-existent
i cannot reveal the code for proprietary reasons but i could furnish a similar version if need be.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
detailed steps to reproduce the behavior: 1. run `vim --clean` (or `gvim --clean`, etc.)
yesterday, i run this jupyter nootbook successfully.
i should see logging on ml engine that indicates validation accuracy for each epoch, like this: epoch 2/2000 1/200 - eta: 2:00 - loss: 5.7345 - acc: 2/200... when i set `steps_per_epoch` to be only 200, i * do * see validation accuracy logging as expected.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.12.0 python version: 2.7 bazel version (if compiling from source): 0.19.2- (@non-git) gcc/compiler version (if compiling from source): 4.8.5 cuda/cudnn version: no gpu model and memory: no you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
cuda libs i am very grateful for to everyone that working on this interface.
if i run the exact same model on a gpu, everything works as expected (eg.
i expect the same nice output as in previous versions, such as `input_1[0][0]`, `dense_3[0][0]` or `concatenate[0][0]`.
1. run `vim --clean popup.vim -c "source %"` phofcode 2. the popup window can be scrolled with `j/k`, `d/u`, `f/b` and `g/g`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): we are running a docker container with these benchmarks: phofurl - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - tensorflow installed from (source or binary): docker container: - tensorflow version (use command below): 11.13.1 python version: python 3 cuda/cudnn version: 9.1 gpu model and memory: k80 full output of a failed worker: phofurl
3. create a facebook login app 3. add the facebook redirect_uri configuration to environments > development > server.json 4. create a tunnel to localhost:1337 5.
... steps to reproduce the behavior: 1. go to editor.swagger.io 2. paste yaml
strategy successfully opened cuda library libcublas.so.10.0 locally creating cudasolver handles stream creating cudasolver handles stream info:tensorflow:loss step info:tensorflow:loss step (29.929 sec) info:tensorflow:saving checkpoints 3 into /tmp/model.ckpt.
`:next` 11. the buffer is empty 12.
i 've been profiling my code by programmatically recording millisecond timings.
i can save my model using model.save in tf.keras and have a well formed h5 file with a graph, weights and inputs and outputs as expected verified in netron.
ctrl + c will not trigger keyboard interrupt in terminal(cmd) of terminal vim when set termwintype=conpty
there should be no error
commands to reproduce the results and corresponding time are shown below in the screenshot below.
runtimeerrortraceback (most recent call last) in <module>() 6 if(not 7 os.makedirs(output_dir) ----> 8 train_and_evaluate() in train_and_evaluate() 36 estimator=classifier, 37 train_spec=train_spec, ---> 38 eval_spec=eval_spec 39 ) in train_spec, eval_spec) 428 config.task_type != 429 logging.info( 'running training and locally (non-distributed). ')
when i attempt to load my .tflite model on the android (after applying the gpudelegate), it fails saying "internal error" (see traceback below).
for the inceptionv3 model, the top1 miss rate of tf-trt int8 quantized model shall be smaller than 3~5% after calibration, however, i got top1 miss rate >29%
i have initialized it properly, but it is not available to me.
3. then run: phofcode the word `world` is still highlighted even though the match `mid` has been removed.
- tensorflow version (use command below): 1.14
the sample points are shape=(), dtype=float32) so i use tf.eval() to get the actual float number.
i get the following error message when i set tf_cudnn_use_autotune=0: e the primary convolution algorithm failed memory allocation, while a secondary algorithm is not provided.
should load normally, and i should be able to continue training where it left off using the loss.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - - os platform and distribution (e.g., linux ubuntu 16.04): arch linux - mobile device (e.g.
i'm expecting the model to learn _something_ - which doesn't mean that i'm expecting it to achieve a specific accuracy.
in tensorflow 1.13 this runs correctly, but in 1.14 it throws an exception: traceback (most recent call last): file "test.py", line 17, in <module> file line 634, in __call__ outputs = call_fn(inputs, *args,
`embedding_matrix[2]` is not equal to
train on 50 steps, validate on 157 steps epoch 1/2 30/50 eta: 0s loss: 1.3660 accuracy: 0.6500 traceback (most recent call last): file line 570, in read_variable_op "dtype", dtype) this function does not handle case of path where all inputs are already eagertensors.
alternatively, .../aot/tests/build, removing tfcompile_flags from and setting enable_xla_hlo_profiling to false makes the issue go away.
`cumsum` has small (possibly rounding or something similar) errors, and these errors accumulates quickly as the sequence length increase, e.g.
what does above error log mean, and is there any way to fix it?
* converting the 200-class model to .tflite using quantization toco goes off rails (~30% drop top 3 accuracy).
provide a reproducible test case that is the bare minimum necessary to generate the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
k can be used as 1-d tensor
`:cdo` is expected to stop when it encounters an error.
when filing the bug, set the verbosity to 10 (on linux, `export autograph_verbosity=10`) and attach the full output.
(i do not have macports on this machine and have had issues in the past when i mix brew and macports on same machine, but open to installing it and trying it if that may fix issue, but hoping to get to the root cause since it 's my understanding running the python cert install should have solved the issue) thanks for help!
1. create one-to-many relation 2. console.log lifecycle methods
when convert to lite quantized model with representative_dataset, it throws `std::out_of_range`, when model includes `tf.split()` my model implement pixel shuffle with some split and concat operation, i narrow down to tf.split() which is causing the issue
instructions for updating: use standard file utilities to get mtimes.
the code should print the list of trainable variables of the net.
the python script for conversion is here: phofcode and in android code, i called
it throws this failedpreconditionerror: error while reading resource variable name9/conv_linear/bias from container: localhost.
i get the following error message when i use tensorflow in gpu mode.
- vim version (below) - os: macos 10.14.5 - terminal: terminal.app combined with tmux vim - vi improved 8.1 (2018 may 18, compil aug 16 2019 version macos rustines incluses : 1-1850 compil par homebrew norme version sans interface graphique.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 64bit - mobile device (e.g.
when using gru with bidirectional wrapper, setting, `return_state=true, return_sequences=false` in gru parameters and `merge_mode="sum"` in bidirectional parameters does not merge outputs.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): `pipenv install --pre tensorflow==2.0.0-beta1` tensorflow version (use command below): `2.0.0-beta1` python version: `3.6.8` include any logs or source code that would be helpful to diagnose the problem.
error occurs when trying to compute the jacobian in the following code
- os platform: os x 10.13.3 - custom code - tensorflow version: 1.12.0 - python version: 3.6.5 traceback: typeerror traceback (most recent call last) in <module>() 2 x = tf.placeholder(tf.int32, [none, 10, 1]) 3 cell = dtype=tf.int32) ----> 4 output, state = inputs=x, dtype=tf.int32)#, initial_state=state) 5 in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope) 662 swap_memory=swap_memory, 663 --> 664 dtype=dtype) 665 666 # outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].
python script phofcode java code ( android environment ) phofcode
tried to load this v3 graph multiple times for purposes of testing: phofurl .
i can circumvent this problem by allocating new bytebuffer for every frame.
the train metric should be on train data only and validation metric (*_val) should be on validation data only.
finding the good for each neural network is a bit complicated since i have to grope.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i guess `model.evaluate` gets called when processing the validation set, so i tried to have a minimum reproduction sample with just directly calling `model.evaluate`.
if including tracebacks, please include the full traceback.
either all the metrics should be created with the exact same name, or they should all have an integer added to the end of the second metric with that name.
a clear and concise description of what the bug is.
aasset_close(asset); return false; }` ` code that runs successfully: const char *external_path jni_false); const char *relative_path char + strlen(relative_path)]; strcpy(tflite_path, external_path); strcat (tflite_path, relative_path); model( if(!model){ printf("failed to mmap model "); exit(0); } resolver; interpreter; if resolver)(&interpreter) == ktfliteok) { interpreter->invoke(); logd("tfliteok!
however, after you extract it and see the include files, they are protobuf==3.6.0.
devices: streamexecutor device (0): <undefined>, <undefined> traceback (most recent call last): file line 27, in <module> d_weights = t.gradient(loss, model.trainable_weights) file line 1001, in gradient file line 76, in imperative_grad file line 666, _ones return _fast_fill(value, shape, dtype) 621, _fast_fill dtype=dtypes.int32), 246, constant allow_broadcast=true) 254, _constant_impl t = ctx, dtype) 115, convert_to_eager_tensor return ops.eagertensor(value, handle, device, dtype) valueerror: typeerror: object of type 'tensor ' has no len(
1. run command line `npm install -g strapi@beta` 2. after install i cannot make strapi run, all commands are not found and when i add the env variable on windows 10, it says error.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.12 python version: python 3.6.8 |anaconda, bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
the memory keep growing with given representative data set.
the model by itself runs very well when is not applied.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): source.
example with nnapi switched off: image phofimage example with nnapi switched on: image phofimage
when `tf.keras.layers.lambda` is created without `mask` argument, it does not support masking ( phofurl however, when a mask is passed, it is silently ignored and `none` is returned as the output mask ( phofurl
phofcode the following setup works in comparison: phofcode
two of my plugins collect match information with since patch stuck in sandbox with ":s/../ =function/gn" problem: stuck in sandbox with solution: don 't skip over code to restore sandbox.
i trained a model with python and save it as ckpt files, and then transform it to pb file.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i've setup a couple repls to help reproduce and hopefully help come up with a solution.
phofurl -> states that the op is supported.
when i try to calculate the gradient of a tensor w.r.t some variables that the calculations involving them are happen in eager block, error in link 2 is raised (typeerror: cannot convert provided value eagertensor.
the dropout layer accepts a `noise_shape` argument.
in tensor_bundle.cc method getvalue if the tensor has entry.size() larger than 2gb the underlying hdfspread method accept a int32 type of size but entry.size() has 64 bit the static_cast will trigger a error of invalid param since it will cast the 64bit to a negative integer
valgrind reports the following: phofcode
i use yolov2 to train my model,and transplant it to mobile phone,in office demo phofurl it crash,output this bug: failed to run tensorflow inference with inputs: input], outputs:[output] e/androidruntime: fatal exception: inference process: org.tensorflow.demo, pid: 17423 concatop : dimensions of inputs should match: shape[0] = vs. shape[13] = t [[{{node concat_9}} = concatv2[n=19, t=dt_float, tidx=dt_int32, phofhyperlink ]] at method) it confused me long time,can anyone help me ,thanks a lot
the value changes from different inputs are very small, it seems to me this is under-fitting?
after the program runs for a while, i receive the following error.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): win 10 - mobile device (e.g.
5. retry with ` 6. it works.
repl phofhyperlink clicking the `refactoredbutton` causes the internal state of `refactoredbutton` and `value` to go out of sync.
... steps to reproduce the behavior: 1. fire up ui with it pointing at the petstore api.
`tf.print` should print out values as was specified in phofurl
i have a simple test to serialized and deserialize a model which has a stateful lstm.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
i am trying to port mnasnet's tpu implementation to the gpu.it works fine when using fp32.but when using mixed precision training, the speed is very slow.by using the profiler to track the training loop, i found that consumed too much time.i tried adjusting loss_scale but it didn't work.i want to know if depthwise convolution does not support backpropagation at half precision, or there are performance issues.
the word `in` should be moved to the 4th column.
i would like `fit_generator` to complete even when i use my custom callback.
we hope the engine_map in trtengineop can maintain a pair, which the first value can be tensorrt engine.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no - os platform and distribution (e.g., linux ubuntu 16.04):linux ubuntu 16.04 - mobile device (e.g.
you can reproduce the problem using the following code: phofcode
save the same data with a json field from the admin vs strapi.request sdk.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below):1.13.0-rc0 python version:python 3.5.5 :: anaconda, inc bazel version (if compiling from source):na gcc/compiler version (if compiling from source): cuda/cudnn version:10/7.4.1 gpu model and memory:titan v, 12 gb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" phofcode include any logs or source code that would be helpful to diagnose the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): archlinux - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.10 python version: 3.6 cuda/cudnn version: cuda 9.0, cudnn 7.1 gpu model and memory: geforce gtx 1060
using the c api or the original source code, the configuration of gpu must be the one defined with code.
the training speed should improve much more when going from config_gpu_0 to config_gpu_4 and it should improve by more than a factor of 2 when going from config_gpu_4 to config_gpu_8 (notice that in addition to using twice as many gpus, each gpu in config_gpu_8 is v100 which is in itself should be much more powerful than p100 in config_gpu_4).
2. what exactly is value that is issue, in this case batchsize ?
provide a reproducible test case that is the bare minimum necessary to generate the problem.
after running for 1 minute, the memory reaches 6gb.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
info:tensorflow:found tpu system: i0705 found tpu system: info:tensorflow:
the preceding tab character is not highlighted.
warning: logging before flag parsing goes to stderr.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
it crashes on declaration line with error phofcode it seems i am giving wrong input dimension.
config estimator '/tmp/tf-bug', config=config) tf.int32)) ``
i am not able to access any data from my private s3 bucket, i just keep getting this error notfounderror: object s3://bla/bla does not exist i have checked that my local machine has aws credentials in ~/.aws/credentials not only that i have also tried setting environment variables phofcode it still doesnt let me access my data from s3
scatter_max can work with mirroredstrategy in tensorflow-gpu 1.10.0
large logs and files should be attached.
steps to reproduce the behavior: 1. install algo in freebsd 2. reboot
the following code fails because the internal `simplernncell` is not built before it is used.
i would expect, that model_copy is known to both gradient tapes and can be used w/o using meta_model.
large logs and files should be attached.
3. open a new c file eg: `vim test.c` and paste the following code: phofcode 4. observe: here `test`, `linux_yield_hack` and `is_control` are highlighted with group `cmacroname` as expected.
large logs and files should be attached.
i 've even created stackoverflow phofhyperlink question as i am not entirely sure if this is a bug, but looks like it, since the tf.keras si relatively new, cannot find much help elsewhere
when i run the following code from the latest python3 gpu build, nothing happens.
-iproto -dhave_config_h -o2 -g -pipe -wall -werror=format-security -fexceptions -fstack-protector-strong -grecord-gcc-switches -m64 -mtune=generic -fstack-clash-protection -fcf-protection -d_gnu_source -d_file_offset_bits=64 -u_fortify_source -d_fortify_source=1 linking: gcc -l. -wl,-z,relro -wl,-z,now -fstack-protector-strong -rdynamic -wl,-export-dynamic -wl,--enable-new-dtags -wl,-z,relro -wl,-z,now -wl,-z,relro -wl,-z,now -l/usr/local/lib -wl,--as-needed -o vim -lm -lselinux -lncurses -lacl -lattr -lgpm -ldl -wl,--enable-new-dtags -wl,-z,relro -wl,--as-needed -wl,-z,now -wl,--as-needed -fstack-protector-strong -l/usr/local/lib -l/usr/lib64/perl5/core -lperl -lpthread -lresolv -ldl -lm -lcrypt -lutil -lc ``
the output should be following: phofcode i got correct output for following versions of the function: phofcode and (when tf.function is removed) phofcode and when i use small value for np.eye phofcode can you reproduce my error ?
works fine without distribute strategy or implementing map function without tf.py_function.
the official documentation also has no explanation for the cause of this error.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
it is expected that all the experiments above should succeed.
repeated tf.train calls train for the given amount of steps.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
but with same code, can train the model without any problem.
import tensorflow as tf import tensorflow.contrib.lite as lite scalar_input = tf.placeholder(shape=(), name= 'input ', dtype=tf.float32) b = tf.constant(1.0) val = scalar_input + b out = tf.identity(val, name="out") with tf.session() as sess: converter [scalar_input], [out]) tflite_model converter.convert()
if including tracebacks, please include the full traceback.
2. load that spec using the swagger-ui 3. observe error messages flash at the top of the page and disappear
`tf.io.gfile.gfile` does not work correctly with the python built in `zipfile` module.
we can import tensor_util in tf 1.13.
not a single reference to members of tensorflow can be found in ide(pycharm).
this is tricky as i 'm using c#, and the code is large.
when i 'm using keras layers to define the models and i 'm not calling `train_on_batch` or `compile` or any other method to train the model that 's pure keras, the update operations should be placed into the graph (having thus same behavior of batch normalization layer present in `tf.layers`) and executed when model `.trainable` is `true`.
- i work on vue.js project - i have form with input file.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.10.1 python version: 3.6.6 cuda/cudnn version: 9.0 gpu model and memory: nvidia gtx1080ti, 11gb gpu/ 64 gb ram
both are manually editted by me.
skip the current evaluation pass as results are expected to be same the same checkpoint.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
but still giving 10 objects as output in the android [1,10,4].
1. convert tf inceptionv3 to tflite (with optimization for size) 2. load model to android device - fails sometimes not finding a matching model or the error above.
open a large file with vim81.
the expected behavior would be that all positive labels in each example be assigned value 1 instead of `1/num_true`.
the expected behavior is that i am able to load multiple inception v3 graphs and afterwards unload all of them without any memory leak.
for example, "sensor 1" reports at a frequency of 2hz, while others may range up to 16hz.
this repl demo phofhyperlink contains the bug, though it 's not directly visible since it 's ssr-only.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04):colab - mobile device (e.g.
here is the current output (please find code below): phofcode please also note that the same code works as expected when running on the cpu.
i expect that getting the link url from an operation gives me a correct deeplink, e.g.
gif phofimage the issue disappears if i do any of the following: - reset ` 'showcmd '` (`set noshowcmd`) - reset ` 'showmatch '` (`set noshowmatch`) - set ` 'matchtime '` to 0 - press `x` after `&matchtime` empty ` 'indentexpr '` (`setl inde=`) comment `normal!
i just computed sqrt(2) 100 times in google colab.
layer1_w1 layer1_w1_ph:0 layer1_w1_assign:0 layer1_b1 layer1_b1_ph:0 layer1_b1_assign:0 layer2_w2 layer2_w2_ph:0 layer2_w2_assign:0 layer2_b2 layer2_b2_ph:0 layer2_b2_assign:0 i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 fma i cpu frequency: hz xla service executing computations on platform host.
* ubuntu 16.04 * python 3.6 * tf 1.12
running the sparse_matmul_op_test.py script on an arm architecture device can reproduce the failure.
i printed out the smallest and largest from the sampe and i got these should be 2!
as a result, tensorflow's xla cannot work properly, if it is used on a machine where path to cuda is different from compilation machine.
you will see that `conv2d` layer 'conv1 ' has the kernel shape `(3, 3, 1, 32)`, indicating that 1 is depth/number of channels for input.
after upgraded to tensorflow 2.0.0 it stops working and memory usage increasing without finish the program.
4. click `delete item` button.
also tried error is same: load library fails.
i needed feed different category images to different gpu(all the images have same size, such as input dogs' images to gpu 0 and cats' images to gpu 1) and apply the averaged gradients.
failed: build did not complete successfully
return the batch size as plain tf.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): installed from source tensorflow version (use command below): tensorflow-1.8.0 python version: python 3.0 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: cuda 9.0 gpu model and memory: tesla v100 (16gb) you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" architecture of vgg and yolo is bit different.
15. an alert or feedback announced when user activates discussed button users.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
autograph should works just find even after restoring from a checkpoint.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes (but the model should be running) - os platform and distribution: linux ubuntu 16.04 - mobile device (e.g.
i have created `.tflite` with single `conv_2d` op with bias tensor index `-1`(= optional).
import tensorflow as tf hello = tf.constant('hello, tensorflow!')
could error come from 32bit architecture raspberry-pi ?
if you have any ideas on these kind of debugging, will be very grateful hear.
detailed steps to reproduce the behavior: 1. create the file "syntax_prop.vim" containing the following: phofcode 2. run `vim --clean -s syntax_prop.vim syntax_prop.vim` 3. note that in the last line, `(abc)` is incorrectly highlighted: the initial matchparen highlighting includes `a`.
this issue seems to have been caused by commit #87cc788 which changed unit framework.
- vim - os: debian gnu/linux (debian 10) - terminal: gnome terminal the problem is, that the `:help expr-!~?` command calls `find_help_tags()` which converts the arg to a search pattern, but it handles this particular arg value as an exception and does not escape the `~` with a backslash before invoking regex pattern compilation.
it's almost like tflite doesn't use updated data buffer when it sees that buffer reference hasn't changed.
the minimal failing example with the code and data is here phofhyperlink .
for r, 1-based, it 's `0 < i <= bitwand(n, -4)` versus `bitwand(n, -4) < i <= n`.)
i ran my estimator with the distribution configuration i used is the whole error log is attatched at the end.
phofcode is there a way catch this error or check for it and recover/attempt reinitialize?
large logs and files should be attached.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes, i have written custom python code.
however, i have a minimal example which reproduces the bug when i mimic the `model_iteration` phofhyperlink function.
the quantization scale in perchannelquantizebias() phofhyperlink in phofhyperlink appears to be applied incorrectly.
- tensorflow gpu version: 1.13 - python version: 3.7.3 - cuda version: - cudnn version: 7.6.0 this discrepancy between versions exists even when setting the numpy and tensorflow random seeds before training.
please contact your administrator or your service provider to determine which device may be causing problem.
- vim: - windows 10 - gui/console
the build method should raise some kind of error saying that tensor computation is not allowed in build.
so started to uncomment my code and rejoin one part after the other.
all terminal tests should be passed.
might be related to issue #33767.
but the output of `seed` does not stays the same and changes over different executions.
phofcode the same issue happens when i add or multiply (but not divide!).
using debugger, memory increase occurs on initialisation.
if including tracebacks, please include the full traceback.
the pipeline set up like this: feature_description = { "features": tf.float32), "label": tf.fixedlenfeature([1], tf.float32) } def return feature_description) ds = for f fs_train]) ds = ds.map(_parse_function) ds = ds.flat_map(lambda v: v["label"]))) # filter data, only allow ls[0] ls[1] ds ds.filter( lambda _, y: tf.equal(y, ls[0]), tf.equal(y, ls[1]) ), []) ) # relabel re-map labels 0 1 ds.flat_map(lambda x, y: tf_relabel(y) - base_label.value))) # create sliding window for lstm shift=shift, stride=stride, drop_remainder=true) ds.flat_map(lambda x, y: y.batch(window_size)))) # batch prefetch ds.batch(batch_size, drop_remainder=true) tried many values, nothing works
if including tracebacks, please include the full traceback.
the expected behavior is for this error to not occur, and for the int8 calibrated and quantized graph to be produced correctly.
a model with many sequential dependencies would have many sequential dependencies when computing gradients).
any way, it works well in gpu version of tf.
there are two documented ways to use the tpu infeed ops: 1. place them on the cpu device and set device_ordinal to the requisite device 2. place them on the tpu device and leave device_ordinal empty the first way works fine, second way does transfer data to device, but then blocks, never returning control to client.
2. configure an ubuntu 19.04 client with an ip address of and a gateway/dns of 3. install and configure wireguard as specified in 4. start wireguard service via systemd.
- vim version: - os: [macos 10.15.1] - terminal: [iterm
phofcode all images in test_image_paths are
image phofimage - os & version: windows 10 pro version 1803 - redis-server version: redis on windows 3.2.100 i'd love to upgrade the rdm version but i don't think i can, due to the old version of redis server.
predict results of fitted model with tf.data.dataset using model.predict
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
when i run "./algo" i get an error message.
phofcode the code above is what i copy from tf docs phofhyperlink which will result an error like: phofcode at first i thought i had a wrong build of tensorflow without gpu support.
the code below produces a `typeerror`.
when i enter them i got the following message reported below i'm under proxy so maybe it's due to that, but i don't know where to set it up.
we extract the symbolic tensors.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
training stops with the attached error log when retraining mobilenet v2 ssd model.
another issue phofhyperlink of mine described more detail.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): maxosx 10.13.6 - mobile device (e.g.
should be able to create a trt model using the latest docker build.
the preceding tab character is also c wrongly c highlighted.
memory-leak-example.zip phofhyperlink the above code is a modified version of the `iris premade estimator` phofhyperlink example to loop over `train()` multiple times.
if including tracebacks, please include the full traceback.
being able to write to write to the summary from within the training loop.
---- the cycle detection algorithm could be optimized to have a linear time complexity.
it seems that the issue, although not in the bug tracker, should be already known to the authors: phofurl however, i can make a minimal example to reproduce the problem on demand.
- vim version does not reproduce the crash) - os: macos 10.14 - terminal: iterm2
adding visible gpu devices: device interconnect streamexecutor strength 1 edge matrix: 0: n created 5139 mb memory) -> physical gpu (device: name: geforce 6gb, pci bus id: compute capability: 6.1) info:tensorflow:calling model_fn.
sony xperia xz1 compact_ - tensorflow installed from (source or binary): _binary_ tensorflow version (use command below): phofcode ` model description as produced by [pose.zip]( phofurl
the keras layer should follow same path.
<+80>: mov 0x50(%r15),%rax <+84>: lea -0xa0(%rbp),%rbx <+91>: mov %rbx,%rdi <+94>: mov (%rax),%r8 <+97>: mov 0x48(%r15),%rax <+101>: (%rax),%rsi => <+104>: -0x18(%r8),%r9 how did -0x18(%r8) get illegal?
we expect to get the same results when we use v1 or v2.
-iproto -dhave_config_h -dmacos_x -dmacos_x_darwin -g -gdwarf-4 -ddebug -o0 -fno-omit-frame-pointer -u_fortify_source -d_fortify_source=1 linking: gcc -l. -l/usr/local/lib -o vim -lm -lncurses -liconv -framework appkit -lpython3.7m -framework corefoundation -framework tcl -framework corefoundation debug build ``
except error when run data type mismatch at component 0: expected int32 but got variant.
only the first 32 ops will run on the gpu, and the remaining 88 on the cpu.gpudelegate prepare: first dimension is supposed to be batch and always equal 1.node number 120 (gpudelegate) failed prepare.` what 's wrong with my model?
i can successfully get the .tflite file.
i get the error below when i try to import tensorflow on windows 10. also, i don 't know if it is related but i upgraded my gpu.
dependencies have all been installed, virtualenv is active.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
a stack trace for how coc.nvim calls `:redraws` upon `bufunload` can be found at phofurl
similar timeline shown as for e.g.
traceback (most recent call last): file line 1334, in _do_call return fn(*args) file line 1319, in _run_fn options, feed_dict, fetch_list, target_list, run_metadata) file line 1407, in _call_tf_sessionrun run_metadata) data type mismatch at component 0: expected int64 but got variant.
detailed steps to reproduce the behavior: 1. run `vim --clean` 2. type `:copen<cr>` 3. type `:copen<cr>`
- prepared & trained a keras sequential model based on this example script phofhyperlink , which contains dropout layers - passed the trained model to a kerasmodelwrapper phofhyperlink , a class provided by the cleverhans phofhyperlink library - instantiated a saliencymapmethod phofhyperlink attack object on the wrapped model, and attempted to craft an adversarial example with it - tensorflow encounters this error: phofcode removing the dropout layers from keras model prevents this error from occuring.
would you like to modify the code
seems like latest tensorflow version has changed a way how to call inputs or set weights.
using conditional statements on tensors passed to keras subclassed models without `@tf.function` decorator leads to valueerror exception ` valueerror: incompatible shapes between op input and calculated input gradient.
`import tensorflow as tf class qpwcx(layer): def __init__(self, sharp=100,
probably, `.call` method shouln 't just return output tensor, but it should return a new `model` that shared same parameters of orignial one, but defined with a new input (and thus with its update ops aware of input).
note that the csv file 'immunotherapy - immunodataset.csv ' can be obtained from the uc irvine machine learning repository at this url: phofurl # sample code for `tensorforestestimator` with `pandas_input_fn` ## python standard library imports phofcode ## non tensorflow imports phofcode ## tensorflow library imports phofcode ## aliased tensorflow library imports phofcode metadata for csv columns ordering of csv columns generate lists of features and labels from metadata helper function for shuffling and exporting subsets this function is used to export training, evaluation, and test data sets as csvs, shuffling the rows.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i convert the checkpoint to hdf5 model (graph with weights) with this script phofhyperlink .
but with the `tf.function` declared before the function, there will raise an attributeerror: 'tensor' object has no attribute 'numpy'.
for what it 's worth, the training still works and produces .h5 files.
i have been trying the demos of tensorflow 2.0, particularly the transformers example.
the model should be able to resume training after loading weights trained on multiple gpus.
not possible, as autodesk flame framework is required
the model cann 't be provided.
therefore, i want to be able to load model weights and resume training the model from a checkpoint.
the code also does same with `merge` ead branch.
should these be looking in `tensorflow_core/...`?
it seems to me that my inputs x and y (coming from my generator) are correct shape: phofcode for sake of completeness, training data generator is set to read in images from a directory with some augmentation:
all layers should be correctly created
also i cannot understand while sharing variables should be slower than instantiating from initializer with constant tensor...
i 've searched for a solution and found that sessions cannot be shared across process which is cause of hang.
i expect it to show the graph for the model.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the program should print some logging information in autograph conversion e.g.
1. npm install 2. view version of in package.json
- have i written custom code (as opposed to using a stock example script provided in tensorflow):
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.11 python version: 3.5 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: 9 gpu model and memory: titan you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
do note that i have successfully used the reshape functionality just fine in straight feedforward models.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): google colab - mobile device (e.g.
as long as it doesn 't display any wrapped lines, the popup-filter works great (apart from being very long).
when working with one hot encoding,
the format of the testscript is described at phofurl this test passes locally when run in docker container.
i want to try tensorboard from keras, it work if tensorflow-gpu=1.13.1 & tensorboard=1.13.1, but get the error if tensorflow-gpu=2.0.0a0 & as below: epoch 1/50 - eta: 11:31 - loss: 2.3852 - acc: 0.1562 notfounderror traceback (most recent call last) in <module> ----> 1 model.fit(x_train, y_train, epochs=50, 2 3 model.evaluate(x_test, y_test) in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing,
the behavior on cpu is also consistent with other activation functions.
according to definition of hann window phofurl phofhyperlink , which should be: `(*window)[i] = 0.5 - 0.5 * cos((2 * pi * i) / (window_length - 1));` please correct me if i missed anything
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: n/a you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with: 1. tf 1.0: `python -c "import tensorflow as tf; print(tf.git_version, tf.version)"` 2. tf 2.0: `python -c "import tensorflow as tf; tf.version.version)"`
it works when i set only one gpu avaiable.
`tf.function` converts almost identical functions into completely different graphs.
resnet50 local with output phofhyperlink : phofcode vgg16 local with output phofhyperlink : phofcode distributed using the same ps command with output phofhyperlink : phofcode resnet50 distributed with output phofhyperlink : phofcode vgg16 distributed
phofcode when using a lower level implementation it works correctly: phofcode
sometimes the calculation order becomes o(n2).
instead of jumping to last line, this will jump `45`.
google colab link: phofurl phofhyperlink interesting code: phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): google colab - mobile device (e.g.
the all_reduce tensor `reduced_weight` gives proper answer on all workers.
i have a mapping in help files to jump to the next help tag; it sometimes unexpectedly skip a tag.
if including tracebacks, please include the full traceback.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no - os platform and distribution (e.g., linux ubuntu 16.04): macos high sierra 10.13.6 - mobile device (e.g.
3. when wrapped lines appear in the popup window, the key mappings that scroll more than one line won 't work properly.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
things i 've tried without success: - setting `cpu_relocation=true` parameter - disabling eager mode
if the tutorial was correct, the code should run without any bugs and the evaluation results should be the same after loading.
1. tensorflow documentation should state clearly the preferred way to use `tf.keras` models inside `tf.estimator`, given the knowledge that `tf.estimator` is built on `tf.keras.layers` and thus the expectation that interops is seamless.
$ vim -nu none -s /tmp/s.vim +'b a.txt' the file `a.txt` is not folded in the top window.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): tensorflow-gpu 1.13.1 python version: 3.6.7 cuda/cudnn version: v10 gpu model and memory: gtx 980m
vim wrong data when saving text which can 't be converted.
log example of parameter of is as below.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:nvidia tx2 - tensorflow installed from (source or binary):binary from phofurl tensorflow version (use command below):1.110 python version:3.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version:cuda9.0 cudnn 7.15 gpu model and memory:8g you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
then the execution fails with: phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:n/a - tensorflow installed from (source or binary):binary tensorflow version (use command below):1.12.0 python version:3.6.8 bazel version (if compiling from source):n/a gcc/compiler version (if compiling from source):n/a cuda/cudnn version:n/a gpu model and memory:ati mobility radeon hd 6370 you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
there seems to be some issue with `tf.function`.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): source tensorflow version (use command below): 2.0.0 alpha python version: 3.7.3 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: 10.0/7.5 gpu model and memory: v100, 16gb > typeerror traceback (most recent call last) > in <module> > ----> 1 > in __call__(self, w) 110 k.epsilon() + k.sqrt( 111 math_ops.reduce_sum( --> 112 math_ops.square(w), axis=self.axis, keepdims=true))) 113 114 def get_config(self): in sqrt(x) 1878 a tensor.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below): python version: bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
1. change the favicon path settings at like suggested here phofurl 2. watch nothing change or 1. replace the favicon.ico in any folder 2. watch nothing change
the quantized model gives similar output values with the original tf graph.
i would expect the last `gk` to move the cursor on the previous screen line.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): `pip install tensorflow==1.13.1` tensorflow version (use command below): 1.13.1 python version: 3.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: full error for grad of `ragged_embeddings_1`: userwarning: converting sparse indexedslices to a dense tensor of unknown shape.
in the past, positioning the cursor on an url and press `gx` would open the url in a web browser (using `open` command).
large logs and files should be attached.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 1803 - mobile device (e.g.
i would expect training and validation loss to be constant throughout training and equal to each other
try to change score of zset menber: image phofimage image phofimage try to open a zset twice at the same time: image phofimage - os : ubuntu 18.04 - redis-server version [2.8.17]
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: python 3.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: only cpu gpu model and memory: the command fails with the following message: check failed: name.substr(colon_pos == string::npos (1 vs. name must only have digits after colon aborted (core dumped) `
import pandas as pd import tensorflow as tf import numpy as np from sklearn.preprocessing import multilabelbinarizer class def __init__(self, *args,
if including tracebacks, please include full traceback.
however, what is observed in tensorboard is: - training loss behaves as expected - validation loss is different in the beginning, slowly converging towards the constant value of the training loss - increasing the learn rate seems to reduce this effect significantly.
so i installed android studio and tried to run it from there.
it have no error when i run code with tf-2.0.0-alpha but the error happend with beta version tensorflow
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.13.0rc2 python version: 3.6.5 bazel version (if compiling from source): 0.19.2 gcc/compiler version (if compiling from source): 4.8.5 cuda/cudnn version: 10/7.3.1 gpu model and memory: titan xp see above
i would expect the names to match the layer names that the output comes from.
using the code from phofurl i get syntax errors on google colab and on jupyter notebook - tensorflow installed from (source or binary): !pip install tensorflow==2.0.0-alpha0 - tensorflow version (use command below): mesh-tensorflow 0.0.5 tensorflow 2.0.0a0 tensorflow-estimator 1.13.0 tensorflow-hub 0.4.0 tensorflow-metadata 0.13.0 tensorflow-probability 0.6.0 - python version: google-api-python-client 1.6.7 ipython 5.5.0 ipython-genutils 0.2.0 ipython-sql 0.3.9 opencv-contrib-python 3.4.3.18 opencv-python 3.4.5.20 python-apt 1.6.3+ubuntu1 python-chess 0.23.11 python-dateutil 2.5.3 python-louvain 0.13 python-rtmidi 1.2.1 python-slugify 3.0.2 python-utils 2.3.0 here is the source code phofurl here is the video showing the results: phofurl
detailed steps to reproduce the behavior: 1. enter a tty with gpm phofhyperlink running which gives you a mouse cursor in console.
the example in the attached zip file is using `imports85` data.
calling the keras phofcode function in non-eager mode fails after one epoch, if specifying a tensorboard callback which has phofcode .
large logs and files should be attached.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): see above.
first run the docker image then run the code in python: docker run -it --runtime=nvidia --rm -e -v python 1. import tensorflow as tf print(tf.matmul([[1., 2.
could anybody help me asap?
the results are still correct, but this disables arithmetic optimizer for some (all?)
provide a reproducible test case that is the bare minimum necessary to generate the problem.
gif phofimage the issue is specific to `'cursorline'`; i can't reproduce with another window-local option such as `'colorcolumn'`: $ vim -nu none --cmd 'au bufread,bufnewfile *.xx setf xx' --cmd 'filetype on | au filetype xx setl cc=12' +'wincmd w' -o xx.xx y
the network connection between your computer and the vpn server could not be established because the remote server is not responding.
large logs and files should be attached.
large logs and files should be attached.
- vim version: > vim - vi improved 8.0 (2016 sep 12, compiled apr 10 2018 included patches: 1-1453 - os: ubuntu on windows 18.04 - terminal: mintty if i change `bufread` to `bufenter` or `bufwinenter`, the message can be shown.
when compiling the example custom op from phofurl with gcc 5, a segfault is encountered when running with tensorflow 1.14 and python 3.6 (on linux).
__for tensorflow 1.15-rc0:__ the same code will run forever without error message.
i would not expect arithmeticoptimizer to fail.
large logs and files should be attached.
hope all the tensor could be eagertensor in tf2.0
term-01 phofimage term-02 phofimage term-03 phofimage - vim version output: phofcode - os: windows 10 home version 1903 os build - terminal: gui - git: version 2.22.0.windows.1 mintty: mintty 3.0.1 (x86_64-pc-msys) type: xter
large logs and files should be attached.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): tensorflow version (use command below): python version: 3 bazel version (if compiling from source): colab gcc/compiler version (if compiling from source): colab cuda/cudnn version: colab gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
... steps to reproduce the behavior: 1. load any oas3 definition with a severs object.
specifically, when calling the fit method with a `validation_split` parameter, a proportion of the samples equal to (count of samples) * (validation_split) should be removed from the training data.
i 've stumbled upon this being unable to import after mapping a function onto a dataset, even though i can import it in ipython.
does not truncate the destination file before overwriting.
using different `tf_random_seed` values for `tf.estimator.runconfig` produces the same model performance and parameters for
this could mean that was uninitialized.
import tensorflow as tf class def __init__(self, num_units,
commit phofurl fixed this for non-eager mode case and added a test case to help prevent future regression.
i0409 coordinator.py:219] error reported to coordinator: only tensorflow native optimizers are supported with distributionstrategy.
expect the training to slowly progress given the memory is not big enough to hold the tensors then the swap should be able to help.
intuitively, i would think way to fix issue would be to follow an alternative route within `tf.while_loop` 's source code when using symbolic tensors (_e.g._ that used when eager disabled), and keeping eager-suited one otherwise.
not sure why `output_feature_names` has to be a `softmax` as its a detection problem.
wrong syntax highlighting: <img src=" phofurl expected syntax highlighting: <img src=" phofurl - vim version: (not working), well) - os: macos 10.15 - terminal: apple terminal, macvim gui add any other context about the problem here.
... steps to reproduce the behavior: 1. go to '/imagesvalid ' resource 2. click on 'try it out ' 3. make sure that 'select file ' button is presented 4. go to '/imagesinvalid ' resource 5. click on 'try it out ' 6. make sure that 'select file ' button isn 't presented
error does not occur if i use double backslash to join checkpoint_folder and model_filename instead of os.path.join.
... steps to reproduce the behavior: 1. go to phofurl 2. copy paste the provided spec 3. click on post /test endpoint 4. under responses, change the response content type/accept to "application/xml"
func() {{{1 ', \'endfu \']" + 'sil %y ' + 'wincmd w ' + 'exe "norm!
the protobuf version should be consistent.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.6.8 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: full stack trace with i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 fma i xla service executing computations on platform host.
when attempting to resume training after loading weights that were trained with multiple gpus and tensorflow crashes with the following error: phofcode the error does not occur when the has been pre-trained on cpu, or only one gpu.
i 've observed this expected behavior when using and
i tried to export the resulting .pb file to tflite via this command in the terminal: `tflite_convert --output_arrays=output `
the corresponding tests in conv_test.cc phofhyperlink and depthwise_conv_test.cc phofhyperlink , e.g.
fails when eager is left enabled, with the following error messages depending on the tensorflow 2.0 installation used: 2.0b1: phofcode 2.0 nightly: phofcode note that decoration with `@tf.function` does not solve issue, as functions wrapped this way do not accept keras symbolic tensors as inputs
- have i written custom code (as opposed to using a stock example script provided in tensorflow):yes phofcode phofcode - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
terminate called after throwing an instance of 'std::out_of_range ' what(): _map_base::at abort (core dumped)
i am performing an image inferencing task using a model (`.pb file`).
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): pip wheel tensorflow version (use command below): 1.13.1 python version: 3.6.5 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: na gpu model and memory: (running on cpu only) include any logs or source code that would be helpful to diagnose the problem.
i trained model with tf.keras 's functional api with tensorflow 2.0 and save model with tf.keras.model.save() and convert the h5 model with when i try to serve the model the command phofcode i get error message >runtimeerror: the session graph is empty.
not able to call more than once to create tf-trt nodes for disjointed sub-graphs.
or at least just close window that 's current, though we should probably update `:help winbar` to state that commands are executed without changing current window (though looking at code, i don 't think this is true).
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: dont think its possible - tensorflow installed from (source or binary): preinstalled tensorflow version (use command below): tensorflow 1.12.0 python version: 3.6 bazel version (if compiling from source):na gcc/compiler version (if compiling from source):na cuda/cudnn version: gpu model and memory:na but here 's the complete log of the error phofcode ``
i expect the value of the global random seed to be taken into account every time a pseudo-random number is generated.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac 10.14 - mobile device (e.g.
so, issue is not specific to `-o`; it probably applies to all flags which create windows (`-o`, `-o`, `-d`).
when i try to fine tune a faster rcnn + resnet50 object detector i get a runtime error.
this is the first layer for tf.keras.model that accepts dict of tensors as the input.
` code import numpy as np import tensorflow as tf class def __init__(self, dense,
dns-level ad blocking is not enabled.
this only happens on gpu.
1. add `set lispwords=hello` to `~/.vimrc` or 2. run `vim /tmp/hello.scm`.
but it always predicts the first class label with the same probability score.
if is used as the first layer in a keras model created by subclass tf.keras.model, model.build(input_shape) will fail with any type of input_shape.
phofcode it seems that many other users are experiencing similar issues on tf2.0-bet
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): my script is included below.
note: in tensorflow 1.x, the tf.image.resize method had an 'align_corners ' parameter that toggled between defective and proper behavior and was set to false (defective behavior) by default.
actually, i have written two snippets of training code, the first code snippet is : phofcode the second snippet of the training code is: phofcode
this should cause a segfault.
i am trying to use tensorflow gpu version to train and test my deep learning model.
i will fill another bug report for this thoug
this is without changing any configuration values, and answering with default settings for everything except server ip.
phofcode should create iterator object.
its means not taking load.
logging summary scalars (for review in tensorboard) leads to significantly increased training times.
`params` is a python `dict`.
if including tracebacks, please include the full traceback.
i am trying to optimize (decrease) the inference time and model size of my tiny yolov3 model.
]).repeat() return labels)) # configured the estimator for either distributed or non-distributed evaluation # training is always distributed if do_eval_dist: distribution ) config else: distribution config classifier config=config, print( '
if applicable, add screenshots to help explain your problem.
i would like this function to work faster for 8k size images.
<tf.tensor: id=xxx, shape=(2, 1), dtype=float32, numpy=array([[1.
the expected behavior would be that using `tf.function` results in the same behavior than the eager mode.
h1 128 number neurons in fully-connected layer.
during inference, i have observed that the model converted with the latest tensorflow version is ~2x slower
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): source tensorflow version (use command below): 2.0.0-beta1 python version: 3.6.8 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: include any logs or source code that would be helpful to diagnose the problem.
it's always going to be the last child glitching out.
after successfully installing vpn (and confirming via macbook) i sent the user.mobileconfig to my iphone xs max via imessage and gmail.
in database (mysql in my case) this field would be empty.
when calling estimator.train with 'evaluator ' included in runconfig, a valueerror should be reported.
i'd expect the sgd optimiser to be usable in eager execution mode.
i have tried your hosted model phofhyperlink , it works perfect on my device on cpu and opengles delegate.
instructions for updating: use tf.where in 2.0, which has the same broadcast rule as np.where w0617 deprecation.py:323] from (from is deprecated and will be removed in a future version.
popup (broken): <img width="166" alt="screenshot at 11 59 45 am" src=" phofurl normal window (correct): <img width="205" alt="screenshot at 12 00 56 pm" src=" phofurl
- have i written custom code (as opposed to using a stock example script provided in tensorflow): i am using custom code - os platform and distribution (e.g., linux ubuntu 16.04): os: windows os kernel version: os release version: 10 os platform: - mobile device (e.g.
w invalid argument: upper bound check fail input 1 from node to node input bounds = backing_tensor bounds = t [[{{node w invalid argument: upper bound check fail input 1 from node to node input bounds = backing_tensor bounds = t [[{{node t [[{{node _send_add_1_0}}]] invalidargumenterror: invalida...nterror()
provide a reproducible test case that is the bare minimum necessary to generate the problem.
convert to tflite without error
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): docker tensorflow version (use command below): 1.11, 1.12, 1.13.1, python version: version supplied in docker container bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.0 gpu model and memory:
however, you can get `cancellederror` in few trials phofcode
the requested linux headers from algo on a proxmox ct is
following are errors i get while trying to build for android:
python3 type hinting should not fail the code.
is there a certain value shouldn 't exceed with `steps_per_epoch`?
error: none values not supported.
large logs and files should be attached.
run the test code below.
large logs and files should be attached.
train model using tf and tf.keras
i retrained a ssd_mobilenet_v1 as described in this guide phofhyperlink .
very fast to jump to definition
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): debian 9.8 - mobile device (e.g.
when i try to freeze my tf graph that uses phofcode layers i get the following exception.
get same inference time with jni on android.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): arch linux - mobile device (e.g.
have also added control dependency between layers to ensure that my custom op gets called one layer after another.
1. run this shell command: $ echo test | vim -nu none +'set cursorline' - 2. press `yy` to yank the current line, then `p` to paste it above.
should load the graph without an exception.
detailed log could be found [here phofhyperlink
does not manages foldl operation.
the code works as expected when we provide a constant learning rate.
if including tracebacks, please include the full traceback.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): debian x86_64 gnu/linux - mobile device (e.g.
but please understand that svelte is run by unpaid volunteers in their free time, and issues that follow these instructions will get fixed faster.
instead, `dense` behaves on a 3-rank tensor as it would behave if it was wrapped in a `timedistributed` layer, making me question the utility of `timedistributed` at all.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i was unable to reproduce this in the repl (i assume it doesn't support dynamic imports), so here is a repro repository: phofurl just run `yarn build && yarn start` and then open up the webpage and attempt to click the button.
i think that we should not get warnings about deprecated objects when we are not calling any deprecated method, attribute, etc.
if including tracebacks, please include the full traceback.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
if including tracebacks, please include the full traceback.
this is usually triggered by winbar command which does `:tabclose!`.
it fails with a deadlineexceedederror (stack trace below).
it shows the correct output.
as you can see in the attached screen shots , while model section is confined to depth limit (1) example value is unlimited causing entire appear in example section.
i expect the same behavior as in regular python: `i` should be the last value that was run in the `for` loop.
callbacks that need full graph visibility need their `set_model` function called after full graph is created.
this code was perfectly normal in tf 1.x
1. run `vim --clean` 2. set option `:set virtualedit=all` 3. type phofcode 4. now the cursor is at the column 3, while it should be at the column 2.
run this shell command: $ vim -es -nu none + 'cno <c-a> xxx ' + '%p|qa! '
i thought `tf.gather` have almost same functionality, so i test it and it seems no problem at all.
to work as though if it was run without `@tf.function` (e.g.
this is a netrw bug so i 'm not sure if this is the correct place to post it, but with the ownership question i 'm not 100% sure who owns netrw now and where i should file a bug for it to be fixed and updated to vim 's bundled plugin.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
additional info: the expected behavior occurs when creating `gradienttape` with `persistent=true` and replacing last line of code with `g.jacobian(y, x, however, that workaround is extremely slow with real data.
just set the explicit flag to false to get the error.
the error message, reported below, is misleading since it talks about a non-first invocation when the function is invoked only once.
a clear and concise description of what you expected to happen.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
if including tracebacks, please include the full traceback.
when run with tf 1.12 this code snippets behaves as expected
since `input_scale` and `output_scale` are the same, `real_mulpliter` shall be same as `filter_scale` - more even, these parameters shall be same for most conv ops in quantized mobilenetv1.
is a new keras layer added in tf2.0a0.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.13.6 - tensorflow installed from (source or binary): from pip install - tensorflow version (use command below): 2.0.0-beta0 python version: oct 20 2018,
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes, see below - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - tensorflow installed from (source or binary): pip install tensorflow - tensorflow version (use command below): `tensorflow==1.13.1` and python version: 3.7.2 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: n/a in my opinion, the bug is here: phofurl and still here: phofurl in `on_epoch_end`, `step` should always be set to `epoch`
i 'm totally stumped however on what might be causing this.
n_batches 1 est the model will stop training once specified number of trees is built, not based on number of steps.
large logs and files should be attached.
the model only updates for model1 without any checking for model2.
instead, i have to wrap the variable in a `tf.identity` call in order to get its value properly.
n/a i thought that the problem that (inserting `a`s) is very slow on vim might also related this, however, it wasn't related
subclassing `tf.module` should expose the variables (not to mention training_variables) from all sub-computations, including keras layers, so long as these computations are set as members of the class.
i didn't manage to get a minimal example using `fit_generator` (basically it relies too much on me using my model which is complex).
> import numpy as np > import pandas as pd > import tensorflow as tf > import ssl dftrain = pd.read_csv( ' phofurl dfeval = pd.read_csv( ' phofurl y_train = y_eval = try: %tensorflow_version 2.x except exception: pass tensorflow as tf tf.random.set_seed(123) print( "tensorflow version {}".format(tf.version)) platform vocabulary fc tf.feature_column categorical_columns [ 'sex ', 'parch ', 'class ', 'deck ', 'embark_town ', 'alone '] numeric_columns [ 'age ', 'fare '] def vocab): return vocab)) feature_columns [] for feature_name in categorical_columns: # need to one-hot encode categorical features.
* converting both models to .tflite format using toco works just fine (again, identical performance in inference).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): - mobile device (e.g.
4. saving to the savedmodel format works with just dense layers: this works without errors.
1. run `vim --clean` (or `gvim --clean`, etc.)
the setup is as follows, `main.go` initially populated with: phofcode we are in insert mode, with the cursor at position `(4,5)`.
4. modify the request body value.
any help will be appreciated.
1. click the "try it out" button.
large logs and files should be attached.
when i am trianging my model with tf.estimator and tf.data, this issue occurs: `invalidargumenterror (see above for traceback): concatop : dimensions of inputs should match: shape[0] = [128,64] vs. shape[1] = [127,16]` the batch of data is 128, so the first feature ' s is correct, the second wrong.
... steps to reproduce the behavior: 1. click on try-it-out 2. click on execute ( leaving the form empty )
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: none - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 python version: 3.6.4 bazel version (if compiling from source): none gcc/compiler version (if compiling from source): none cuda/cudnn version: none gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)"
use gpu to accelerate inference
all commits should be pushed to a repo.
3. observe the debug output.
the frozen model does not seem to be corrupted because i was able to deploy the other version successfully.
before: phofurl after: phofurl notice how the order is different.
there should not be such a paradox.
then reran it and returned to 8k.
the caller indicates is not a failure, but may mean there could performance gains if more were available.
checkpoint frequency is determined based on runconfig arguments: save_checkpoints_steps none or save_checkpoints_secs 300. warning:tensorflow:from map_and_batch (from is deprecated will be removed in a future version.
when running the same program again (after stopping the previous run) it doesn 't detect any gpu devices.
if you download the python 2.7 cpu-only official whl file (following phofurl phofurl you will see that it requires protobuf>=3.6.1.
it should return false otherwise, i guess.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): centos linux release (core) - mobile device (e.g.
when i try to use `renorm=true` with under i get the following error: `handle` is not available outside the replica context or a call.
simple use the svelte repl and paste the following into app.svelte phofcode * if you can demonstrate the bug using phofurl please do.
phofcode this is related to issue #28581. after elaborating on issue and not receiving a response, i have opened this new issue.
i am also seeing this issue in my environment.
therefore, when a mask is passed, it should raise an exception similarly to what `layer.compute_mask` does ( phofurl
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): mac os - mobile device (e.g.
on the android device(google pixel 3), i am able to successfully benchmark the model using the tensorflow provided benchmark utility ( phofurl running same model on ios device (iphone x) without gpu delegate option works fine.
i cannot load a model containing a custom initializer, or a custom regularizer, or a custom constraint, if they are defined as regular functions (rather than by subclassing the appropriate classes).
no warnings should be come from the tensorflow header.
this might be acceptable for mnist size datasets but when using imagenet `tf.data.dataset::cache` will create a single cache file with well above 350gb vs 144gb of original tfrecords.
when using a custom metric, the metric is computed on training data and validation data together.
stats multivariate_normal normal def test_tf (x): z = x[:,0]
detailed steps to reproduce the behavior: case 1: 1. run `vim --clean -c 'set number ' tmp.vim` 2.
.tflite model converted without post-training quantization works well on cpu and nnapi.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
do the same thing again, happens the same way
cannot save a tf.keras model if trained with mirroredstrategy either by calling save_weight or by a but does work if mirroredstrategy is not used.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macos mojave (10.14.2) - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12.0 python version: 3.6.6 (anaconda) here 's the traceback: traceback (most recent call last): file "repro.py", line 8, in <module> tf.no_op) file line 488, in new_func return func(*args,
i found that the statusline flickers when scrolling with `<c-f>` or `<c-b>`, with `syntax` and `number` set to `on`.
see colab link in description.
... steps to reproduce the behavior: 1. add the security scheme to the phofcode 2. open the documentation and see console.
custom loss function should be loadable.
test.vim phofcode 1. vim test.vim 2. :so % 3. errors occur
no memory leak, per phofurl
lcd %:p:h` issues message output with a directory name while executing `:vim /something/
i 'm not sure if i am describing this adequately.
previously, this call was serializable phofcode now, this is giving issues phofcode this behavior can be reproduced without using `spark`.
am i doing something wrong?
the assertions below all pass.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): i have - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.14.3 - mobile device (e.g.
it seems to occur under these conditions: 1. mouse support is enabled (+mouse) 2. the mouse option contains n (also the situation with `set mouse=a`, of course) 3. the ttymouse option is blank.
seeing as tensorflowsharp is a pretty thin wrapper class, this seemed to be more related to the internals/usage of tensorflow itself.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes.
phofcode here is the colab link phofhyperlink
- in normal mode, type `:terminal python` to open `python` in `terminal` provided by vim.
- os platform and distribution: linux ubuntu 18.04 - tensorflow installed from (source or binary): binary - tensorflow version: 1.13.1 - python version: 3.6.3
meta graph exported by export_meta_graph cannot be imported by import_meta_graph
popup window can not scroll with mouse wheel in gvim in terminal vim, it works as expected.
tensorflow fails with segmentation fault when using custom ops built with gcc5.
should save a checkpoint model file and not crash
more generally it would be preferable if gfile followed the api of python 's `io.iobase` phofhyperlink .
as you can see there is nothing fancy going on in the code.
`:e` 6. both files are hidden
all 3 tests should pass.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 14.04 - mobile device (e.g.
after i installed tensorflow-serving-api 1.10, error occured.
3. policies `isexecuted` not executed.
expected not to downgrade performance when upgrading tensorflow
name="flatten")` should allow access to flatten operation from the exported graph using name `flatten` and should not require naming it separately.
when i am using the with multiple replicas, `tf.math` ops do not work on `mirroredvariables` when inside a cross-replica scope.
... steps to reproduce the behavior: 1. update file swagger-config.yaml to add "displayrequestduration: true" 2. run "npm run build" 3. copy files under list/ directory to your website location 4. open your oas and use try it out feature then check in the response and you would see that there is no "request duration" box.
detailed steps to reproduce the behavior: $ vim --clean -c "set buftype=nofile selection=exclusive" -c "call setline(1, " \'here \' ")" -c "normal!
a clear and concise description of what the bug is.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
if including tracebacks, please include the full traceback.
`image_data = 'rb ').read()` my tf1.x version below run well `image_data = 'rb ').read()`
start_col seems to be wrong for textprop.
detailed steps to reproduce the behavior: 1. create a temporary directory and `cd` into it: `cd $(mktemp -d)`.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
--tag_set 'serve ' --signature_def 'serving_default ' --input_exprs x=[1];y=[2] traceback (most recent call last): file line 827, in <module> sys.exit(main()) file line 823, in main args.func(args) file line 644, in run init_tpu=args.init_tpu, tf_debug=args.tf_debug) file line 304, tag_set) 49, get_meta_graph_def ' could not be found savedmodel ') runtimeerror: metagraphdef associated with tag-set 'serve ' could not be found savedmodel ``
trying to convert a graph containing a reshape layer followed by batch normalization appears to trigger an incorrect op reordering.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below): '1.11.0 ') python version: bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" '1.11.0 ') > > futurewarning: conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated.
1. run `vim --clean popup.vim -c "source %"` phofcode 2. press `<c-j>` or `<c-k>` repeatedly to scroll the popup 3. the popup repeatedly jumps back to its original position - `<c-u>` and `<c-d>` do nothing 3. remove the `sign place...` line and run `vim --clean popup.vim -c "source %"` again 4. the buffer in the popup can now be scrolled as expected.
then, i 've tried implementing the same using keras functional api, but was greeted by the error message below: attributeerror traceback (most recent call last) in <module> ----> 1 inputs = name= 'features ') 2 x = layers.dense(128, 3 x = layers.dense(64, 4 5 baggage_pred = layers.dense(1, name= 'baggage ')(x) in input(shape, batch_size, name, dtype, sparse, tensor,
should find 12 threads and runs.
python import tensorflow as tf @tf.function def filename): out = tf.cast(img_decoded, tf.uint8) out = quality=100) out) def filename): out = tf.cast(img_decoded, tf.uint8) out = quality=100) out) img 127) # example gray image writejpeg_graph(img, "./tfwrite_graph.jpg") # "tfwrite_graph.jpg" not created writejpeg_eager(img, "./tfwrite_eager.jpg") # "tfwrite_eager.jpg" created ``
1. create new content type "news" 2. add "date" field, don 't make it required by default 3. create a new "news" page without filling "date" field and save it.
it takes a lot of time to train a robot.
have a plugin to automatically set height focused window; it succeeds into maximizing its after leaving window; but vim resets its right afterward, if geometry terminal has changed was opened.
when a standard search (no re in my case ) hits the start or the end of the file (i.e.
the embedding matrix was divided and placed on each ps with corresponding update ops.
s:do(event) abort if exists( '# '.a:event) exe 'do '.a:event. '
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): pip tensorflow version (use command below): 1.13.0-rc1 python version: 3.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: irrelevant i can do this if the patch will be accepted
provide a reproducible test case that is the bare minimum necessary to generate the problem.
8. on "response content type" combo box clearly visible.
closing the window can be done only with the mouse and results in a `segmentation fault`.
this link is the source code.
actually, i 'm not sure if there 's an official format for tags files, but before that patch, tags files without a trailing newline would work as well.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04.2 lts - mobile device (e.g.
$ vim -nu none -s /tmp/s.vim +'b a.txt' i bisected the session file `s.vim` to find out which line(s) exactly triggers the issue; it appears to be: badd +0 /tmp/a.txt the issue is triggered only if `:badd /tmp/a.txt` is run while the current buffer is not `/tmp/a.txt`.
macos: - `vim - vi improved 8.1 (2018 may 18, compiled aug 3 2019 - os: `macos 10.14` - terminal: `iterm2` windows: `vim vi improved 8.1 (2018 may 18, compiled aug 5 2019 `ms-windows 64-bit console version` os: `win10` terminal: `powershell
when a matrix multiply is called such that one of the input tensors is of data type fp16 and declared sparse, the function returns incorrect results.
large logs and files should be attached.
therefore it is also suprising that this function helps at all during eager execution.
i unfortunately only have custom code for now.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):
in short i am using the keras library with tensorflow gpu as a backend in rstudio for deep learning, specifically convnets.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
when `ndim_y_true` also yields strange result `2` (this time, three versions tried), while expect `[64,]` case.
then run to start server.
on a mac, create a file called zero_out.cc with the following code (copied from phofurl phofcode then run: phofcode you should see an error like this: ld: library not found for clang: error: linker command failed with exit code 1 (use -v to see invocation) ``
when i run the example code from phofurl i get this: phofcode
phofcode this outputs: phofcode whereas this: phofcode outputs: shape=(?, ?
devices: streamexecutor device (0): <undefined>, <undefined> traceback (most recent call last): file "hello_mnist.py", line 55, in <module> tensorboard_callback]) file line 806, in fit shuffle=shuffle) file line 2503, in _standardize_user_data file line 456, _method_wrapper result = method(self, *args,
1. ping is working for local ip e.g.
some apps are extremely unreliable.
by contrast, without eager, all epochs ran at same speed, for 8 / 9 seconds (15 ms/step).
even though train metrics did improve to almost perfect levels and the loss remained stable and low after a while, the inference i got was gibberish.
get mysterious errors when running model_visual_test.py: phofcode it doesn 't work even if train model on rpi, by running gen_test_train_data.py, then run train_model.py, which takes forever.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the model should run on phone using nnapi, without any problems.
input names in graph definition and timeline label in metadata have the same names
i would expect to be able to visit the webserver site without issue and load like normal.
6. keyboard on expand/collapse carrot visible.
`popup_create()` doesn 't accept buffer-local text-property types, contrary to what is recommended in `:h popup-textprop-pos` (under "some hints").
... ust send request with matrix parameters
getting different results on s390x for 32 vs 64 bit data types is nothing new.
the nadam optimizer will be deterministic.
still, this worked perfectly in vim 8.0.776 but is basically so bad in vim that have to disable tags scanning.
i 'm having troubles running a piece of code on my raspberry-pi that actually works just fine on my local computer.
i would expect `fit_generator` to work just the same as in the vanilla-keras version or in the normal `fit` version.
using tfdbg command 'run -t 100 ', we get execution as normally expected.
may be relevant: - phofurl - phofurl - phofurl
compare `let s = localtime() | echo expand("a") | echo localtime() - s` to `let s = localtime() | echo expand("a \'") | echo localtime() - s`
the `popup_*()` functions that create a popup window, ignore the `normal` highlight group and use `pmenu` instead.
i get none as the values of the model when i use input layers.
somehow sessionrunhook turned into perreplica at some point in evaluation code of estimator.
a tflite file is expected to be created and written to the local directory.
the caller indicates that this is not a failure, but may mean there could be performance gains if more memory were available.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.14.3 - mobile device (e.g.
not found: resource does not exist.
it's also significantly slower than executing the same inputs with graph mode in tf1.x while passing data with variable batch sizes is uncommon on most dl tasks, it is required for tasks on geometric learning.
this works with standard estimator models but doesn 't when using keras models with model_to_estimator (doing this still seems advised way to do distributed learning with keras phofurl (we also tried new standalone mode without any success) some nodes are failing an io error when trying to save first checkpoint concurrently when creating estimator on each worker it calls model_to_estimator on each worker which calls _save_first_checkpoint l457 phofurl
per `:help complete-functions`, two calls are made to a designated `omnifunc`, distinguished by the value of the first argument.
instead i get an error like this: phofcode - vim version: phofcode - os: ubuntu 18.04 - terminal: gui weirdly, this * only * seems to affect files created by nautilus.
... 1. open swagger ui with the json definition i shared 2. open /system/info 3. click try it out 4. enter a value (e.g.
i got error: phofcode here 's the traceback: > traceback (most recent call last): > file line 1334, in _do_call > return fn(*args) > file line 1319, in _run_fn options, feed_dict, fetch_list, target_list, run_metadata) file line 1407, in _call_tf_sessionrun run_metadata) another thread on parallel threw: traceback (most recent call last): file line 52, in <module> sess.run([train_op]) 929, run run_metadata_ptr) 1152, _run feed_dict_tensor, options, run_metadata) 1328, _do_run 1348, _do_call raise type(e)(node_def, op, message)
how should i give the input shape in this custom layer?
the test program should complete normally.
should not affect the behavior and the code should run in both cases.
steps to reproduce the behavior: 1. execute command `./algo ' 2. choices are sequentially: `9` `y` `y` `` `y` `` `y` `` `localhost` `my public ip`
in all, vim crash with core output.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - tensorflow installed from (source or binary): pip - tensorflow version (use command below): 1.13.1 python version: python 3.6.7 cuda/cudnn version: 10.0/7.4 gpu model and memory: geforce gtx 1060, compute capability 6.1, 6gb ram this issue follows on from phofurl
import tensorflow as tf import numpy as np import gzip import json from sklearn.model_selection shufflesplit with "r") as fin: housing = json.load(fin) for train, test in shufflesplit(1, 0.2, 'data ']): x_train = y_train = x_test = y_test x_mean x_train.mean(axis=0) x_std x_train.std(axis=0) xs_train (x_train - x_mean) / x_std xs_test (x_test - x_mean) / x_std class linearregressiontf(): def __init__(self, eta=.1): self.w tf.variable(0.)
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): python notebook using python v3.7.
to have yardstick for cpu execution, re-wrote same o(n4) using numpy exclusively and it takes just 210 seconds.
(issues related to the runtime files should be reported to their maintainer, check the file header.)
v_aux mask * v_masked v_norm [0, 1, 2]) x_init tf.nn.conv2d(x, v_norm, strides=stride_shape, padding=padding)
be able to run the scripts.
i bisected the issue to the commit phofurl there was no increased latency just before.
i0603 session_manager.py:502] done running local_init_op.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 python version: 3.6 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: cuda 9.0 cudnn 7.0.5 gpu model and memory: nvidia titan v the `build_info` module is generated during the build process by `gen_build_info.py` phofhyperlink .
setbufline failed if the buffer is added by bufadd function.
however, after the first `session.run`, the collective op caches its output shape (which is `16` in our case) in but should it be expected that collective op keeps its graph-defined polymorphic behavior?
constant values for the weight also work fine, just in combination with a placeholder it seems to be problematic.
if i omit the `class_weight` parameter, training proceeds normally with constant memory.
it 's fine with either tf.constants for record defaults or plain string to decode, eg: parsed_fields = record_defaults) numpy version: numpy 1.16.0 [blas_openblas] conda-forge
optimize for inference ` --input=frozen.pb --output=stripped.pb --frozen_graph=true --input_names="sub_7" ` graph transforms --in_graph="stripped.pb" --out_graph="flatten.pb" --inputs= 'sub_7 '
this can be ok in a full-keras solution, where connection among models, update operations and so on, are managed by `train_on_batch` + `model.compile` + `model.fit` call (that do some magic in background).
... this is on a local development environment so i cannot share a link right now, but any suggestions of things to try, parameters to add, etc - would be appreciated.
4.1. the same happens with nested dictionaries.
- i have custom code, however gpu delegate part i have used as described in here provided sample - ubuntu 18.04 - samsung galaxy j5 2015 (crashes), galaxy s7 and lg g6 (works fine), - tf build from thank you guys for your effort to make this available to us, you are doing very good job!
wireguard should support the network transition and maintain or at the very least, quickly as in ipsec reestablish the vpn connection and allow traffic to pass.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
here is an example phofurl there are 2 cases shown.
user-defined subclasses of `tf.module` do not recognize and include variables from keras layer object members.
--- the previous minimal vimrc can be reduced to these 2 lines: let &t_ti = " e[?1004h" let &t_te = " e[?1004l" but if you want to avoid `e349`, you need these 2 additional statements: exe "set <s-f18>= e[o" exe "set <s-f19>= e[i" if want the code to be useful, need these 7 additional lines: fu!
large logs and files should be attached.
on a colab with tf-nightly phofhyperlink , #27507 seems fixed but this issue still reproduces
from collections import namedtuple import tensorflow as tf import numpy as np convargs = 'layerinput, numfilters, filtersize, stride, init, nameprefix ') = (none,) * len(convargs._fields) # set to false to see shape output of the final, flattened layer.
> typeerror: 'datasetv1adapter' object is not an iterator
phofcode running this twice produces: first run: phofcode second run: phofcode so even though the same random seed is used in both runs, the order of elements differs between them.
7. on "try it out" button visible.
let me know if the build time requirement is changed.
trying to save a model with a layer throws because it doesn't implement get_config().
would you like to modify the code
i expect file sizes and inference time to decrease.
however when i press `j` or `k` again focus shifts back where i 'm expecting it be.
only the first 67 ops will run on the gpu, and the remaining 10 the cpu.gpudelegate prepare: no egl error, but eglcreatepbuffersurface failednode number 77 (gpudelegate) failed to prepare.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
seems to happen when re-allocating the screen.
open a file with the following or paste it in vim.
... steps to reproduce the behavior: 1. load above yaml 2. indent `inventoryitem` so it is inline with `type: object` 4. models component crashes with error
* foo4 virtually identical to foo2, which pretty good, but why didn 't it get same magic as foo3?
if including tracebacks, please include the full traceback.
furthermore, (x @ y) and (y.t @ x.t).t produces different results.
each of the datasets i want to sample from is a tfrecord dataset.
when phofurl was added, then the full tab was highlighted by `helpnote`.
i would expect the cursor to stick to the end of a word that it 's previewing for insertion, but instead it 's just jumping around in the keyword i already had there, with some entry it won 't show me until i hit escape.
copy() actually carries out the reordering of the data in memory.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
adding visible gpu devices: device interconnect streamexecutor strength edge matrix: 0 0: n y 1: y n created 10400 mb memory) -> physical gpu (device: ti, pci bus id: compute capability: 6.1) created 10400 mb memory) -> physical (device: 1, ti, pci bus id: compute capability: 6.1) info:tensorflow:calling model_fn.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.11.0 python version: 2.7.12 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: 9.0/7 gpu model and memory: gtx 1080, 8gb env.txt: phofcode this may be nvidia 's bug as well, yet not able to test that directly.
python import psutil import tensorflow as tf for _ in range(100): input_dim=3000)]) / 2
so set my session config as follows to use the cpu which has 100gb of ram.
i think it should gives a good result
i have tested this on both tensorflow==1.14.0 and tensorflow-gpu==1.14.0 both with
multiple, until the above condition is satisfied 4. describe the error see 1
i expect no error, `@tf.function` should ensure that `super()` works normally.
@regularize.py:20] the following tensors will be regularized: idn/idn/conv0/w:0, idn/idn/conv1/w:0, idn/idn/conv2/w:0, idn/idn/conv3/w:0, idn/idn/conv4/w:0, idn/idn/conv5/w:0, idn/idn/conv6/w:0, idn/idn/fc1/w:0, idn/idn/fc2/w:0, idn/idn/idn_feat/w:0 warning:tensorflow:tried to colocate with an op tower0/cond/prod_3 that had a different device: /device:cpu:0 vs /device:gpu:0. postponing error-checking until all devices are assigned.
i'm using python to build a graph (using tf.keras.layers) that contains cudnnlstm, timedistributed, dense layers.
`python -c "while true: pass"` 5. press `ctrl + c`
you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)"
and no matter what value i set to "inter" and "intra", there are still many cpu cores were used many threads were created.
this has also been addressed in #4882 very briefly.
if including tracebacks, please include the full traceback.
the gpu should be used almost if not entirely.
instead of tf.tensor( (which is what it is supposed to print).
t [[{{node additional grpc error information: received from output shapes, got [16], but expected [64].
output is "unexpected exception, this is probably a bug: [errno 13] permission denied".
`expand("%:p")` should complete in less than one millisecond.
training steps use 100% of gpu as reported by tegrastats; validation steps use 0% of gpu and 350% cpu.
valueerror: no such layer: embeddings
and what shall i do?
gnome terminal, mintty, iterm2, tmux, gnu screen] (use gui if you use the gui.)
phofcode tested with `pip3 install tensorflow==1.13.1`, python 3.5.2 `b.txt` should have `aaa` as the content, not `aaa bb`.
__init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs) 2386 2387 -> 2388 self._function 2389 if add_to_graph: 2390 *args,
if including tracebacks, please include the full traceback.
the buffer of `./tags` is not focused.
this is a to detect objects images (yolo v3 tiny) and constated that values obtained with python code are correct ones.
tf.float32), got tf.string this warning did not shown with 2.0.0-alpha0, but does.
it should be able to handle larger tensors and batch sizes.
i spent some time extract all the static libs of tensorflowlite from bazle-out folder and linked them in my cmake.
1. run `echo 'a' > a ; echo 'b' > b ; vim --clean a b` 2.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): tf.version.version: tf.version.git_version: python version: 3.6.8 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: n/a here is the stacktrace: nameerror traceback (most recent call last) in <module> 7 return i 8 ----> 9 loop() # nameerror: name 'i' is not defined (see full stacktrace below) in __call__(self, *args,
phofurl deeplab v3 tflite model: deeplab segmentation phofhyperlink
to do this i believe i am supposed to use the buildfrombuffer() api.
`npm i 3. configure to use nodemailer in plugins > email > provider 4. create under extensions an email folder with controller (extensions > email > controllers email.js) last time i did this - last week - it seemed that the email folder was created for me.
`getbufinfo()` is not updated in accordance with various events, such as `bufhidden` and `bufwinleave`.
as per the docs, `tf.boolean_mask` phofhyperlink should accept a scalar `tf.tensor` phofhyperlink object as axis parameter.
# pb file in the below link phofurl graph_def_file = input_arrays = ["input_to_model"] output_arrays = ["concat_1"] converter = graph_def_file, input_arrays, output_arrays) tflite_model converter.convert()
got error right after the success message: task [debug]
i expect some groups of nodes / subgraphs to be converted into trtengineops so that they can run in tensorrt.
if object being used internally, a more efficient alternative might be pass it as function argument, instead of storing it internally
i also discovered that it exits normally if i pass `shared_name=""`.
inference running without any issue.
the above dockerfile, plus: phofcode `
steps to reproduce the behavior: 1. install dependencies phofcode 2. activate virtual environment phofcode 3. invoke algo (./algo)
if including tracebacks, please include the full traceback.
1. edge devices are connecting to this raspberry pi on mqtt port 1883 all failing to connect after vpn is established successfully
import tensorflow as tf import tensorflow.keras.backend as k from tensorflow.keras import model from tensorflow.keras.layers import input, lambda inputs = input(shape=(1,)) y = k.variable([0.0], dtype=tf.float32, name= 'x ') add_y = lambda(lambda x: tf.math.add(x,y)) outputs = add_y(inputs) model model(inputs=inputs, outputs=outputs) model.summary() model.summary() shows no trainable variables
this test case is modified from #23702 to include multiple model inputs.
since phofurl it is required that there not be tensor computations in a layer's `build` method.
model.predict() will return the correct predicted value for the given data.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.13.1 python version: 3.7.2 bazel version (if compiling from source): 0.21.0- (@non-git) gcc/compiler version (if compiling from source): cuda/cudnn version: cuda version gpu model and memory: titan x (pascal) 12 gb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" this issue seems different from the other hundreds (thousands?)
devices: streamexecutor device (0): <undefined>, <u ndefined> successfully opened dynamic li brary libcuda.so.1 xla service executing computa tions on platform cuda.
3. tensorboard visualisation somehow shows bert node as using 175mb.
do you wish build tensorflow with opencl sycl support?
415 """ 416 return cls(
modifying the example code given here phofhyperlink to have `stateful=true` leads to the following error: traceback (most recent call last): file "tmp.py", line 6, in <module> y = layer(x) file line 701, in __call__ return super(rnn, self).__call__(inputs,
-iproto -dhave_pathdef -dwin32 -dfeat_cscope -dfeat_terminal -dfeat_job_channel -dwinver=0x0501 /mp -dhave_stdint_h /ox /gl -dndebug /zl /mt -dfeat_mbyte_ime -ddynamic_ime -dfeat_mbyte -ddynamic_iconv -ddynamic_gettext -dfeat_tcl -ddynamic_tcl -dfeat_lua -ddynamic_lua -dfeat_python -ddynamic_python -dfeat_python3 -ddynamic_python3 -dfeat_mzscheme -i "c: program -dmz_precise_gc -ddynamic_mzscheme -dfeat_perl -dperl_implicit_context -dperl_implicit_sys -ddynamic_perl -dfeat_ruby -ddynamic_ruby -ddynamic_ruby_ver=24 -dfeat_huge /zi linking: link /nologo /subsystem:console,5.02 /opt:ref /ltcg:status oldnames.lib kernel32.lib advapi32.lib shell32.lib gdi32.lib comdlg32.lib ole32.lib uuid.lib /machine:amd64 libcmt.lib user32.lib /nodefaultlib:lua53.lib wsock32.lib /pdb:vim.pdb -debu
i would expect that using @tf.function decorator for the train function would compute the gradient update steps in a similar amount of time as model.fit().
the expectation was to get one compiled xla cluster for each replica.
when initiate the runconfig in the following code with only chief and ps, a error happends.
*this generates and saves an 84mb file* running the following opens a tensorflow debug session.
filetype indenting should not slow down redraws so much.
libffi-dev is already the newest version (3.2.1-8).
traceback (most recent call last): file "<stdin>", line 1, in <module> file line 104, in ragged_tensor_getitem return _ragged_getitem(self, key) file line 153, in _ragged_getitem row = file line 654, _slice_helper name=name) 820, strided_slice 9334, strided_slice message), none) "<string>", 3, raise_from slice index 3 of dimension 0 out bounds.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.11.0, python version: python 3.7.0 (default, jun 28 2018, bazel version (if compiling from source): build label: 0.18.0- (@non-git) gcc/compiler version (if compiling from source): gcc version 4.8.5 (red hat 4.8.5-28) (gcc) cuda/cudnn version: n/a gpu model and memory: n/a include any logs or source code that would be helpful to diagnose the problem.
large logs and files should be attached.
- os: [windows 10 1809,] - terminal: [e.g.
or report a more informative error regarding 'validation_steps ' requirement (similar to the error if steps_per_epoch are omitted for the training data).
when using: `"borderchars": [ ' ', ' ', ' ', ' ', ' ', ' ']` in popup arguments.
i have a problem, when using the file viewer and pressing "i" the list of files is re-sorted this didn 't use to be this way in older versions of vim.
the converter should run and save optimized model to 'models/mymodel_tensorrt
if including tracebacks, please include the full traceback.
` file line 23, in <module> output_data, final_state = lstm_model(e_tr1, lstm_size, keep_prob_, batch_size) file "c: users line 54, in lstm_model lstm_outputs, final_state = tf.nn.dynamic_rnn(cell, embed, file nn.py", line 664, in dynamic_rnn dtype=dtype) file nn.py", line 733, _dynamic_rnn_loop const_time_steps, const_batch_size = 904, as_list raise valueerror("as_list() is not defined on an unknown tensorshape.")
i did some digging and it turned out that the tests only pass if sharding is enabled.
only by passing the popup window option `{ 'wrap ': v:false}` to `popup_dialog()` will draw dialog correctly.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
this will hinder performance and waste ram.
use that pr and regenerate the documentation you will see them being populated at the top of the file.
see the code for details.
w0327 `eval_strategy` not passed in.
using `tf.data.dataset::cache` and will fail with `cache should only be read after it has been completed.
we expected the code to run through all data parts without issues.
1. run `gvim --u none` 2. do: phofcode
i trained `ssd-mobilenet-v1-fpn` from tensorflow object detection api with default configurations.
tf.tensor([ 0 11 0 10 9 0 0 12], shape=(8,), dtype=int32) tf.tensor([ 11 10 9 24], shape=(8,), dtype=int32) tf.tensor([ 11 10 9 36], shape=(8,), dtype=int32)
phofcode notice that `gvim` does not fork/detach from the terminal.
this model has multiple placeholders (3 ph) and the post-training quantisation process breaks because of that.
steps to reproduce the behavior: 1. setup algo and add a wifi with emoji to the on demand-exclusion list 2. try to update users afterwards 3. experience error
100) then these entries contain numbers significantly larger than zero.
copying from keras issue 11858 phofhyperlink , as requested by @ymodak.
9. repeat steps 5 and 6 and note that url is not new one.
i expect to return the same concrete function whether i pass it 1, or 2, or tf.constant(3) or tf.constant(4) as an argument.
do have h5 module installed raspberry pi: phofcode in other words, a trained rpi fails to load rpi.
the code written by me should get the same result as the tensorflow c++ code.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
the 'tf ' version returns an error, the 'h5 ' version works ok.
1. run `vim --clean tmp.vim -c "source tmp.vim"` phofcode 2. the more characters are added to `showbreak`, the wider the popup dialog window gets, and eventually exceed the window width.
8. look at the password.
- os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
since the line entered in step 2 above is longer than 80 characters and ` 'formatoptions '` contains the `l` flag, the line should not be reformatted.
opening the hdf file of the saved model shows that the input layers aren 't saved in model configuration.
we only need decorate the outermost method.
however there is no error when the embedding layer is set to trainable=false, or to use the embedding layer in a functional api.
you can see this behavior in public/good.html
large logs and files should be attached.
input index: 0. original input shape: (64, 1024).
valueerror traceback (most recent call last) in <module> 23 ds = ds.batch(batch_size=10) 24 data = ---> 25 output = m.predict(x=data, steps=1000, verbose=true) in predict(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing) 1094 # batch size.
if, still insert mode, we now hit backspace remove `n` on line 4, all is fine: we see following vim channel log: phofcode all is still well at this point.
tf gpu wont identify two gpus, only gpu 0 appears
when `matchdelete(id, winnr)` is called from another window, the match `id` is deleted but the highlighting of the match remains until a `:redraw!` or `<c-l>`.
when i apply my own (rather obscure) loss i get a
using an lstm with the keras package works great until i try to fit my data.
1. create a new strapi 3.0.0-beta.5 project with mongoose 2. add a user document 3. create an api for the post collection with the following fields: title (string), author ("has" relation towards user) 4. uncomment `afterfetchall` of post and put a `console.log(results)` inside 5. add a post document 6. perform get request on `/posts` or `/posts?title=something` 7. perform get request on
however, i now want to read the model from the assets/ directory.
after pasting a line above the current one, while `'cursorline'` is set, the previous line is sometimes wrongly highlighted by cursorline.
if applicable, copy/paste the text or add screenshots to help explain your problem.
2. have no `/etc/vimrc` and in `~/.vimrc` only have `syntax on` and `set mouse=a`.
5. narrator (screen reader) read error message when we activate "execute" button.
`tf.io.gfile.mkdir()` on linux does not throw an exception if the path exists.
if including tracebacks, please include the full traceback.
the first read works as expected.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.9.0 python version: 2.7 bazel version (if compiling from source): 0.16.1 gcc/compiler version (if compiling from source): 6.3 cuda/cudnn version: 9.0 gpu model and memory: titan xp.
first run in command line: mkdir foo; cd foo; touch a; chmod u-w a;vim --clean phofcode
set cursorlineopt to 'number' - but it still would show 'line' still.
... see the screenshots below for examples.
we find that the tensorflow jar file contains this resource file, but the file failed to extract, resulting in an exception.
----> 7 sess = 8 # runs the op.
when pressing `q` or `a` after the attention message `e325`, vim should not open the popup window.
... steps to reproduce the behavior: 1. phofurl 2. click on the "pet" resource 3. click on the "/pet" operation 4. the operation is opened, the browser url changes to phofurl 5. open this url in a new brower tab 6. the deep link works: operation post "/pet" is opened 7. go back to 2, get url of "/pet" operation 8. the (incorrect) url is phofurl 9. open this url in a new brower tab 10. the deep does not work: post "/pet" won 't be opened
steps to reproduce the behavior: 1. go to redis desktop manager 2. click on connect to connect to the redis server 3. expand the nodes and notice the child nodes 4. the keys are duplicated.
attaching a test: phofcode phofcode
provide a reproducible test case that is the bare minimum necessary to generate the problem.
instructions for updating: colocations handled automatically by placer.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
1. have installed, configured 2. set up content type with multi media 3. go to content type containing multi media 4. attempt to upload multiple images 5. view results
when deeplab phofhyperlink is run on a sample image using gpu delegate, the result is significantly different to that generated using cpu.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
when i train my model in one python file things go on well.
my model runs out of memory, but just after 3 epochs.
the issue can be reproduced on my machine, a 10 core skx core i9 follows here 's how to reproduce issue phofcode then check shard 18. see phofcode note 0 tests run.
1. c-v the first one phofurl 2. c-s i still got randomly phofcode if delete it then press c-s again, it is before cusor again.
in the toy example being used it worked correctly, although this may not be case.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
type in the code below and see no memory usage increases.
... steps to reproduce the behavior: 1. go to `sql` get and execute query without header `x-irest-conn` (`x-irest-conn` is an optional header).
when there is a textyankpost autocmd, visually selecting text can cause erroneous syntax highlighting to appear.
tf should utilize at least 80%+ gpu in my case since i'm just running inference sessions in parallel in one process, instead of having to resort to limiting ram usage and starting several processes.
python import numpy as np import tensorflow from import input, dense from import sequential from tensorflow.python.keras backend as k from tensorflow.python.keras optimizers original_dim = 160 intermediate_dim = 32 # load data train_features = test_features = np.random.rand(10000, ############# build model ############# autoencoder sequential() autoencoder.add( ) autoencoder.add( dense(intermediate_dim) ) autoencoder.add( dense(original_dim) ) autoencoder.build() autoencoder.summary() # crashes: #works: optimizer= 'adam ') # train model train_features, verbose=1, shuffle=true) ``
#!/usr/bin/env python # coding: utf-8 # in[1]: import tensorflow as tf import numpy as np from pprint import pprint import time network_architecture = { "channels" : 10, # size of z variables.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes.
it pass when saving and loading model from .h5 format.
i would like the set-up to work with tensorflow as is with theano that is to share a reference with workers to save memory.
solution: move v: variables to evalvars.c.
error: node number 199 (tflitegpudelegate) failed prepare.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): google colab gpu execution - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 / 1.10.0 / 1.9.0 python version: python 3.6.7 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: 8.0 gpu model and memory: tesla m40 24gb
- have i written custom code: _yes_ - os platform and distribution: _macos x, ubuntu 16.04_ - mobile device: _none_ - tensorflow installed from (source or binary): _from source_ tensorflow version (use command below): python version: _3.7.3 (macos) and 3.6.7 (ubuntu)_ bazel version: _0.20.0-homebrew (macos) and 0.20.0 (ubuntu)_ gcc/compiler version: (macos) and 7.3.0 (ubuntu)_ cuda/cudnn version: _none (macos) 10.0 (ubuntu)_ gpu model memory: _none geforce gtx 1080 ti (ubuntu) 11178 mib memory_ include any logs or source code that would be helpful to diagnose the problem.
failed to construct interpreter aborted can you provide the corresponding optimised pb file before tflite conversion, for the same model
detailed steps to reproduce the behavior: 1. open an empty buffer.
the moving variance should not be nan
a possible workaround might be to build a static `tensorflow_cc.lib`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - mobile device (e.g.
expected not to see `-lcanberra` since the sound feature is disabled (-sound).
so this works: let [a, b, c] =<< trim end xx x xx end but not this: fu func() let [a, b, c] =<< trim end xx x xx end endfu call func() second, the strings in the heredoc must be unpacked in several variables (see `:h :let-unpack`), and there must be a space around one of the comma separating variables.
attributeerror: module has no attribute 'attention '
i wouldn't like to open whole egress traffic just for updating users..
work normally with bazel -c dbg and bazel -c opt.
w0522 deprecation.py:323] from colocate_with (from is deprecated and will be removed in a future version.
- tensorflow version:1.12 - python version:3.6 - no gpu, and with gpu(gtx 1080ti) the model can be frozen but could not be load.
- os platform and distribution (e.g., linux ubuntu 16.04): debian 9 - tensorflow installed from (source or binary): preinstalled - tensorflow version (use command below): 1.13.1 - python version: 3.5
there should be no spurious changes in syntax highlighting when visually selecting.
i would have expected `expandcmd( '~/foo ~/bar ')` to expand the `~` anywhere in the string, just like `%`.
should provide a "try out" to get the entities from the service.
i have noticed this behavior with and i would be willing to bet most provided optimizers save some state optimization procedure run into this same issue.
significant reduce in the amount of warnings, ideally to zero.
... describe a path as written above
model should compile without errors as in pure keras
this is a reprex written in r. i 'd be happy to port to other languages if that 's preferable.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu lts - mobile device (e.g.
i 'm currently testing the avx512 builds.
follow the document to activate provider for google social sign in, then go to phofurl phofhyperlink
i updated vim today to `vi improved 8.1 (2018 may 18, compiled jun 17 2019 and now i get warnings when i open a file.
build completed successfully, 1 total action running command line: '--mean_values=128 ' '--std_values=128 ' '-info: build completed successfully, 1 total action i converting unsupported operation: i unable to determine output type for op: f check failed: getopwithoutput(model, output_array) specified output array is not produced by any op in this graph.
calling normal.log_cdf(x) with a float64 raises a typeerror exception.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
if including tracebacks, please include the full traceback.
large logs and files should be attached.
- have i written custom code: yes - os platform and distribution (e.g., linux ubuntu 16.04): macos mojave 10.14.1 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12.0 python version: 3.6
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: 2.7/3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10 gpu model and memory: k80 you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)"
... steps to reproduce the behavior: 1. start the dev server with the config from above 2. go to ` phofurl 3. observe that the pet operation is not unfolded
this time, error message is: phofcode this seems to be due to lines phofcode in code of found here phofhyperlink here phofhyperlink .
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): r1.13 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with: 1. tf 1.0: `python -c "import tensorflow as tf; print(tf.git_version, tf.version)"` 2. tf 2.0: `python -c "import tensorflow as tf; tf.version.version)"`
if the `py_function` op is defined for cpu no error raised.
here is summary by node type for 1.10: phofcode and summary by node type phofcode
hi, i have a problem with some additional syntax.
* if that 's not possible, we recommend creating a small repo that illustrates problem.
indeed, for comparison this is the mean squared error loss for training and validation: <img width="364" alt="loss" src=" phofurl
i have followed the steps from the guide, but it gives an error: phofcode i have also looked at this other issue ( issue #24592 phofhyperlink ) but the solution provided there does not apply to me.
i 've been using t2t training script using cloud tpu.
it requires the `objgraph` phofhyperlink to show the peak memory growth every iteration and generate an visualization of in-memory `tf.graph` instances.
the assignment is what actually killed svelte compiler.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
tensorflow lite can run float16 model with gpu delegate correctly.
if you replace the rnn layer by e.g.
as per the `keras` api and current documentation, the two properties should fail with an `attributeerror`.
1. download the attached example file (`base64.txt`: 128k), and rename it to `base64.json`) 2. run `vim --clean base64.json`) 3. the error occurs (see below for complete error) the error message: phofcode
"num_layers" : 6, # number of resnet blocks for each downsampling layer. }
in other words, it 's impossible right now (i think) use tf.data pipeline without minibatching.
i am using this phofhyperlink script and a few helper functions from here phofhyperlink .
the custom ops are detected and work with regular tensorflow and in distributed gpu mode with `mirroredstrategy`, but are not getting detected when using `tpustrategy`.
- have i written custom code: yes, using a custom tflite model and android app to run on my devices.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macos mojave 10.14.3 - mobile device (e.g.
please see this gist: phofurl
in the tensorflow 2.0 preview, the `tf.rnn.dropoutwrapper` and `tf.rnn.residualwrapper` wrappers are incompatible with cells from `tf.keras.layers`.
tensorflow transform throwing error indices out of bound when processing large number of records (> 1mio) but running well on small records (< 10k).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.12.6 - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 & macos 10.14.5 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.14.0-rc1 python version: 3.6.8 & 3.7.2 cuda/cudnn version: none gpu model and memory: none
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04.2 lts - mobile device (e.g.
can 't enter debugging mode with enable eager execution
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.12.0 python version: 3.5.2 bazel version (if compiling from source): 0.15.0?
the setting is simple and common for each tensorflow version.
see repro code for an example.
i recently had the requirement to code a slightly different version of the convolution layer.
example, if i create two `binaryaccuracy` metrics: phofcode whereas, if i create two `recall` metrics: phofcode after testing every metric class in `tf.keras.metrics`, i found that the same behavior as `recall` is produced by the following metrics: - `auc` - `falsenegatives` - `falsepositives` - `meaniou` `meanrelativeerror` `precision` `recall` `truenegatives` `truepositives`
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): n/a - mobile device (e.g.
1. narrator (screen reader) should read the value against the value label, which is represented in the form of asterisks, and should inform the user that value is encoded and secured.
to debug my code i have set up some runs with the following: train and val sets are the same, both consisting of 1 sample batch size is set to 1 both for training and validation learning rate is set to 0.0 so that no changes happen to the network the question was originally asked in stack overflow phofurl many thanks, michael
- os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - tensorflow installed from (source or binary): virtualenv and pip - tensorflow version (use command below): tensorflow-gpu 2.0.0-beta1 - python version: 3.6.7 gcc/compiler version (if compiling from source): 7.4.0 cuda/cudnn version: 7.6.0 gpu model and memory: gtx1660ti, 6 gb
i observed increase of cpu memory usage (and not releasing memory) when fetching big tensors with `sess.run` from gpu and also not releasing cpu memory when closing session and resetting graph.
command used: `tflite_convert ` error: tflite_convert traceback (most recent call last): file line 11, in <module> sys.exit(main()) file line 412, in main app.run(main=run_main, argv=sys.argv[:1]) file line 125, in run _sys.exit(main(argv)) file line 408, in run_main 100, _convert_model converter = 87, _get_toco_converter return converter_fn(
provide a reproducible test case that is the bare minimum necessary to generate the problem.
valgrind shows the following: phofcode
`tf.boolean_mask` phofhyperlink does not accept a scalar `tf.tensor` phofhyperlink object as axis parameter.
tensorflow preprocessing function to convert sparse to dense tensor.
calling `tf.gradients` with returns appropriately shaped zero tensors for unconnected resource variables.
1. generate libtensorflow.so and artifact for raspberry pi.
it is initialized only when doing (something like a) a `:subs` command (i.e.
the specific change is the introduction of these lines: phofurl they make the assumption that a model 's inputs will be deeper than any other node.
build a keras model, compile it, run it.
as of now, my installation (compiled from source based on yesterday 's status of the r2.0 branch) does not have tf 2.0 behaviors enabled by default, thus i compared the run-times for a supervised learning task depending on whether i `enable_v2_behavior` or simply it turns out the former yields a significant decrease in performance.
... steps to reproduce the behavior: 1. go to ` phofurl 2. open your developer tools 3. select the first operation of the 'pet store ' section with the element inspector 4. take a look at html markup of this element: phofcode
[my .h5 model and a test file, if you would like to try it yourself]( phofurl
tf-1.11.0 with mkl phofcode output: phofcode tf-1.11.0 without mkl phofcode output phofcode
2. training process gets slower after every epoch by around 1.5 times.
hi, i tried to use tf.gradeints to calculate `integrated-gradients` which is one of the explainability methods.
see in wireshark that request looks like example above.
if including tracebacks, please include the full traceback.
for sample 1, compute top 2, sample 2 compute top 1, etc.)
build succeeds on this platform.
when any value of -h, -help, -helpshort, -helpfull are provided as arguments to scripts using `tf.app.flags`, the help list should appear.
i expected the converted function to be invariant of the optimiser instance.
tensorflow 1.14 introduced a bug in keras models.
import data, create model, train, evaluate.
steps to reproduce the behavior: 1. attempt to connect to the redis server
import tensorflow as tf tf.version.version) import sys print(sys.version_info) tf_a = tf.variable(1.0) print( 'variable tf_a initialized to lr = 0.1 def get_lr(): global lr return lr tf_opt = @tf.function def train_step(): with tf.gradienttape() as tf_tape: tf_loss = tf_a
... 1. run this endpoint with the provided sample data: phofurl and notice your response will include headers.
the exception must be saliently catched: phofcode
i expect other sources of network disks may demonstrate this issue also (more extremely if latency is high enough, in fact), but have not confirmed it properly.
this makes the on-ramp to tpu training much more manageable
no leaking `platform/logging.h` and no those warnings about redefining `glog` macros.
if i transfer the model to the rpi, i cannot load it.
large logs and files should be attached.
the weights to the convolution are processed using the following equation: `w = g * v/2-norm(v)`.
- have i written custom code: no - os platform and distribution: linux ubuntu 18.0 - tensorflow installed from (source or binary): binary (from pip) - tensorflow version: i've tried 1.5, 1.10, 1.14 & 2.0 `tensorflow` and `tensorflow-gpu` binaries python version: 3.6.8 gpu model and memory: gtx 1060 3gb
the first 5 time stamps for "sensor 1" at 2hz would be something like [0.0, 0.5, 1.0, 1.5, 2.0] while "sensor 2" at 8hz would be something like [0.0, 0.125, 0.25, 0.375, 0.5]) so it 's more helpful to provide each individual feature 's timestamps alongside the feature.
large logs and files should be attached.
if `train_labels` is this: `array([[1, 0], [1, 0], [0, 1]])` and the resulting `y_pred` 's are `array([[1, 0], [1, 0], [0, 1]])` then the sum of phofcode is 6.
1. go to 'marketplace ' 2. click on 'download ' of roles and permissions 3. admin panel goes to infinite loading after server restart.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux - mobile device (e.g.
the body is raw json (application/json): "score":5.8, "predictionmodel": so i 'm clearly sending "computedmodel" as a parameter, but strapi doesn 't acknowledge it?
to fix the error just change `subcom.svelte` to: phofcode
in a quickfix window, the `quickfixline` highlighting starts from the end of the text, instead of the beginning of line.
the buffer of `./tags` is listed.
my code: phofurl master branch is the naive implementation of dilated conv1d model samples are also in the repo.
- os platform and distribution (e.g., linux ubuntu 16.04): - tensorflow installed from (source or binary): - tensorflow version (2.0): - python version:(3.6)
for this, set up instance (which, for simplification, contains random data provided code) based pre-generated numpy arrays data, and use `padded_batches` method to format my inputs as desired.
the original works fine but fails.
tf.signal.stft should work as it does without eager execution
detailed steps to reproduce the behavior: 1. have vim with the `"*` register enabled (+clipboard, +x11) 2. run `valgrind --tool=memcheck --leak-check=full --show-leak-kinds=all vim --clean + 'au textyankpost * let x=10 ' prime.c`.
i was expecting it to work the same whether i define my input layer as an input layer or as tf.random.uniform() tensor.
if applicable, add screenshots to help explain your problem.
expected behavior is that the two methods to use a model are consistent.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): os x 10.14 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.13.1 python version: 3.6.5 cuda/cudnn version: n/a gpu model and memory: n/a full stacktrace: phofcode
activating the usennapi option doesn 't really do anything performance wise aswell as setting the number of threads.
parser.add_argument( '--vali_file ', type=str, help= 'gcs location validation data .tfrecord file location. '
provide a reproducible test case that is the bare minimum necessary to generate the problem.
we suspect this is due to the program filtering files to be extracted by suffix names (.so).
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no - os platform and distribution (e.g., linux ubuntu 16.04):linux ubuntu 18.04 - mobile device (e.g.
phofcode phofcode ### tried solution 1. i 've specified the multiprocessing start method.
i understand that there may be good technical reason for this, but even if there is, i fear that users will pass python scalar values or arrays (instead of tensors or numpy arrays) to @tf.functions they won 't understand why their system blows up (slowed down by all traces new concrete functions piling up in ram all time).
my expected behavior is that tensorflow 2.0 version with savedmodel would use gpu acceleration.
in the code below, i expected to have checkpoints 1, 2, 4, 5 at the end.
the order should change for every iteration.
host: fedora 30 aws ubuntu 18.04 and 19.04 option 9 all default ipsec disabled on config
the error occurs because of converted ssd model.
i deployed algo with ten users, and then added one user by editing the config and running algo update-users.
i expect it to work as advertised.
phofcode if you do not set the `kernel_initializer`, or if you set it to `"zeros"`, everything works fine.
for this particular example, a batch size of 50 was enough to consistently reproduce the error.
the mean squared error metric should always be lesser than the loss, as it is when training the model with custom code instead of using the `fit` method.
~ cannot perform operation: mount --rbind /mnt permission denied
b) however, what is strange or rather would be considered ill advised is the warning generated by tensorflow i.e.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
however when i scanned the tensorflow code for checking how the `concat` operations work, i found that it deals mostly with int32 type.
the normal mode commands `vi '`, `vi"` and possibly other related commands malfunction with selection set to "exclusive".
* have i written custom code (as opposed to using a stock example script provided in tensorflow): n * os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 * mobile device (e.g.
original official colab: phofurl issue first reported based on this colab: phofurl simplified colab reproducing the error with a simple model: phofurl
import numpy as np import tensorflow as tf keras = tf.keras kl = keras.layers k = keras.backend bgr_shape = (128, 128, 3) #batch_size 132 #max -tf.1.11.0-cuda 9 batch_size 86 #max -tf.1.13.1-cuda 10 class def __init__(self, size=(2, 2), data_format=none,
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): google colab - tensorflow installed from (source or binary): binary - tensorflow version (use command below): tensorflow==2.0.0-beta1 python version: python3 the problem does not occur when dilation_rate ==
when displaying popups with `set completeopt+=popup` where there are long `info` parts causing line wrapping, 'showbreak ' characters are displayed with the background colour of the window below, not the popup background colour.
currently, one has to define a new highlight group with a different name and the exact same attributes as `normal` in order to use `normal` highlighting for popup windows.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no.
to eliminate the possibility that this is caused by a difference between the running mean/variance and batchwise mean/variance, a very small value of batchnorm momentum was trialed.
(check whether your graphdef-interpreting binary is up to date with your graphdef-generating binary.).
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no.
when i use ctrl-p insert-mode completion, as i repeatedly hit ctrl-p to iterate through matching items, the cursor jumps around to the length of each completed entry but the text of the entry to be inserted doesn 't show until i hit escape or tags scan completely finishes.
valueerrortraceback (most recent call last) in <module>() 7 8 if not tf.executing_eagerly(): ----> 9 1 frames in device_policy, execution_mode) 5459 5460 -> 5461 server_def=none) 5462 5463 in device_policy, execution_mode, server_def) 5538 else: 5539 raise valueerror( -> 5540 must be called at program startup.")
provide a reproducible test case that is the bare minimum necessary to generate the problem.
either target a non-quantized output format, or change the input graph contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about accuracy of results it seems that toco can not support tf.slice op ??
open strapi and you can see all content types
steps to reproduce the behavior: 1.
while using tuples for multi output model works fine, using a dictionary fails.
but this problem occurs occasionally, estimator.train works well if i have review labels in integer with 0 1 as the data in the first table.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
with and `workers=0`), `on_epoch_end` doesn't get triggered
vim crashes when resizing the terminal with a popup visible.
i want cindent to indent code like this: string[] defs = finddefs( new string[] { "postprocessing.editor", "recorder", }); but instead, it does this: string[] defs = finddefs( new string[] { "postprocessing.editor", // unwanted indentation "recorder", }); i believe this is a bug in how cino-jn is handled.
so i am not putting much stock in any discussion from these issues: - phofurl - phofurl does anyone have any pointers where i could look for solutions?
it fails with the following errors: tflite_convert traceback (most recent call last): file line 426, in import_graph_def graph._c_graph, serialized, options) # pylint: disable=protected-access input 0 of node was passed float from during handling of the above exception, another exception occurred: traceback (most recent call last): file line 10, in <module> sys.exit(main()) file line 442, in main app.run(main=run_main, argv=sys.argv[:1]) file line 125, in run _sys.exit(main(argv)) 438, run_main 122, _convert_model converter = 109, _get_toco_converter return converter_fn(
not failing with when subclassing.
steps to reproduce the behavior: 1. fresh install algo (ad blocking and ssh users enabled) 2. download mac wireguard client phofurl 3. connect to the server using the user.conf import 4.
i coded a naive version of 2d convolution using a o(n4) code just for basic prototyping.
i am using the standard structure for `model_fn` in `tf.estimator`.
large logs and files should be attached.
i'm working with our team on tensorflow.net phofhyperlink and we wanted to exploit c#'s multithreading capabilities with tensorflow.
clearing `save_checkpoint_secs` and setting `save_checkpoint_steps` 1000 now.
the parent terminal will show: phofcode the segfault seems the most common, but there are some variations.
t [[{{node traceback (most recent call last): file "advanced.py", line 84, in <module> train_step(images, labels) file line 428, in __call__ return
i want to use to check whether my code is right by step by step debugging while inference.
when i increase the repeat count from to it takes about 24 seconds.
on a large graph, this results in a low gpu usage and growing cpu memory usage for several iterations until most sequence lengths are seen.
split datasets for traning, evaluation, and testing set up `tensorforest` hyperparameters pluck feature columns from metadata dictionary instatntiate `tensorforestestimator` object define a wrapper training evaluation using `pandas_input_fn` train on data evaluate the model define a wrapper prediction make predictions on test data
computing intermediate values within a subclassed model doesn't work in graph mode.
wrapping would ideally occur at a '/' in the path and more horizontal space should be given to the summary.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: tensorflow installed from (source or binary): pip binary tensorflow version (use command below): 1.12.0 python version: 3.6.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): gcc 6.5 for custom ops cuda/cudnn version: 9.0/7.3.1 gpu model and memory: titan x with 10gb
it seems that the memory consumption of the tensorforest is relatively large?
even after 700k steps with a small learning rate, it did not convergence.
when using this submodel i notice an inconsistency between model.call() and model.predict().
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): colab tensorflow version (use command below): 1.12 python version: 3.6.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: warning:tensorflow:your dataset iterator ran out of data interrupting testing.
warning:tensorflow:from checkpoint_exists (from is deprecated and will be removed in a future version.
full logs in last cell of colab [colab]: phofurl
internal error: unexpected failure when preparing tensor allocations: d1 == d2 || d1 == 1 || d2 == 1 was not true.node number 21 (add) failed prepare.
either exception is thrown or the code is executed every time.
i am running a keras model within a custom estimator and giving the size of a hidden layer as a parameter.
when `.map` is called on a `tf.data.dataset`, some available imports are getting lost.
(check whether your graphdef-interpreting binary is up to date with your graphdef-generating binary.
we have two clusters in elasticache, each with just one node for now.
i created a custom tensorflow model and converted it tflite with `converter = [img], [out])` `tflite_model = converter.convert()` the process was fine.
much higher (mse) loss with tensorflow 2.0.0-alpha, running 500 epochs of 'sgd' using model.fit() of a 1-layer tf.keras.sequential model, implementing a simple linear regression (6 data points), when running the same code first with tensorflow 1.13.1 (performance well/as expected) and then with 2.0.0-alpha.
have .pb model shown below, followed by .tflite model: freeze phofimage tflite phofimage as you can see, conv2d layer becomes depthwiseconv2d.
trying the same code as below in other networks that has only one placeholder works as expected (although the reference implementations of quantise/de-quantise which are located at the beginning and at end of network is extremely slow, but this is another issue).
the comments don 't solve my concern.
`lan-passthrough` configuration in `/etc/ipsec.conf` is not working as expected on raspbian stretch and not connecting ssh port including other ports via local network e.g.
we have attached with this issue, the graph and the error log while trying to benchmark the tflite model.
import tensorflow as tf python import tensorflow as tf class def __init__(self, name="normalize",
* ' > .gitignore` 2. run `vim --clean` 3.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): gcp tensorflow version (use command below): 1.11+ python version: n/a bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: tpuv2
=> {"msg": "unexpected templating type error occurred on (set -o pipefail echo "{{ lookup( 'file ', wireguard_pki_path + '/private/ ' + item) }} " | wg pubkey): coercing to unicode: need string or buffer, int found"}`
ram usage should not increase with epochs.
when i use `tf.estimator` together with for single worker multiple gpus training, i meet the following error if i try to define `tf.train.scaffold` for to configure the saver parameters.
when i load the .tflite model on android it fails saying; ` caused by: internal error: failed to apply delegate: tflitegpudelegate prepare: shader compilation failed: error: 0:6: 'unknown' : not a legal layout qualifier id` what exactly does this error mean?
is saved as padding is valid.
you can reproduce a similar memory growing behavior with the flower example phofurl when i run it on gcp ai platform notebook with tf 1.14 and nvidia tesla k80, with multiprocessing on, i.e.
skipping registering gpu devices... device interconnect streamexecutor with strength 1 edge matrix: 0 0: n w (one-time warning): not using xla:cpu for cluster because envvar was not set.
it should display last or next shell cmd when press up&down key on shell
`yarn build` (if not quickstart) 3. enter phofurl 4. see login screen instead of register
here is the profiling information at peak: 100.00% (page allocation syscalls) mmap/mremap/brk, --alloc-fns, etc.
it seems official is using a newer kernel for depthwiseconv2d and hence runs significantly faster compared to our trained model.was official trained or optimised using tf2.0 tools?what could be done to achieve similar speed (depthwiseconv2d) using tf 1.13.0 or do we have migrate and retrain our tf 2.0?
tf.gather throws error in presence of another independent tensor inside tf.function
provide a reproducible test case that is the bare minimum necessary to generate the problem.
3. while model is running and after 15-20 min this happens.
phofcode python # eager mode import tensorflow as tf x = tf.random.normal([5], seed=1) for i in range(3): print(x.numpy()) # output # ] # ] ] ``
this needs be done in scripts run_preprocessing.sh and run_training.sh 4. start ./run_and_time.sh.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): 14.04 or 16.04 - mobile device (e.g.
i expect `tf.make_ndarray()` to return a numpy array as advertised :)
when a `sequential` keras model contains `inputlayer` and it is saved, it cannot be loaded and fails with a message phofcode
this may consume large amount of memory.
moreover, if i add some redundant print statement inside converted_call, it prints nothing.
i 'm not sure why this is happening, and any suggestions about what to look for would be great
i can use batch renormalisation under mirrored strategy.
tflite should output the same result with pbfile when using only one convolutional layer.
flops should be linear in batch dimension
--- one could argue that you still squash a non-focused window by maximizing focused (`:wincmd _`); but that 's an indirect way achieving desired result, which have other side-effects (i.e.
in tf 1.14.0 the module spec in `tensorflow.__spec__` is none: phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): '1.13.1 ') python version: 2.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.0 gpu model and memory: tesla p100, 16g you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
note that this only applies to buffers which are _currently_ displayed, in a window the current tab.
by chance, i noticed that some output of conv int8 is unexpected.
1. create an embedsmany relation where the embedded model has a composite id as described by: phofurl 2. add an array of embedded objects where one (e.g.
using the following `dockerfile`: phofcode then follow these steps: phofcode then within the container: phofcode which will fail with: phofcode you will notice that in this setup it is possible to run `xterm`: and just to demonstrate that `vim` itself works:
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i am trying to load a model (hdf5) which has a keras custom layer.
in addition, this small python script shows the same results: phofcode
can you please take a look?
expected behavior according to my understanding is that as soon as
this code (as a file `zero_out.cc`) is taken directly from ` phofurl phofcode compile the op with the commands: phofcode run the op: phofcode this prints the following: phofcode
when runnig codes below it show errors phofcode
i successfully froze the graph and was able to convert the non-quantized 32-bit-float version.
tensorflow version (use command below): phofcode python version: python 3.6.8 bazel version (if compiling from source): not applicable gcc/compiler version (if compiling from source): since python was compiled source via `pyenv install`, including compiler version: phofcode cuda/cudnn version: not applicable gpu model and memory: not applicable other information phofcode python environment is managed with `pyenv` version.
`sparseapplyrmsprop`, operations), which results in an error.
`invoke_listeners` does not seem to compute the total number of lines added, just the last: for (li = li != null; li = li->li_next) { ... t added = (char_u *)"added"); }
:mf command in netrw doesn't select files with glob patterns
in keras 2.2.4, fit_generator simply calls the next(gen) function on the generator provided to fit_generator().
3. click on `public` 4. check the box next to the newly created route 5. try to access the route in the browser.
run the test code, the program throws abortederror, info is: phofcode
in that case a correction of code would be useful for me and other people as well who had update their code as * write_grads * parameter has been removed from tensorboard callback in version 2.0.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
if applicable, add screenshots to help explain your problem.
are graph-compatible, wrap call using original error: get
bur when running the same code in my local machine (gtx 1080ti) with the same tensorflow docker image.
running phofurl with the following options: phofcode or (the error message is all the same) phofcode
the exception appears in tf_loadlibrary.
i have an online staging site.
lightly edited example using canned estimators: phofurl
inference with the trained model gives phofcode a solution is to make sure that number images is multiple `batch_size`.
if none of gpus is available, it could just return the empty list, not crushing itself.
2. run in another terminal window: `vim --clean -c "set 3. type `foo` and press `<c-w>}` with the cursor on `foo`.
sess_; sess_options_; tensorflow::graphdef graph_def; auto default_env = model_path, &graph_def); run_model();
this error appears: phofcode a similar error appears when i try to use cudnnlstm
a way to achieve this would be to disable `mouse_xterm` by default when `clipboard` is disabled.
metric values should not get mixed up
def __init__(self, latent_dim=32, intermediate_dim=64, name='encoder',
they suspect it 's a bug with the callback or me misusing `steps_per_epoch` somehow.
when a large sparse tensor is reshaped using `sparse_reshape` on windows it fails, due to the fact that it uses `np.prod` to determine whether the number of elements in the old and new sparsetensor are the same.
a clear and concise description of what the bug is.
i 'm training a 3d u-net model with a tensor size of in order to develop models for brain mris.
then it casts targets wrong dtype, causing errors.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
1. create a content type with a boolean field for example `enabled`.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): tf.version.version == tf.version.git_version == python version: 3.6.8 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: i know i could use instead: phofcode but using stateful metrics should be possible.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macos - mobile device (e.g.
private boolean ismodelquantized; // float model // number of threads in the java app private static final int num_threads = 4; // config values.
when vim is run with the `-t` argument, the first call to `:argedit` with a single file does create a new buffer with the, but the new buffer is not focused.
it shows `test...` and after one second `test...done`.
1. write these lines in `/tmp/vimrc`: let &t_ti = " e[?1004h" let &t_te = " e[?1004l" exe "set <s-f18>= e[o" exe "set <s-f19>= e[i" fu!
the width changes as wrapped lines show up during scrolling.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04.2 lts - mobile device (e.g.
as noted in comments, the bug disappears if we don't use `tf.function`, or set `persistent=false` in gradient tape.
but it 's crucial that i be able to view logging as the model 's training.
phofcode this snippet of code is a simplified version of this colab notebook phofhyperlink .
this issue originates from trying to serve a tensorflow model containing to check the occurrence of a word in a vocabulary.
now we trigger the omnifunc with `<c-x><c-o>`; in the context this results in a single completion result, `main`, which duly completes to give us: phofcode the sequence from vim channel log corresponding to this is: phofcode ~note:~ * ~two callbacks to single `listener_add` callback~ * ~each called with multiple, identical changes~ ~otherwise,~ all is well at this point.
then console.log the ctx parameter from after save hook.
phofcode when i ran it on my tensorflow-gpu, here is the result: bug phofimage looks like it stops computing halfway through and gave the rest 0 as a result.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):yes - os platform and distribution (e.g., linux ubuntu 16.04):ms windows10 x64 1809 build17763 - mobile device (e.g.
if torch is imported before tensorflow, tensorflow is unable to use the gpu.
but on linux, it gives a null pointer.)
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): source tensorflow version (use command below): i don't use python since building tensorflow lite python version: see above bazel version (if compiling from source): see above gcc/compiler version (if compiling from source): gcc (rev2, built by msys2 project) 8.3.0 cuda/cudnn version: no gpu model and memory: no see above i created pull-request phofurl
- vim version (current latest head) - os: macos 10.14 - terminal: iterm2 this resembles #4888 but different report (because this reproduces with replacing map with filter also reproduce the crash.
- os platform and distribution : linux ubuntu 16.04: - tensorflow version 1.13: - python version: 3.5 - cuda/cudnn version: 10.1 gpu model and memory: gv100 32gb
`tensorboard --logdir "s3://[bucket]" --inspect` error log: `no event files found within logdir s3://[bucket]` i the operation failed and will be automatically retried in seconds (attempt 1 out of 10), caused by: failed precondition: not a directory`
the expected behavior would be that the last 2 dimensions don't swap which is the case for all conv2d layers.
the exception is never raised if the dataset does not contain string data, iteration stops and the rest of the code is executed.
once the files are copied the filenames are yielded by the generator function so they can be read in a reading function that i am trying to map.
i git-bisected the repository and the bad commit was; is first bad commit commit author: bram moolenaar <bram@vim.org> date: thu aug 29 2019 +0200 patch code for handling v: variables in generic eval file problem: code for handling v: variables in generic eval file.
thus, when specifying `validation_steps` < `validation_dataset_size / batch_size`, then every evaluation will be performed on a different set of examples.
first explanation: vim temporarily focuses the bottom window; and the cursor must be drawn somewhere, so the height of the window is set to `1`.
task [wireguard : generate public keys]
found some theoretical workaround (collected update operations `model.updates` + collect inputs `model.inputs`, loop over inputs, feed correct input execute with sess.run updates), but those are really ugly they don 't work: can trigger parameter update, but time-step of execution is wrong solution does not converge: moreover, when model becomes complex (like in example of bigan below) it can be real mess code become unmaintenable.
either the save_model should always work (i believe this is a feature goal) and the documentation should reflect this, or if the save is likely to produce incorrect results it should raise an error and the documentation should continue to suggest that custom models can only be saved with save_weights feature.
the above solution (using `call` instead `__call__`) also applies here.
the current `conv2d` and `dense` layers in the `tf.keras` package have `glorot_uniform` as the default kernel initializer which doesn't play well with advanced activations like `relu`, `prelu`, `selu`, etc.
a mini-batch contains 32 samples.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
* / 1/ ' +x /tmp/log vim /tmp/log i found this list: - `bufenter` - `bufnewfile` - `bufwinenter` - `winenter` `winleave` is not included, which probably explains unexpected status line earlier.
the linking process should be performed successfully and produce a library file called zero_out.so.
we are using this benchmarks repository phofurl inside the container.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.14.1 - tensorflow installed from (source or binary): pip - tensorflow version (use command below): 1.12.0 python version: 3.6.7 |anaconda, inc.| (default, oct 23 2018, cuda/cudnn version: n/a gpu model and memory: n/a see code above.
large logs and files should be attached
however, when we gather apply gradients with respect even though our model has one unique weight, kernel for `dense`, will return two variables, gradients will be calculated thus applied twice, which i would say is bug, insofar as it 's unexpected.
- os platform and distribution (e.g., linux ubuntu 18.04) - tensorflow installed from (binary) - tensorflow version (1.13.1) - python version none.
when converting keras model to estimator it converts all integer inputs to floats or doubles.
large logs and files should be attached.
should broadcast the same value to all devices?
output of above code on p3.8: trial 0: ips trial 1: ips trial 2: ips trial 3: ips 4: 5: 6: 7: 8: 9: overall average: meanwhile, %cpu (observed using the top command) is on p3.16: 0: 1: 2: 3: 4: 5: 6: 7: 8: 9: overall average: %cpu is between
3. type `:run<cr>` with focus on `evalfunc.c`.
phofcode run with the path to the attached file as a command line argument.
error: unsupported type for iota t [{{node = _retval[t=dt_float, index=9 phofhyperlink ]] traceback (most recent call last): file line 1334, in _do_call return fn(*args) file line 1319, in _run_fn options, feed_dict, fetch_list, target_list, run_metadata) file line 1407, in _call_tf_sessionrun run_metadata) error evaluating input 0 as a compile-time constant.
i keep hitting strange dimension issues, where tf complains about none.
the issue in this scenario is that it seems execute entire call routine print summary which essentially means that takes 980 seconds just print summary!
tried a couple of times.
import tensorflow as tf import numpy as np class def __init__(self, *args,
i looked for the corresponding source code and found the following snippet: phofurl the problem seems to be that conv2d weights have a 4d shape [h_kernel, w_kernel, c_in, c_out], which is not intended as convolutional layers case in the above code.
i am unable to restore the weights of any of my tf.keras models only when restoring from a new initialization of the model.
the balloon to remain on screen until the mouse is moved, per the reported gvim behavior.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution: colab or wsl ubuntu 18.04 - mobile device if the issue happens on mobile device: - tensorflow installed from: binary tensorflow version: 1.13.1 or python version: 3.6.8 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory:
it should be directly copy-paste-able phofcode if you flip the `custom_training` variable between `true` and `false` (line 72) you 'll see what i mean.
defs = finddefs(root_folder, new { "postprocessing.editor", // unwanted indentation "recorder", }); i can work around it by starting the initializer on a line: defs finddefs(root_folder, });
basically just as it is now, without increasing used memory every single time i do the inference.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): pip virtual env tensorflow version (use command '1.11.0 ') python version: 2.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: geforce 940mx
phofimage - vim version: phofcode - os: arch linux - terminal: termite, kitt
when calling `model.fit(dataset, epochs=2)` with a finite shuffled dataset, the model is trained on the same dataset order at each epoch.
see animated gif below: tf_prefetch phofimage the actual dataset is filtered in pipeline training stalls whenever sequence of values that is filtered out is occurring in data.
this breaks all code that loads the model and relies on the correct names.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 python version: 3.6/2.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: cuda 9.0.176 / cudnn 9.0 gpu model and memory: geforce gtx 1060 6gb the package `nvidia-modprobe` is already installed.
when `run_eagerly` is set true the code will eventually get this phofhyperlink line and that 's where the code fails, as keras 's optimizer interface doesn 't have `apply_gradients` method defined
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): macosx 10.13.6 - mobile device (e.g.
i have added some codes to configure cluster in this file as follows phofcode for the ps server, i have done the similar things.
import tensorflow as tf input = = [none,none,3]) output = model = # the model consists single 1x1 convolution.
i 'd expect the error to be the same in case of as in case of division as it turns out to be the dtype issue.
i use openssl s_client command checked the connect to the mnist url.
1. run `vim --clean` (or `gvim --clean`, etc.)
gif phofimage in the gif, `c-n` is pressed 210 times for about 5 seconds.
under tensorflow r1.13 my single node multi-gpu training code produces a phofcode error when constructing the model, before a single forward pass has happened.
or am i just going crazy.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
tf1.14 is working good and uses gpu as it supposed to, but tf2.0 runs on cpu.
1. run `:set spelllang=sr@latin spell` 2. vim enables spellcheck for serbian latin
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04):linux - mobile device (e.g.
traceback (most recent call last): file line 297, in stop_on_exception yield file line 465, in run self.main_result =
should be able to run the inference using the gpu delegate on ios with the same model that worked on android
but using it from normal python code, it works without any problem.
also have tried build new demo code in bazel, which perform correctly.
i would expect tf 1.11 to be as fast or faster as tf 1.10
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): osx - tensorflow version (use command below): 1.13.1 - python version: 3.6 results of the scripts: phofcode
gitter.im report: krzysztof szromek @szromek sep 26 06:21 hi guys!
large logs and files should be attached.
in 8.0, jumping to the middle of a large file is still slower with `filetype indent on`, but not so slow it exceeds redrawtime.
i defined the input to accept variable sized grey scale images (none, none, none, 1).
workers need restart training if any fails.
`from import input, lambda, conv2d, reshape, timedistributed` `from import *` `import tensorflow.keras as keras` `a = input(shape=(12,), dtype = 'int32')` `# a.shape == (none, 12)` `d = reshape((-1, 2, 2))(a)` `# actual result d.shape == (none, none, 2, 2)` `# expected result d.shape == (none, 3, 2, 2)`
a clear and concise description of what you expected to happen.
a warning is logged and the code continues to execute.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
text selection with the mouse does not select text to the end of the line, if only selecting text within a line.
for example, following will fail: @tf.function def has_init_scope(): my_constant tf.constant(1.)
`tf.nn.relu` should handle `nan` inputs from all sources consistently.
i can train and save the model 's weights just fine.
the expected behavior is that it would finish training.
it works just fine in `1.14.0`, `2.0.0b0` & `2.0.0b1`.
commenting out `set guioptions-=t` results in a near instant start up.
(see screenshot 4. in ui try to add new quest
provide a reproducible test case that is the bare minimum necessary to generate the problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): yes tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: here the full logs: attributeerror traceback (most recent call last) in shared_name) 1852 # some datasets (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - tensorflow installed from (source or binary): pypi - tensorflow version (use command below): 1.13.1 python version: 3.5.2
i have created a simple regression model and trained it using keras and also using a custom training loop.
for instance layers or other nn abstractions) tensorflow fails with the error: i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 fma i cpu frequency: hz i xla service executing computations on platform host.
large logs and files should be attached.
if including tracebacks, please include the full traceback.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i got different loss values whether my last layer was: phofcode or phofcode if -in the first case bce do apply sigmoid before computing loss.
in each of following test, i run test program for an adequately long time and use `tf.assert_equal` to capture bug.
this does not work: phofcode snippet of the error: phofcode this works: phofcode
if including tracebacks, please include the full traceback.
large logs and files should be attached.
not any character in the new file
ideally this would just work?
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): tf 1.12, tf 1.13, tf-nightly python version: 3.6.7 cuda/cudnn version: not applicable gpu model and memory: not applicable with eager execution it works correctly
the `model.summary()` method does not display the
this inconsistency does not happen for all samples but happens brokenly.
- tensorflow version: 1.4 - programming language: c++ - operational system: windows 10 - cpu implementation the erros are both bellow: `check failed: dtype() == expected_dtype (9 vs. 1)` image phofimage
should i be using the v1.0.zip instead?
* when using a "built in" loss function, the "accuracy" metric is
instructions for updating: colocations handled automatically by placer.
run this powershell script phofhyperlink 2. run vim.exe 3. exit
detailed steps to reproduce the behavior: 1. run `vim --clean` (or `gvim --clean`, etc.)
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
phofcode provide a reproducible test case that is the bare minimum necessary to generate the problem.
the expected result is `a_sqrt_2`.
when i use tf2.0 in another conda environment, it gives me: phofcode
phofcode provide a reproducible test case that is the bare minimum necessary to generate the problem.
the formula to compute the trace of a matrix with `einsum` computes instead the sum of all the elements of the matrix.
thus checking type if is int8 and has dequantize layer here seems not make sense?
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - tensorflow version (use command below): 1.12.0 - python version: 3.6.7 cuda/cudnn version: 9.0 gpu model and memory: titan v because the model was way too big, i couldn't serialize it to .pb file, so that i had this error: `[libprotobuf error exceeded maximum protobuf size of 2gb:
phofurl ng: phofurl head ok: phofurl head > $ git checkout -f bug > $ make input_data.h > $ make > $ ./hello_world # ng git checkout head make ./hello_world # ok
since input here is keras.input, name_scope should be applied to keras graph instead of default graph
file line 3496, in _fused_batch_norm message), none) file "<string>", line 3, in raise_from could not find valid device for node.
each of the 4 gpus should indicate that memory is being copied to the device and processed.
i expect kl_divergence(x, x_pred) in my code to return a scalar, and no out of memory errors.
import tensorflow.keras as keras from tensorflow.lite.python import lite, interpreter #define model that uses upsampling 2d and depends on it 's output changing with the input size def demomodel(): inputs = 3], name="input_1") a = b = c = inputs]) return outputs=[a, c]) #convert to tflite model demomodel() converter #run the tflite model inter inter.allocate_tensors() ``
it is unusable when images are requested.
repo url: phofurl * if that 's not possible, we recommend creating a small repo that illustrates the problem.
the conv nn should have worked just fine
specifically, during very first ~3k steps, gpu volatile-utilization was between 60% 95%, cpu utilization was only around 8%.
passing highlight group that is linked to `normal` doesn't work either.
then i save my model as a pretrained on as `grapg.pb` format and try to reuse it in another python file.
here is a notebook to reproduce the problem.
i sent some data from one model 's loss function to another model.
however, if you take a strided slice over the result with `axis=-(r+1)` or `axis=r`, the arithmeticoptimizer will fail with a warning.
conv3dtranspose layer is producing an error message when the input layer dimension size is given none.
empty requestbody is ignored, description of path is avaliable.
`adam = `adam.learning_rate = # this does not update learning_rate, but changes python-typed ` `hyperparameters in "_hyper" dictionary to tf.variables` `adam.learning_rate = # this finally updates the tf.variable ` calling adam._hyper before update yields: `{ 'learning_rate ': 0.001, 'decay ': 0.0, 'beta_1 ': 0.9, 'beta_2 ': 0.999}` calling adam._hyper after first update yields: `{ 'learning_rate ': <tf.variable 'learning_rate:0 ' shape=() dtype=float32, numpy=0.001>, 'decay ': <tf.variable 'decay:0 ' shape=() dtype=float32, numpy=0.0>, 'beta_1 ': <tf.variable 'beta_1:0 ' shape=() dtype=float32, numpy=0.9>, 'beta_2 ': <tf.variable 'beta_2:0 ' shape=() dtype=float32, numpy=0.999>}`
on cpu, `tf.transpose()` can be 10 times slower than depending on the dimensions of the tensor.
i am using a `tf.data.dataset` with the image urls as content.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 python version: python 3.6.5 :: anaconda, inc. bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: n/a gpu model and memory: n/a you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" 1.12.0 include any logs or source code that would be helpful to diagnose the problem.
i expect behavior not to differ from when providing arrays or lists of arrays as input.
5. hit the `s` key a few times to cycle the sorting order (current sorting order can be seen in banner).
check out this minimal repl - phofurl in safari (12.1.2), select the input control in the middle and make any sort of edit.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - mobile device (e.g.
1. run `vim -nu none` 1.
the reason for this issue is fact that definition of `std::function` has changed between gcc4 and gcc5.
the loss produced is "nan" (it should be around 0.3).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac 10.14.4, ubuntu 18.01 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): | 1.14.0rc0 | 1.14.0 python version: 3.6.8 cuda/cudnn version: gpu model and memory: phofcode
2. now using nodejs or any other language, get the authentication token 3. now, send a post to the content that was created.
compilation error due in file included from from from from fatal error: wchar.h: no such file or directory #include_next <wchar.h>
when using embedding layer inside a subclass model with variable length between batches, during training (model.fit), errors out as expected list of [batch_size, previous_batch_seq_len], received [batch_size, current_batch_seq_len].
file is saved, with "?"
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the script doesn 't finish.
`echo &signcolumn` outputs 'auto ', no signcolumn 2. :so % 2. signcolumn show up
if no, both model should not accept dict input and vice versa.
tf_bug.zip phofhyperlink basically the issue is a recursion issue, tf does not understands the recursive nature of foldl op.
4. testheader input field doesn 't show the default value "test-value".
devices: streamexecutor device (0): geforce rtx 2080, compute capability 7.5 cpu frequency: hz xla service executing computations on platform host.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6.0 the exception: file line 98, in __init__ self._func = module_v2.load(handle) file line 80, in load return file line 324, in load export_dir) file line 63, in __init__ 100, for node_id proto.bound_inputs] 100, <listcomp> for node_id proto.bound_inputs] 117, _get_tensor_from_node return obj.handle 317, __getattr__ return getattr(self.get(), name) attributeerror: object has no attribute 'handle ' ``
when i try to save a function of tf.module as saved_model that calls another function with different input shapes, it fails with the following error: w0717 save.py:129] skipping full serialization of object <__main__.outer object at because an error occurred while tracing layer functions.
i am interested in seeing the profiling information for each of the layers/operations and i am able to do that by adding the enable_op_profiling=true flag.
now, if reverse control dependencies from benchmark script as follows: `for i in reversed(range(0, len(grads), 1)):` then see that gradients coming into dummy op are different for some of layers, and they are random from run run.
expecting it to be faster on gpu
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
info:tensorflow:loss = step = 1 info:tensorflow:saving checkpoints 2 into tmp/model.ckpt.
i run into the following error when trying to run a tflite model.
training parameter should be true in training and false in inference
there are also no abnormal temperature or other sensor readings in diagnostics when this happens.
python import time import numpy as np import tensorflow as tf tf.enable_v2_behavior() d = 2 n = 22 # note: tf starts getting a lot slower than numpy at n=9, it seems.
the following code shows the issue.
if including tracebacks, please include full traceback.
it is expected the warning is written to stderr.
- os platform and distribution (e.g., linux ubuntu 16.04): centos 7.4 - mobile device (e.g.
the generated `.desktop` files in `runtime/` pass `desktop-file-validate`, and this is verified when they are generated and committed
below you can find the simple model i 've built to reproduce this issue.
python3 # works fine import tensorflow as tf # none of the following work; they all produce variants of the aforementioned error from tensorflow.keras import layers from tensorflow import keras from tensorflow.python.keras import layers from dense tensorflow.python keras ``
if there is any efficient alternative way of doing this (hopefully yes, as indicated by the warning message in the code "conv2dbackpropfilter uses a while_loop.
vim --version : vim - vi improved 8.1 (2018 may 18, compiled apr 14 2019 ms-windows 64-bit console version included patches: 1-1023 compiled by kerwin@xuhengxiao huge version without gui.
evaluation of `tf.nn.conv2d` in a `tf.data.dataset.map` call fails during `session.run()` call of with error: phofcode see the full error log and the attached code below for the full example.
vim should open the url in a web browser.
- os platform and distribution (e.g., linux ubuntu 16.04): archlinux - tensorflow installed from (source or binary): binary - tensorflow version (use command below): - python version: 3.6 cuda/cudnn version: 10 gpu model and memory: 1080 ti
<esc>set ts=2 5. insert '<tag> ' 6. insert a newline 7. observe: indent for the line is 4, it must be 2 8. insert a "<" 9: observe: indent for the line is 4, correct 4. describe the error
i want to use mirrored strategy parallelize training by splitting batches across gpus.
or at least it should be possible to close the empty popup window with `<c-w>z` or using the mouse.
osc-52 escape codes from `:terminal` should be propagated to the terminal vim is running in.
the issue was resolved after changing this line phofurl by removing if statement and calling convert_to_tensor() unconditionally.
large logs and files should be attached.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 1809 - mobile device (e.g.
unfortunately i cannot provide my .tflite file, but it obviously contains a 'mean' operation.
while training a model with `tf.keras.layers.lstm` and having in callbacks, `model.fit` stops with an error message at end of last epoch, and no model weights is saved as `modelcheckpoint` should do.
when using `from_generator` to create a `tf.data.dataset` instance, reading from a one_shot_iterator and not explicitly closing the session object, i observe a warning: phofcode
- why do i get different outputs android?
@model_utils.py:64] trainable variables: name shape dim idn/idn/conv0/w:0 [3, 3, 128] idn/idn/bn0/gamma:0 [128] 128 idn/idn/bn0/beta:0 [128] 128 idn/idn/conv1/w:0 [3, 128] idn/idn/bn1/gamma:0 [128] 128 idn/idn/bn1/beta:0 [128] 128 idn/idn/conv2/w:0 [3, 256] idn/idn/bn2/gamma:0 [256] 256 idn/idn/bn2/beta:0 [256] 256 idn/idn/conv3/w:0 256] idn/idn/bn3/gamma:0 [256] 256 idn/idn/bn3/beta:0 [256] 256 idn/idn/conv4/w:0 idn/idn/bn4/gamma:0 idn/idn/bn4/beta:0 idn/idn/conv5/w:0 idn/idn/bn5/gamma:0 idn/idn/bn5/beta:0 idn/idn/conv6/w:0 idn/idn/bn6/gamma:0 idn/idn/bn6/beta:0 idn/idn/fc1/w:0 [57600, idn/idn/fc1/b:0 [1000] 1000 idn/idn/bn7/gamma:0 [1000] 1000 idn/idn/bn7/beta:0 [1000] 1000 idn/idn/fc2/w:0 [1000, idn/idn/fc2/b:0 [1000] 1000 idn/idn/bn8/gamma:0 idn/idn/bn8/beta:0 idn/idn/idn_feat/w:0 [1000, idn/idn/idn_feat/b:0 total #vars=31, size=236.37mb @base.py:208] setup callbacks graph ... @argtools.py:146] "import prctl" failed!
provide a reproducible test case that is the bare minimum necessary to generate the problem.
if you want xla:cpu, either set that envvar, or use experimental_jit_scope to enable xla:cpu.
the issue described there seems to be related, but not identical to this one.
currently i am using distributed tensorflow to connect two machines one with only a cpu(ps) and one with a cpu and gpu(worker), the worker machine runs only the cluster and server commands followed by a join command and then remains idle, the training algorithm is run on the ps machine after defining cluster server then training is run but i get "e not found: tensorflow device gpu:0 was not registered" between each epoch of training it is not run on gpu (the delay is very big)
the functions that calculate the flops statistics phofhyperlink do not seem to distinguish between floats and complex numbers.
i tried to test some autograph 's features with rnn.
if including tracebacks, please include the full traceback.
`redraw` does not affect the behavior.
large logs and files should be attached.
when called, it gives the error as ` typeerror: reduce() arg 2 must support iteration`.
ix evpzd" \' -o /tmp/.a.vim /tmp/.b.vim
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): (debian-derived) - mobile device (e.g.
in this sample code, i have one dataset (range between 0 and 7, batch of 1, 1 epoch), i train the model for 4 steps, stop it, restart for 4 steps the training by restoring checkpoints (model and input).
when following the transformer walkthrough phofhyperlink there seems to be an issue with the newstest2014.de 'download and preprocess datasets ' step the newstest2014.de doesn 't appear to be in valid text format.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
sample tf_config of parameter server: index and type varies for chief, worker and evaluator on other tf_configs.
to evaluate the compiled and the non-compiled graph prediction of a single image duration, add these two lines to the end of notebook: `%timeit -n 20 predict(x)` `%timeit -n 20 model.predict(x)`
the 64 are uint16_t while 1s are int 4 bytes.
variables of inner `tf.keras.dense` layers should have different names, such as: `dense/kernel:0` `dense/bias:0` `dense_1/kernel:0` `dense_1/bias:0` right now they have the same name: `kernel:0` `bias:0` `kernel:0` `bias:0` this is problematic for saving / loading parameters.
here i provide a reproducible test case.
`popup_create()` uses `number` and `cursorline` settings of the current window and displays both in the popup window.
:version vim - vi improved 8.1 (2018 may 18, compiled may 15 2019 included patches: 1-1332 huge version with gtk2 gui.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary):pip3 tensorflow version (use command below): 1.13.1 python version:3.5.2 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 7.5.0 gpu model and memory: geforce gtx 1060 5gb include any logs or source code that would be helpful to diagnose the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): google colab - mobile device (e.g.
detailed steps to reproduce the behavior: 1.
21. the name property focusable element 'curl ' must not null.
to do that, i have to treat ~30,000 x ~30,000 matrix(pred) like the following code.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
phofcode macos 10.14.4 iterm2 i am working on a test to repro this and will look at a fix if i can.
the code executes normally, but raise valueerror when computing gradients (`tape.gradient)` if i decorate the training function with `@tf.function`.
the warning disappears if use nightly tensorflow docker images, but model training still slow.
phofcode there are no errors reported.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
vim/gvim cannot use a shell command if the shell variable includes a space.
loading a model with produced with , and resuming training produces different results from running the training without save model + restore model in the middle.
see repository phofurl (the tflite part is in when running the app, it prints the ratio of zero values in each output to the android log.
if including tracebacks, please include the full traceback.
at at at source:0) at method) caused by: internal error: failed to apply delegate: next operations are not supported by gpu delegate: mean: operation is not supported.
error: node number 199 (tflitegpudelegate) failed to prepare.
i am trying to add adversarial training to my `tf.keras` model.
when doing inference using tf.keras batchnormalization, for a given input sample x, the output is dependent on the other samples in the batch.
you can also try removing the `signatures` argument in the `tf.saved_model.save()` call, it will fail.
actually, moodel behaves different with different selection of "model_creation_device".
1.15.0-rc0 0.0, # different dropout mask was used for each call # same dropout mask was used for each call ``
in tensorflow 1, with same code as above, but a tensorflow 1 friendly model compilation step do not see same issue, number of steps is as specified, whether data is passed in a dictionary or not.
install tensorflow and tfx in python 3.7 import tensorflow as tf import tensorflow_transform as tft import as tfma
is this due to the way feed dictionary gets copied from python to c++ ?
at first, i tried tensorflow 's official example phofhyperlink with python 3.6 colab notebook with tensorflow version as r1.12.0.
1. run open -a macvim a-folder 2. see the list order 3. press i in normal mode to see file details 4. see how the list is now re-sorted 5. cry, baby, cry 6. find your file again in the list (no, it isn 't under the cursor, why would it) 7. get used to list being re-sorted (this is hard) 8. accept is re-sorted (this is harder) 9. be frustrated 10. stop using vim altogether, because, omg.
phofurl requirements: visual studio 2017+ happens both in debug or release builds.
1. it is much slower (~5x) create datasets in graph mode as opposed eager mode.
import logging import tensorflow as tf formatter = %(asctime)s >> %(message)s ') stream_logger = # set handler streamhandler = logging.streamhandler() world") stream_logger.info("this is message 1") ``
i used the tf code from the tflitecamerademo example.
the code below is from the colab tutorials for tf2.0 in boosted tree models.
warning:tensorflow:from colocate_with (from is deprecated and will be removed in a future version.
contiguous lines are combined in a single `<p>`.
the code in second time fails, because it is new obj , so there 's no cache in self._embedding_weights, then will run with no reuse: phofcode
the objective is to run and get the prediction result.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
mailed with @cecamp , just raised up a new ticket to record this.
i eventually discovered that the optional chaining will transpile into code with in expression assignments.
inputimagebuffer new creates the output tensors and its processors.
_*laravel is a php framework._
in this example the error pops up when n is 8193 or higher, no error when n is 8192 or lower.
when taking the gradient respect to a variable and reduce mean the outcome should be 1 - (1/(# of reduced dimension)) however it does not happen.
when running vim inside xterm with `'noesckeys'` set, typing a capital letter in insert mode does immediately exits insert mode without entering the character.
note that, i have already used
instead it will try to assign the value directly to the property.
somehow, my ipv6 was partially disabled in /etc/sysctl.conf (i found out because my wg-quick was failing and fixed it with these lines: phofurl this meant that setting up the first time would work but then updating the user list would always fail, and this time not because of missing headers or so.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
large logs and files should be attached.
currently, i 'm working around these mouse woes in my vimrc file: phofcode however, i 'd be willing to try to fix the code in vim itself, negate the need for special vimrc-file code.
1. create a policy with `ctx.query.owner = "anything";` 2. assign it to any api 3. access the api
ideally there should be no error, as it is a model which works fine in keras (tf backend)
it starts off with allocated 5gb of memory and by time it crashes it as exceeded 16gb of memory.
when setting `&syntax` within a popup buffer, then assigning a text property to a portion of the text, any text after the text property is not highlighted according to the syntax rules.
when running the code in > caching the features extracted from inceptionv3 attributeerror: 'tensor ' object has no attribute 'numpy '
it has not yet been set up to save it.
see enclosed memory graph from gcp ai platform job monitoring: ai_job_mem phofimage the same code worked fine on tf 1.12, i.e.
i attach patch which implements this change and provides test.
see this ubuntu bug from a few months ago that nobody noticed: phofurl and see also this ubuntu bug i went to report again today, having forgotten about the first bug because nobody ever responded to it: phofurl
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
if including tracebacks, please include the full traceback.
maybe modify /etc/sysctl.conf when setting up to prevent this error
` for j, (train_indices, val_indices) in train = val = datos.iloc[val_indices] metrics = [ 'accuracy '] model = build_model(dropout, fc_layers, num_classes=num_classes, opt=opt, metrics=metrics) train_datagen horizontal_flip=true, vertical_flip=true, # rotation_range=25, fill_mode= 'constant ' ) test_datagen ) train_generator train, none, x_col= 'file ', target_size=(width, height), batch_size=batch_size, seed=seed, test_generator none, x_col= 'file ', target_size=(width, height), batch_size=batch_size, seed=seed, has_ext=true, shuffle=true) class_list checkpoint monitor="val_accuracy", verbose=1, mode= 'max ', save_best_only=true) callbacks_list [ checkpoint, 'log.csv ')) ] print( 'iniciando entrenamiento ') history epochs=epochs, workers=16, shuffle=true, verbose=1, / batch_size), / batch_size) ) acc val_accuracy loss val_loss 'model.h5 ')) ` `
here code i used: phofcode
cudnn 7.4.2 gpu model and memory: geforce rtx 2080, 7949mib you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): colab tensorflow version (use command below): 1.13 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: i am getting an error message: `__call__() missing 1 required positional argument: 'inputs'
w failed precondition: error while reading resource variable _anonymousvar4 from container: localhost.
(env) macbook:algo-master admin$ ./algo play [localhost]
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): n/a tensorflow version (use command below): cloud nightly python version: n/a bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: cloud tpuv3
... steps to reproduce the behavior: 1. try to execute `get /user` 1. edit `id` to any value 1. it always get reset back to `1`
when creating a model with sgd optimizer in eager mode and setting its `run_eagerly` attribute to true when i fit the model using some tf.dataset (created using phofhyperlink if that matters) as an input, i get the following error: phofcode
provide a reproducible test case that is the bare minimum necessary to generate the problem.
is there any detail im missing may be causing this error (not first ive faced during conversion, of course)?
is there a way to optimize the tensorflow codes to have comparable performance?
have a component with the the html for a paypal donate button.
devices: i streamexecutor device (0): <undefined>, <undefined> w (one-time warning): not using xla:cpu for cluster because envvar was not set.
then tried data format nchw, model convergences quickly after 70k steps.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
if including tracebacks, please include the full traceback.
cluster b: phofurl same cluster, different keys: phofurl - os & version: macos 10.14.4 - redis-server version 5.0.3
write this in `/tmp/vimrc`: set ls=2 hi!
the gradients should be the same.
i expected the script the script to return without error
provide a reproducible test case that is the bare minimum necessary to generate the problem.
", t t t t">=?
also there is a long delay between the start of each epoch.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i expect both the `tf 1.x` and `tf 2.0` ways of predicting from a `savedmodel` to be serializable phofcode let me know if i am approaching this problem the wrong way!
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below): 1.14.0-rc0 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.0/7.6 gpu model and memory: nvidia titan x (pascal) 12gb
the spec should be detected as oas3 on subsequent calls to `swaggerui` or `swaggeruibundle`
here is the snapshot of the models which produced the aforementioned errors.
i'm guessing a better fix is possible though, maybe using a non-windows code path.
instructions are here: phofurl i used the gpu version.
i know how to create tf.keras.model with functional api for sure.
this throws the `invalidargumenterror` as seen below.
if i use link directly from browser, get a valid download.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): no tensorflow version (use command below): 1.8.0 python version: 3.6.3 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: related to #11888 .
4. :call popup_clear() 5. move cursor to line 2 6. :so % 7. right, expected position
i am using tflite for semantic segmentation.
the file `file.pyc` should be openend.
if i compare it with nvidia-smi trace (peaking at ~2500mb) this makes no sense.
according to the docker instructions phofhyperlink phofcode
if i run the code with directly phofcode if i run the code with `cuda-memcheck --binary-patching no python train_simple.py` phofcode
- linux ubuntu 18.04 - tensorflow installed with conda - tensorflow version 1.12.0 - python version 3.6.6 cuda/cudnn version: 9.0/7 gpu model and memory: gtx 1080 ti 11gb
i have tried this a few times, and it seems to always happen.
here's the repl phofhyperlink .
... steps to reproduce the behavior: 1. go to endpoints with a required header.
expectation: the graph can be convert to .tflite file.
the code runs just fine when i only utilize one gpu `strategy = furthermore, if optional argument input_signature is discarded (only using `@tf.function()`) error disappears too (again using multiple gpus).
there is several idle time in the timeline.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.13.1 python version: 3.6.7 bazel version (if compiling from source): 0.21.0 gcc/compiler version (if compiling from source): 7.4.0 cuda/cudnn version: 10.1 gpu model and memory: 1080 ti 11gb
simple models, or not optimized work.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution : linux ubuntu 16.04.5 lts (xenial xerus) - mobile device if the issue happens on mobile device: none - tensorflow installed from (source or binary): source tensorflow version (use command below): python version: python 3.6.5 bazel version (if compiling from source): 0.16.1 gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: geforce gtx 1080 ti with 12gb memory
no parseexample op in inference should be faster.
import tensorflow as tf import numpy as np import time x = tf.random.normal((1024, 1024)) for i in range(int(1e7)): y = np.array(x) time.sleep(0.01) ``
- programming language golang - mobile device (e.g.
using the command in the end of phofurl posted yesterday by @tfboyd .
run the command "python -c "import tensorflow as tf" within conda cli without error.
large logs and files should be attached.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12.0 python version: 3.5.2 you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)"
i want to use debugging tf.keras model, but got an error message like: phofcode
steps to reproduce the behavior: 1. choose one data with json-format 2. change "view as" from "json" to "hex table" 3. change "view as" to "json" 4. see error
- linux ubuntu 18.04 - tensorflow installed from (source or binary): default ubuntu repositories - tensorflow version (use command below): 1.3 python version: 2.7 cuda/cudnn version: cuda 8.0 cudnn 6.0 gpu model and memory: nvidia p5000 16gb
but when ran this new demo on device it shows me errors like: `info: created tensorflow lite delegate for gpu.
creating a numpy array with uint16 datatype and passing it to tf.image.encode_png() yields different results in eager execution mode.
this is indeed what happens if you do as above, but using `ctrl+w+s` and `ctrl+w+v` for horizontal and vertical splits, and then open the files using `:e [file]`
when running a tensorflow program for the first time (in a fresh docker instance) it detects the gpu as expected.
- using tf hub to retrain inception v3 for mnist images - converting the retrained model to quantized model - deploy the converted quantized model to android app - the quantized tf lite works wrong like below: image phofimage
use a.any() or a.all()`` on on the condition of the line of where this condition is triggered (caused by empty output shape call in phofcode
conversion of tf2.0 function containing `softmax` and `reshape` ops to tflite format fails with the following runtimeerror.
the code run perfectly on my jupyter notebook - anaconda server ubuntu 16.04 and also on colab.
when creating a custom `tf.keras.model` with custom `build()` function, the variable name scopes seem not to propagate correctly in `tf.keras.dense` layers.
a clear and concise description of what you expected to happen.
- build a simple svelte application (doesn't matter if it uses ssr or not) - load a standalone component built from another project (using a `<script>` tag or something) - render the component inside a component from the main application using `<svelte:component />` - see `getcontext` in the standalone component return `undefined` because it (i think) doesn't have the same context
correct serialization and deserialization of the code in both cases.
- vim version (latest dev snapshot).
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.1/7.1 gpu model and memory:
display the utf-8 character where spaces would be.
documentation suggests that `a:added` should reflect the total number of lines added.
i am using the code provided in the custom layer code provided in documentation at phofurl
the return value of is mixed with string and bytes, where the bytes portion is in literal form of including the letter b and the quotes, these will then become part of the directory name created.
this warning came now after switching to tf2 and built-in keras api
info:tensorflow:starting evaluation at info:tensorflow:graph was finalized.
when using to distribute train on 16 workers with 2 gpu each, my job always failed with os error or socket closed after running for several thousands of steps.
the training is stuck after throwing the following warning: w allocation of exceeds 10% of system memory.
so, can not do anything
<img width="876" alt="screenshot at 8 17 53 pm" src=" phofurl
on windows, either from a wsl directory, or `:cd wsl$ ` or equivalent manually.
when testing my model, i had several batches of images with batch size equals to 10. as the images are large, i loaded them from hard drive every time.
phofcode s3_image_url is a valid url for s3 object which i confirmed by downloading it with awscli
the same output as the original model
3. use `]m` and `[m` motions to move the cursor across the file.
it outputs the following when running with the latest tf1 and tf2 installations from pip: phofcode i 've included some profiling information for the function of interest (`taylor_v2` in tf 2) which shows that it spends all time in
please create proper documentations for installing algo on rhel/centos 7.x branch
a clear and concise description of what you expected to happen.
phofcode and observe that the loss and metric values are different: phofcode
my jupyter notebook is 5.7.4 on both windows and ubuntu.
a new split buffer is opened with a git-bash terminal window that can execute commands such as ls, grep,...
i would expect the `./tags` buffer to be focused the first time i run `:argedit tags`.
the bug does not happen a terminal size of 58 lines & 202 columns.
to be deployed and users created.
the fix for #5183 (_vim vi ' sometimes does not select anything_) introduces a regression when not selecting from the second ` '`.
if including tracebacks, please include the full traceback.
for reference the two code samples which produce datasets which cause a segfault on second read are: phofcode i've also dug into and minified down to this repro: phofcode ------ please let me know if there's anything else i can provide or help with!
1. go to phofurl phofhyperlink 2. type the following graphql query 3. click "play" 4. see error the query is: phofcode returns phofcode
when i use conv2d layer with dilation rate other than 1 output looses shapes
when running "flaskapp.py", after loading the model and trying to classify an image using "predict", it fails with the error: > error while reading resource variable softmax/kernel from container: localhost.
6. select "second" in examples dropdown.
note that get_config method was implemented in custom layer and custom model.
the `apply_gradients_once()` function should work even when annotated with `tf.function`.
... steps to reproduce the behavior: 1. go to method with user-agent parameter 2. click on execute
model.build(input_shape) will succeed, and corresponding model variables are created.
not to crash when fit.
tldr: index 0 gives behavior you would expect from not having an index.
this argument seems to only shuffle between iterations within one epoch.
this is the behavior in a popup (broken): phofcode in comparison this is the behavior in a normal window (correct): phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): python version: 3.5 bazel version (if compiling from source): 0.25.2 gcc/compiler version (if compiling from source): 6.3 cuda/cudnn version: gpu model and memory: our bazel build command is attached below: bazel build -c opt --action_env --action_env --config=cuda --config=gdr --config=tensorrt --config=verbs --copt=-wno-sign-compare --copt=-march=ivybridge ``
detailed steps to reproduce the behavior: 1. run `vim --clean` (or `gvim --clean`, etc.)
steps to reproduce the behavior: 1. run ./algo
(i 'm assuming i 'm doing something a bit wrong but i am having so much trouble figuring out what i 'm doing wrong)
large logs and files should be attached.
screenshot phofimage vim on linux
estimator model fails if passed a as a parameter to the runconfig object.
following is the code change i made to phofurl phofcode
phofcode ./algo play [ask user for the input]
the error is: traceback (most recent call last): file line 3, in <module> tf.import_graph_def(sg) file line 507, in new_func return func(*args,
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): include any logs or source code that would be helpful to diagnose the problem.
combining tf.data.dataset with tf.keras.model fails since 1.14.0rc0 when using a dataset for training but a numpy array for validation data.
detailed steps to reproduce the behavior: 1.
steps to reproduce the behavior: 1. create two clusters which you will access via nat (which means you must untick the 'change host on cluster redirect' setting) 2. ensure you are connecting to one cluster via port 6379, and one via 6380 or any non-6379 port (incoming to nat only.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i run training phofcode this is only one step training and its goal is to visualize bounding boxes in tensorboard images tam.
phofcode there is also a colab notebook where you can execute the code directly: phofurl phofhyperlink
- have i written custom code: yes - os platform and distribution: linux ubuntu 16.04 - mobile device (e.g.
"); } else logd("tflite -- not -- ok!
variable `__main__` should keep its value `"__main__"` so it can be used to determine if the python code is imported as a module/library or the entry point of a script.
this case uses only cpu device.
unfortunately, as `tf.train.profilerhook` is a fairly new addition have no execution profiles for tensorflow versions older than 1.13.1. there are few differences in optimizer error/warnings that might explain why this difference in training speed happens: - arithmeticoptimizer fails in 1.11 and up phofhyperlink .
the system buffer should not be lost when the application is put in background (or closed).
with 100 inception graphs, the ram usage goes up to around 10gb.
so either someone (maybe @fchollet) need to change the definition in the api so both `placeholder` and `variable` are fine to be valid inputs (looks like they can be compiled fine in both systems).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18 - mobile device (e.g.
phofcode the minimal `iterable` class can be replaced by `tqdm` (`from tqdm import tqdm`), and this should yield the same results.
the swagger ui should display it as html code, rather than plain text
- os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
should generate a good .tflite file
throw an `indexerror` in this call phofhyperlink and make sure the if statement phofhyperlink is not entered.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): arch linux - tensorflow installed from (source or binary): pip - tensorflow version (use command below): tf-nightly-2.0-preview python version: 3.6 cuda/cudnn version: 10 gpu model and memory: gtx 1080ti if i remove the `@tf.function` annotation the code works as expected.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): centos7 - mobile device (e.g.
please set the breakpoint at the last `print(...)` and evaluate the shapes-dictionary.
1. phofimage 2. phofimage 3. phofimage 4. phofimage
following the updating instructions about the third warning may be enough to get rid of it.
- have i written custom code: no ( phofurl - os platform and distribution (e.g., linux ubuntu 16.04): linux opensuse leap 15.0 - tensorflow installed from (source or binary): pip package tensorflow==2.0.0-beta1 - tensorflow version (use command below): python version: 3.7.3 gpu model and memory: cpu only
if you require this feature, please file an issue.
a syntax error is raised.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04.2 lts in google colab - mobile device (e.g.
when importing all keras callbacks or optimizers via `from import *` or `from import *`, the variable `__name__` gets redefined to and
but ports are not accessible via `telnet` or `ssh`.
can reproduce if configure with just make clean ; make distclean ; ./configure ; make although, compile with just `--enable-rubyinterp`, latency not as high as before; it decreases by about 30%, but still remains very high.
please, run phofurl > > try: > # %tensorflow_version only exists in colab.
if including tracebacks, please include the full traceback.
while it 's possible i have misunderstood something, this is the behavior in tensorflow 1 when i run analogous code.
you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" when i run the `tf_env_collect.sh` script, tensorflow just hangs there forever.
large logs and files should be attached.
trainig should start and use the available 4 gpus
francis chollet 's original notebook (as linked to github above) also shows the correct output.
process finished with exit code 139 (interrupted by signal 11: sigsegv);
`npm i strapi@beta -g` 2.
- vim - os: arch linux and others - terminal: rxvt-unicode v9.22 there are two bugs/issues here: 1. vim acknowledges that `csi 11;?` can return `rgba:` phofhyperlink , but is unable to parse the result.
large logs and files should be attached.
au vimenter call timer_start(0, {-> augroup end fu s:trigger_leave_events() abort if match(v:argv, ' c-[doo]$ ') != -1) let curwin = win_getid() windo " call win_gotoid(curwin) endif endfu it seems work, but it also looks awkward.
large logs and files should be attached.
large logs and files should be attached.
phofcode if i define `i` outside of the function, then autograph uses that: phofcode the correct behavior should be the same as without decorating with @tf.function: phofcode
warning: logging before flag parsing goes to stderr.
this happens only when previously the window was scrolled up such that the first line of the buffer isn't visible anymore, and `deletebufline(buf, 1, '$')` was called prior to the job output.
i know this would be a huge amount of images (c_in * c_out), but i think the current behavior is confusing.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.12.0 python version: 3.6.7 bazel version (if compiling from source): invocation id: build label: 0.21.0 build time: wed dec 19 2018 gcc/compiler version (if compiling from source): gcc (ubuntu 7.3.0 cuda/cudnn version: cuda compilation tools, release 10.0, gpu model and memory: gtx 1060 max q, 6gb vram output: temp temp1 temp2 temp3 however, strangely enough, the problem only occurs with tuples when the length of the tuple is 2 or more.
typeerror traceback (most recent call last) in <module>() 17 18 ---> 19 line_thickness=8) 20 21 plt.imshow(image_np) typeerror: takes at least 7 arguments (8 given)
i expected flat_map to treat the windows as datasets, allowing me to perform dataset operations such as `batch` on them.
8.0.776) on same pc exactly same repro steps (minimal vimrc file, same file to edit, same pc, etc.)
i am using in case that might make a difference.
i expect it to work as expected
large logs and files should be attached.
large logs and files should be attached.
tmodel t kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1)), # error t t activation=tf.nn.relu), t t]) t t t t metrics=['accuracy']) tmodel.fit(x_train, y_train, epochs=5) y_test)
- dependency optimizer fails 1.11 up, but not 1.10: phofcode this issue phofhyperlink mentions slowdown caused by additional graph optimization 1.9. do not think that this case here, as it steps of itself slow.
i am saving the biggan-deep-512 module from the tf hub into a savedmodel.
if including tracebacks, please include the full traceback.
i have written a code performing inference on frozen graphs.
i run `python script-name.py` in the command prompt.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
it may not be the correct fix, but it should point to the part of the code where i believe problem to be
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12 python version: 3.6.8 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 9 gpu model and memory:
phofcode if don 't set would get error.
first issue was in `def preprocess_input(x,
vim the system update from 16.04 to 18.04. in the 16.04 vim is ok, but in 18.04 the vim current cursor auto append 'p ' when open file.
swapping the position of operands gives different results: - `1e-8 + 1.0 - tf.identity(1.0)` results in `0.0` - `1.0 - tf.identity(1.0) + 1e-8` results in `1e-08`
- 8.1.837 - it works - - it fails it fails windows 7.1 64-bit gui cygwin 64-bit, bin folder on my `%path%` by changing the command to `sleep 100`, you can fire up sysinternal's processexplorer and examine the command-line that gvim has given the new bash process: when it works: bash -c 'sleep 100' when fails: bash -c bash -c 'sleep 100' what is happening here is that the first starts up a second interactive bash, which never exits
i have my .pb file attached below: my_frozen_graph.zip phofhyperlink my toco code below to regenerate the error: `graph_def_file = "my_frozen_graph.pb" # this is the .pb file.
i have used the exact same code from the tensorflow 2.0 distributed training tutorial as in phofurl phofhyperlink .
the example i modified is this: phofurl specifically, i changed all of the keras imports in this file: phofurl to be tensorflow.keras imports.
... steps to reproduce the behavior: 1) clone the project 2) npm install 3) npm start (or npm serve-static) http-server is required but is not defined in package.json.
i expect that 'try it out ' panel will contain button to select file for media types 'image/* ' , not only for
i think this is one of the most simplest example to reproduce error.
- vim version - os: arch linux.
should the built-in data structures that are tracked be serializable
i just ran the colab transformer notebook: phofurl
see below, which i ran on a osx mojave macbook pro (early 2015), ipython running python 3.5, tensorflow 1.10.0: in [1]: from tensorflow.keras.models import model runtimewarning: compiletime version 3.6 of module does not match runtime version 3.5 return f(*args,
this issue is very similar to #3628, but it only happens in non-eager mode in tf 2.0. you can save a model (having batch norm layer) in
the memory shouldn't grow indefinitely as it did in tf 1.12. see the memory timeseries for tf 1.12 below: ai_job_mem_ok phofimage
if buffer is hidden or open another tab, popup can still be scrolled.
the code does exactly what i expect.
when trying to load it with model = load_model(), i get this error: valueerror: unknown entries in loss dictionary: ['class_name', 'config'].
we identified the slow down is caused by op using this way: for this line of code: decoded, prob = seq_length) running time for session.run(logits) is fine, however session.run(decoded) is very slow we verified the problem occurrs on tf versions 1.12, 1.9, 1.8, 1.5, 1.4 we also verified model ckpt trained with tf 1.9 running inference fast with 1.3. similarly, model ckpt trained with 1.4/1.5 also running fast with 1.3 it looks there no big change for beam search decoder op between 1.3 and 1.4. googling problem got no luck either.
i created a session option with inter=1 and intra=1.
large logs and files should be attached.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os 10.14.2 - mobile device (e.g.
but instead it throws this error: > assertionerror: called a function referencing variables which have been deleted.
1) use a large terminal with >= 203 columns.
for example, i created a session with `inter=1, intra=1` and close it.
in graph mode the problem does not occur: phofcode
1. try to save as described above.
there is a large accuracy difference between running tensorflow in eager versus graph mode (i.e.
tftrt failed to collect calibration info and cannot create the int8 inference graph.
`vim myprog.scm`) 2. set the omnifunc to `:set 2. type a single quote character (`'`) 3. omni complete: 4. resulting error: phofcode
i have a dataset contains labels with true and false, and a column of english test.
one placholder per feature (i.e since the features are already parsed , i do not create tfexample , but rather feed in individual tensors i.e a feed dictionalry where for each feature column i feed in the respective tensor) the number of ops in both graphs are approximately same (multiple placeholders being slightly less ) ~5-6k ops/nodes latency of version 2 is 2-3x higher then version 1
`:q` to close the tab for a
observe the aforementioned files hanging around.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes (see below) - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - tensorflow installed from (source or binary): pip (tf-nightly-gpu) - tensorflow version (use command below): python version: 3.6.8 (anaconda) cuda/cudnn version: 10/7 gpu model and memory: gtx 1050-ti
when filing the bug, set the verbosity to 10 (on linux, `export autograph_verbosity=10`) and attach the full output.
gzv ' 2. press `gk`, `j` and `gk` again.
when you have a bunch of outputs that all have the same output size it becomes impossible to tell which one is which.
detailed steps to reproduce the behavior: 1. run `vim --clean` 2. type `:vsplit` 3. make the current window big with ctrl-w and then `|` (pipe) 4. type `:windo ''` (to do nothing in every window) 5. the tiny window gets expanded
if including tracebacks, please include the full traceback.
however, the same code works on tensorflow cpu version.
code: phofcode printed outputs: phofcode
import numpy as np import tensorflow as tf weight_variable = 1 no_of_features = 10 timesteps = 3 batch_size = 32 def data_gen(): while true: numerical np.random.randint(5, size=(batch_size, timesteps, no_of_features)) y np.random.randint(2, size=batch_size) w np.ones(batch_size) * weight_variable # or np.where() for imbalanced datasets yield {'numeric_input': numerical}, y, w def build_model(): numerical_input no_of_features), name='numeric_input') rnn_out tf.keras.layers.gru(32, dense tf.keras.layers.dense(1, activation='sigmoid', model dense) params { 'loss': 'binary_crossentropy', 'optimizer': 'metrics': } model.compile(
if i connect to the vpn, and power down my lightsail vpn host, wireguard will still show a connected vpn which is impossible if the algo host is off.
they are identical, with exception of those 2 keycodes: set <sgrmouse>=[[<*m set <sgrmouserelelase>=[[<*m they don 't seem to be related to current issue (i.e.
during inference of variable sized examples with tensorflow mkl system memory consumption gradually increases.
4. narrator (screen reader) read mandatory field as required field.
it 's not able to convert the lstm model to tflite format
the quantized model should work on nnapi
when one enters a wifi with emoji in its name (yes, that 's possible - my wifi is called wf), subsequent calles to algo update users fail with a "unable to read yaml" error.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes.
when i load from the hdf5 file phofhyperlink (download password: w2k6) the code phofcode get error messsage that > successfully opened dynamic library libcuda.so.1 e failed call to cuinit: cuda_error_no_device: no cuda-capable device is detected retrieving cuda diagnostic information for host: xieyi-desktop hostname: xieyi-desktop libcuda reported version is: kernel reported version is: kernel version seems to match dso: cpu frequency: hz xla service executing computations on platform host.
when i click the attachment a window opens that shows what looks like an xml file.
it seems that vim always resolve symbolic-link directory to physical directory!
in tf v1, the dataset instance should inherit from `datasetv1`, not `datasetv2`.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: all android devices using tflite example app (replaced the sample models with my custom tflite model) - tensorflow installed from (source or binary): pip installed tensorflow-gpu and tf-nightly-gpu tensorflow version (use command below): tried on 1.14, 1.13, 1.14-nightly, 2.0.0-beta1 python version: 3.6.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: cuda 10.0 gpu model and memory: gtx 1060 6gb include any logs or source code that would be helpful to diagnose the problem.
`setshapefn` doesn't work when called for `register_op` for the custom operators.
the output shape is not inferred correctly anymore for a keras conv1d with dilation_rate != 1 (it is ok when dilation_rate = 1).
the worker should run normally.
if including tracebacks, please include the full traceback.
... - npm install -g http-server - cd - http-server -p 3333 -c-1 --cors - open swagger ui (i.e.
tf-lite benchmark:- adb shell --num_threads=1 --use_nnapi=1 adb: no version information available (required by adb) starting!
converted the model via: `bazel run -c opt -- ` ` ` ` `--mean_values=128 `--std_values=128 `--allow_custom_ops` changes in
detailed steps to reproduce the behavior: use a vimrc with only the following lines: set completeopt= set nocompatible set tags={path to my tags files} edit a file with a lot of keywords starting with the same prefix, for example "modulea" go into insert mode and type modulea and hit ctrl-p a bunch of times
phofcode - os: arch linux 5.3.6-arch1-1-arch #1 smp preempt fri oct 11 utc 2019 x86_64 gnu/linux - terminal: terminator
it takes a while for me to generate a `sparsetensor` phofcode how can i save it just by itself?
this is inconstent with how tf.placeholder behaves.
by the way, neovim is also affected: * arch linux + neovim 0.4.2 * arch linux + neovim git-master ( phofhyperlink ) i discovered this issue when using coc.nvim.
when i run the timing and evaluation script from here phofhyperlink , fps actually decreases as the optimizations are tried, and file sizes stay constant.
large logs and files should be attached.
`import tensorflow as tf` `import numpy as np` `import os` `import sys` `input_layer = batch_size=1)` `layer1 = `layer2 = `layer3 = `model `alpha `# model.fit() is skipped just to get straight to error.` just execute this code as it is, it will throw same error!
- have i written custom code (as opposed to using a stock example script provided in tensorflow): a fix is simple -- the `lambda.compute_mask` should include the test present in `layer.compute_mask`, namely: phofurl
bazel run -- --output_file=$detect_fb --output_format=tflite --inference_type=float --logtostderr # build and install demo bazel build -c opt adb install -r -f 5) then you have to copy .tflite model to you apps app/assert directory and refer to it in you code like mentioned above 6) after gradle build running app on you phone might probably get runtime error i did.
it seems that the tensorforest is inferior to scikit-learn extratrees implementation both in time and space.
also there no `_grouped_model` attribute on self.model anyways; one that i see `_original_model` that said, expected behavior document if a custom callback be designed considering some caveats when using mirroredstrategy and provide a public attribute and/or method have access original model.
ps: when in cpu version, i 'm not even using tf.function decorator.
the same code works well for fetching `all_reduce` tensors.
5. copy and paste the 'send' method from the 6. modify subject on options object
the crash happens earlier with larger batch sizes however.
phofcode implicitly accepts any dtype (as long as input dtypes match).
`vim --clean` (or `gvim --clean`) 2.
exit alternate screen when recieve sigtstp send with kill
however, notice that vi[1] doesn't move backwards, and vim does even in compatible mode.
repl phofhyperlink on removing the curly braces, svelte will correctly invalidate the array.
also, i used exactly the same code with keras on top of tf1, which worked fine there.
when quantization=false, phofcode their outputs looks same.
can 't take gradients, fails with invalidargumenterror: operation has no attr named '_xlacompile '
okay, try `:echo gettabwinvar(2, 1, '&cmdheight')`.
linebreaks are converted to spaces.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary):binary tensorflow version (use command below):1.12 python version:3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory:4g you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
model generation should behave the same from a memory point of view regardless of whether the input shape is specified or not.
choose your default database client ?
according to the docs phofhyperlink on > if the graph-level seed is not set, but the operation seed is set: a default graph-level seed and the specified operation seed are used to determine random sequence.
i am training a small lstm model and until recently, i could use reading numpy arrays directly because all training data fits into memory.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): '1.11.0 ')`, '1.12.0 ')`, '1.13.1 ')` python version: 2.7, 3.5, 3.6, 3.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: (reproducable with gpu explicitly disabled)
the only difference in the code is the following lines: phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow): _yes_ - os platform and distribution (e.g., linux ubuntu 16.04): _ubuntu 18.10_ - mobile device (e.g.
running deep model and some wide linear models.
when i call the custom function, i encountered error: typeerror: using a `tf.tensor` as a python `bool` is not allowed.
the example given for tf.keras.metrics.meaniou returns `attributeerror: 'list ' object has no attribute 'shape '`.
it should finish the save process successfully.
import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from google.colab import drive #get a image as input data for model : tensorflow logo !curl phofurl --output tensor_logo.png def train_input_fn(): def raw_image = tf.read_file(img_path) image_decoded = channels=3) image_decoded = tf.cast(image_decoded, tf.float32)/255.
the 'type ' parameter is dropped from the form data portion of the curl when there is no recognized type.
python import tensorflow as tf import numpy as np import os import psutil n_samples = dim = 100 batch_size = 50 raw_data = np.zeros((n_samples, dim)).astype(np.float32) dataset iterator [none, dim]) process def mem(): return / 1024
when running `model.evaluate` in tpu, each time it gets executed it takes longer and longer.
6. the name property of a focusable element must not be null.
the code is here: phofurl
i have 1chief, 2worker, 1ps, 1evaluator, the training starts with no error, everything seems right, but training does not stop correctly.
code that reproduces the problem.
i'd expect gpu performance to be (significantly) better than 4-thread cpu.
traceback (most recent call last): file line 297, in stop_on_exception yield file line 882, in run self.main_result =
(in ->32.49% 0x5bbe6b8: start_thread (pthread_create.c:333) clone (clone.s:109)
you can try using only one at a time, they all fail.
large logs and files should be attached.
this to me is not "abort on failure" but rather "succeeds at this particular substitute operation."
`:hi statuslinetermnc` or `:call setline(1, join(split(execute( 'hi statuslinetermnc '))))` 1. it looks like "statuslinetermncxxx term=reverse ctermfg=0 ctermbg=2 guifg=bg guibg=lightgreen".
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): (this is the tf 2.0-preview) python version: 3.6.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: here is the output of this program: phofcode
replacing: `optimizer = beta_1=0.9, epsilon=1e-07)` by `optimizer = beta1=0.9)` it works without any issue.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes, duplicated the minimal code from documentation snippet from here phofhyperlink - os platform and distribution (e.g., linux ubuntu lts (bionic beaver)" - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
a model trained and validated on identical data can reach 100% train accuracy but never reach that when validating against the same sample.
this makes it impossible to write accessors, so we end up having to access variables directly, which leads to ugly code & hard to maintain.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): pip install tensorflow version (use command below): 1.6 and 1.10 python version: 3.6.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 9.0/7.05 gpu model and memory: gtx 1080 ti you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
each test uses tensorflow alpha 2.0, and trains using the mnist dataset provided/bundled by tensorflow.
cause: converting <function initialize_variables at attributeerror: 'module ' object has no attribute 'num ' ``
detailed steps to reproduce the behavior: 1. create demo script: phofcode 2. open the script with `vim --clean -s complete_showbreak.vim complete_showbreak.vim` 3. invoke dummycomplete using `i<c-x><c-u>` 4. the `info` popup is displayed with wrapped lines.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.12 python version: 3.6 bazel version (if compiling from source): 0.17.2 gcc/compiler version (if compiling from source): 7.3 cuda/cudnn version: cuda 10.0 cudnn 7.4 gpu model and memory: nvidia geforce gtx 1060 ti runconfig with distribution strategies.
- vim version - os: gnu/linux - gnome terminal
if x is a list of any size, or a tuple of size >= 2, which contains tensors, np.array(x) is unusable as it runs exponentially longer than the tensor.numpy() function.
colab notebook that reproduce problem phofhyperlink phofcode
_wrapped_model function produces signature `{layer: x2}` which is wrong!!
i 'm converting the original ckpt model to pb using frozen graph phofcode then i convert the .pb to tflite using : phofcode any help would be appreciated.
tpu distribution strategy should be able to implement this
am not sure why this has emerged all of sudden as was having no problems yesterday.
the memory shouldnt increase on each interation
phofcode if i don 't include the arg at the end, then i get this error: `typeerror: freeze_graph() missing 1 required positional argument: if i do include it, then i get past that weird error and encounter this one instead: no such file or directory`.
also, the model does not load at all on android.
psenet works as follows: running the image through feature pyramid network to obtain segmentation maps, it then applies a custom algorithm to extract bboxes.
... steps to reproduce the behavior: 1. go to phofurl 2. enter the swagger above 3. expand anything you will until you reach the definition of a under the models.
while the `mouse_xterm` feature indeed technically does not require on the x11 headers, resulting behavior is when running vim in an xterm, you can now make a selection with mouse, but have no way to send selected text to system clipboard.
this does work correctly when not executing eagerly
i 'm using api and trying to make it work in a distributed environment.
the website renders, topbar and all.
attaching perfmon screenshot: image phofimage image phofimage
but that is an ordinary pipeline for estimators.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the form input "shakes", indicating that its a missing and required field.
model conversion from script errors unexpectedly with: f check failed: dim >= 1 (0 vs. 1) fatal python error: aborted current thread (most recent call first): file line 52 in execute file line 251 in _run_main file line 300 in run file line 40 in run 89 main 10 <module>
the traceback is as follows: <pre> valueerror traceback (most recent call last) in <module>() 80 for batch_id in 81 batch_data = # v2 ---> 82 loss, outputs = # v2 83 # _, loss, outputs, inputs = sess.run([opt_op, loss_, outputs_, batch_data]) 84 if loss_metrics is none: in __call__(self, *args,
servers don 't support clean shutdown.
if including tracebacks, please include the full traceback.
`tf.io.write_file` should create an output file whether or not being decorated with `@tf.function`.
1. save the above script into a file 2. run `vim --clean` (or `gvim --clean`, etc.)
runtimeerror when convert frozen graph to tflite with `converter.optimizations = and = representative_data_gen` `runtimeerror: attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.no calibrator found for context.node number 0 (pad) failed to invoke.` i double checked the shape of the input array which is [1, 512, 512, 3] and i can convert it to a float tflite model without any dynamic-sized tensor.
the output `a_sqrt_1` of tf.sqrt phofhyperlink in the provided snippet is incorrect.
running `:redraws` upon `bufunload` breaks the tab line.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes, i am using custom flutter plugin for firebase custom model - os platform and distribution (e.g., linux ubuntu 16.04): macos - mobile device (e.g.
the model should be converted with standard cross entropy implementation and without explicitly defined global step variable.
the program should run successfully.
5.0.1] add any other context about the problem here.
the internal `build_info` module can be used to query information about the build, importing it like: phofcode however, while in previous versions you could get for things like or now the only available relevant attribute is (and in tensorflow 1.13.1 also
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution: linux ubuntu 16.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12.0 python version: 3.5
we are using distributed tensorflow as described here with parameterserverstrategy: phofurl basically we are starting the tf server on every worker and then running train_and_evaluate on each worker.
when i try to expand a group of nested keys that have a large number of keys the app crashes.
after conversion with trtgraphconverterv2 they are changed to default names 'output_0' and 'output_1'.
i have changed grucell to lsmmcell or other rnn-related cell and results are same.
large logs and files should be attached.
i want to do some operations like: 1. split the image into 12*12 squares.
some models(mtcc) uses prelu as an activation layer, and it will be split to small ops including relu, sub( phofurl it's hard to fuse and will impact the performance .
i tried: 1. reducelronplateau as only callback 2. set mode to min 3. different monitor
it would be better if sigmoid is applied, no matter what are the initialization arguments of binarycrossentropy.
w0619 some requested devices in `tf.distribute.strategy` are not visible tensorflow: some requested devices in `tf.distribute.strategy` are visible tensorflow: deprecation.py:323] from (from is deprecated and will be removed a future version.
tensorflow now creates multiple devices, see log.
you need to install the following to make it work: `pip install tensorflow-gpu==2.0.0b1 numpy tqdm` python # imports import time import numpy as np import tensorflow as tf from tensorflow.python.keras import callbacks as cbks from callback from model from training_utils imagedatagenerator data_utils generic_utils tqdm tqdm_notebook # helper function (taken phofurl def workers=1, max_queue_size=10, shuffle=false): enqueuer = generator, shuffle=shuffle) output_generator = enqueuer.get() return output_generator, enqueuer # my silly callback class noise(callback): def on_batch_end(self, batch, logs={}): image_shape = [1, 2
when using the asset file is not exported in the savedmodel assets directory.
- os platform and distribution (e.g., linux ubuntu 16.04): archlinux - mobile device (e.g.
ideally it should work (after all there's 128gib of hbm on the other side of this pipe so 2gib is really not unreasonable here), worst case i'd have expected an error status to be returned rather than a hard assert.
applying batch normalization on top of batchwise tf.map_fn results in retval[0] error.
following the phofurl tutorial, which is based on phofurl all the compilation steps succeed.
on cpu it behaves properly and each time with a different combination even with supported ops, the behavior of the model changes and even not able to benchmark it to find the issues.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes, custom model to count fingers, but the tflite converter code is same as example scripts provided in api docs.
in eager execution, when creating a `tf.keras.sequential` model inside a loop and discarding it immediately, the memory increases over time.
no errors when used correctly (maybe that is the issue).
(will try to do this later)
firewalls, nat, routers, etc) between your computer and the remote server is not configured to allow vpn connections.
this is way i convert ssdlite mobilenet model from model zoo: phofurl this is also way which was recommended to me, because conversion did not work with frozen model provided by zoo (or by getting frozen with inference_graph script).
=> {"changed": false, "gid": 0, "group": "root", "mode": "0755", "msg": "the directory configs/localhost is not empty, refusing to convert it", "owner": "root", "path": "configs/localhost", " size": 4096, "state": "directory", "uid": 0}
have to do this accept variable sized images.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
popup_complete_showbreak phofimage tested in terminal vim in arch linux, and gvim in windows.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): centos 7.5 - mobile device (e.g.
model should be updated with the variable y as a trainable weight
wrapped around a `tf.keras.layers.lstm` instance raises a `typeerror` when trying to pass it an initial state (through the `initial_state` argument of the `__call__`).
i looked briefly but was unable to find one.
should return the screen position per the test above.
tutorial code for rnnlm using ptb_data_set can be found here phofhyperlink modified tutorial code with timline logging added for training loop can be found here phofhyperlink execute the following commands to get the list of operations which have kernel queue times in timeline and execution times are not logged.
phofcode out: phofcode this has the same issue whether the code is run on gpu or cpu.
i am running a custom code using tf.
this shows the exponential time difference when x is a list containing a single tensor.
1. install tensorflow using anaconda and run ipython: phofcode 2. run the following code in ipython: phofcode 3. check cpus allowed list for main thread(in another bash): phofcode output (may vary): phofcode 4. perform matrix multiplication in ipython: 5. check again cpus allowed list for main thread: output (may vary): it become single core!
now converting this model to be used with estimator` then using `train()` then the code crashed with: 'meanmetricwrapper ' object has no attribute '__name__ '
but the inference speed actually slowed down after the graph transformation.
`xrdb -merge <<< 'urxvt*background: 2.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
#install python3.6.8 #install vc_redist.x64 pip install -u pip virtualenv virtualenv --system-site-packages -p python ./keras keras (keras) pip install --upgrade pip (keras) list (keras) install --upgrade tensorflow (keras) python import tensorflow as tf
the following code was working with some earlier release but now it is crahsing phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below):2.0.0a0 python version:3.6.6 bazel version (if compiling from source):- gcc/compiler version (if compiling from source):- cuda/cudnn version: using cpu gpu model and memory: using cpu see link in the described behavior
"converting sparse indexedslices dense tensor unknown shape. "
however see that dynamic_rnn will be removed.
this resulted in a average accuracy loss of 5% compared to passing the tensor through the layer unmodified
training parameter to model passed as none
it is a recreated version of a keras model that reaches accuracy of > 0.8 on the same data, whereas the tensorflow model built from tf.layers gets stuck under accuracy of < 0.3. presumed issue is lack of the moving mean and variance operation in the update ops, but all recommended methods including them ( phofurl have failed.
the code for the two versions only differs in enabling eager execution and the loss function.
phofurl maybe this code is a little bit complex, but it can show you where the problem is.
what the hell is going on?
then pass created dataset to a function annotated with `tf.function` that should perform training loop.
if applicable, add screenshots to help explain your problem.
there are many use cases where the access to model in a callback is required.
totally overrides existing text highlights, where it should be able to merge with them.
similar to this tutorial phofhyperlink , but with inside the `train_step()` to print the prediction.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): tf.version.version == tf.version.git_version == python version: 3.6.8 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: i can work around this issue in multiple ways: the easiest is to replace `super()` with `super(b, self)`.
this seems to happen on the gpu version i've tested on another system, but crashes on the cpu version on my mac.
error: expected string, got 0 of type 'int' instead.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): python class customconv2d(conv2d): def __init__(self, filters, kernel_size, strides=(1, 1), padding= 'valid ', dilation_rate=(1, 1), activation=none, use_bias=true, 0.01), kernel_regularizer=none, bias_regularizer=none, kernel_constraint=none, bias_constraint=none, include any logs or source code that would be helpful to diagnose the problem.
the caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
don't use `--clean` so that netrw is loaded.
when model trained and evaluated using * 1.13 * , model with optimizer applied starts show unreasonably high predictions: phofcode these results above consistent with what we see our models, where predictions became overwhelmingly positive once 1.13 was used for training (since we do use optimizers).
tf.keras.utils.multi does not work with wgan-gp model i also try on keras(not tf.keras) and it was work!
this works: phofcode this fails due to failed assert: phofcode
optimizer increments the global step by 1 and stops without changing the network.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): tensorflow/benchmarks - os platform and distribution (e.g., linux ubuntu 16.04): debian 9.4 - mobile device (e.g.
- have i written custom code: yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10, linux, colaboratory - tensorflow installed from (source or binary): binary - tensorflow version (use command below): unknown 2.0.0-beta0 python version: 3.6.7 cuda/cudnn version: 10 gpu model and memory: quadro m1000m, colaboratory, ...
--- second explanation: when `:resize` was implemented, it could only be run in context of focused window, which can not squashed because cursor has to drawn somewhere.
i used the tf 2.0 conversion script to convert my dcgan sample from tf1.x to tf2.0.
following this step phofurl i installed strapi using:strapi new my-project selecting custom (manual settings) connected to mongodb atlas i am able to create content type builder but not able to add new content.
i tried to build deeplab (input_image + float32) and succeeded # modified export_model.py # line:77 phofcode s. i converted below command and succeeded tflite_convert --output_format=tflite --inference_type=float --mean_values=128 --std_dev_values=127 but.
`pip install --ignore-installed --upgrade - tensorflow version unknown 2.0.0-beta0 python version: 3.6.8 cuda version: 10.0 gpu model and memory: nvidia geforce 1060 3gb _see attached file_ stacktrace.txt phofhyperlink
the output of gpu 1 should be the same as gpu 0.
if including tracebacks, please include the full traceback.
if including tracebacks, please include the full traceback.
the order should be deterministic given the random seed, and there should be a way to reshuffle between epochs.
i apoplgize for any mistakes or missing information.
type in the code below and watch memory usage increase.
the behavior of `sequencefeatures` layer is really different between when it's used with a feature column produced with and when it's used with a feature column produced with `embedding_column`.
1. paste the openapi config from above.
i deliberately slow down one worker (add some sleep in the program of the worker), then i find the other workers can surpass the slow worker by 1~3 iterations
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): archlinux - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12 python version: 3.6 cuda/cudnn version: cuda 9.0, cudnn 7.1 gpu model and memory: geforce gtx 1060 include any logs or source code that would be helpful to diagnose the problem.
to do this, in the parameters for the endpoint add this yaml: phofcode 3. after ui updates, go to post /pets endpoint and click 'try it out ' then `execute` without filling any value for required header.
my running log of `test.py`: phofcode python sess = tf.session() with sess.as_default(): tensor = tf.range(10) print_op = tf.print(tensor) with out = tf.add(tensor, tensor) sess.run(out) phofcode as for my gpus, driver is installed according to official instructions.
'` the full log for printing `__dict__` can be seen as below phofcode
phofcode i think this is a bug.
use attached `.tflite` file to reproduce the issue.
<img width="345" alt="metric" src=" phofurl i am training a model with tf.estimator and it seems that the training metric (root mean squared error) is not divided by the batch size (for both training and validation the batch size is 100).
factoring out a train step function into our custom model lead to a memory leak due to continuously retracing the full autograph epoch dataset loop.
i continue to get the error message despite 'with tf.session() as sess:
however, when the last batch has a smaller batch size than previous batches (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i could be wrong, but i think it 's a bug because `winenter` is fired c in the context of the second window c * after * `cmdwinleave` is fired; so `wincmd _` should maximize the latter after the command-line window has been closed.
therefore, i tried to use tensorflow 's graph transform tool phofhyperlink to make if faster.
detailed steps to reproduce the behavior: assume three files: `a`, `b`, and `c` you open a (`vim a`), split vertically (`:vex`), then open b, split horizontally (`:sex`), and open c. you then edit the three files, and save by `zz` the three files, in any order.
registering sessionrunhook to `training_hooks` does not trigger the error even if it is in distributed mode.
i adding visible gpu devices: 0, i device interconnect streamexecutor strength edge matrix: 0: n y 1: y n created device 15190 mb memory) -> physical gpu (device: 0, name: tesla pci bus id: compute capability: 6.0) created 15190 mb memory) -> physical (device: name: tesla pci bus id: compute capability: 6.0) warning:tensorflow:from checkpoint_exists (from is deprecated will be removed in a future version.
i am using keras functional api phofhyperlink to create a model.
the session should return a scalar float.
buid a simple sequencial model using keras.
when job outputs 2 lines, second line will be visible immediately.
if including tracebacks, please include the full traceback.
./algo 2. install .ps1 file for windows vpn 3. connect using settings
a typical output looks like the following: phofcode
my attempt workaround was comment previous lines, so as use same workaround as did with tensorflow 1.12.0. but this time it didn't work for an obscure reason that can't really understand.
if including tracebacks, please include the full traceback.
... steps to reproduce the behavior: 1. go to phofurl 2. click on 'load swagger' 3. click on it again 4. see error
the rnn classes in `tf.nn` should be compatible with all keras rnn layers.
3. now run this query to also contain image ids: phofcode the query takes forever and thats just with 10 books.
8. click button to connect facebook, you should be redirect facebook, but on inspection of url, uri is still ` phofurl
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i believe it 's due to the `graph` being held as an indirect reference to the custom gradient registry.
phofcode attributeerror: module 'tensorflow' has no attribute '__version__' the following works: import tensorflow as tf from import model print(tf.__version__) ``
i think it will just keep going like this unless i fix the problem......
i want to use a `savedmodel` to run batch inference in `spark`.
- os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04.2 and windows 10 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0.0-beta1 - python version: 3.7
it works in gpu version seemlessly
`strapi develop` or `strapi start` get this error: phofcode probable reason: strapi generate:api replaces "_" with " ", which messes up graphql syntax.
there should be an option to disable the automatic update of `formatoptions`.
the training data is pre-processed and loaded into memory.
want to be able to use tfrecord data set with words and graph to lookup indexes via tf table and then parallel lookup imbedding vectors; compute using 3 dim tensor.
note that many code examples still show `tf.log()` (try searching for ` 'tf.log( '` in the code base, you will find 12 matches across 8 files).
we expect the two graph to output similiarly (normally, we get absolute error < 1e-5)
only expected ' 590 'following keys: {} '.format(name, list(unknown), --> 591 expected_values)) 592 593 valueerror: unknown entries in loss dictionary: [ 'a ', 'b '].
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.0rc0 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.0 gpu model and memory: nvidia tesla p100 you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
large logs and files should be attached.
specifically, what is difference between and when is a matrix.
from looking at graphviz video, this change is introduced in frame 38.
i want to know the policy or rules to match two different information.
it will break down multi-thread computing of numpy and gensim (only using single core for matrix multiplication or learning model)
can you help me solving this issue?
- have i written custom code: yes - os platform and distribution: macos mojave 10.14.1 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12 python version: 3.6.6
specifying no index means that i am accessing a list of tensors, where as specifying an index gives me the tensor at that index.
(to know which changes to make, i looked into the demo app version r1.13.)
func() {{{1 after pressing `zd`, i would expect: fu!
large logs and files should be attached.
if i use a large u-net with an inception-resnetv2 backbone and eager execution is enabled,(default with tf 2.0), the machine runs out of memory.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 14.04+ - tensorflow installed from (source or binary): binary - tensorflow version (use command below): python version: python2 / 3 bazel version (if compiling from source): 0.24.1 assuming this is related to the virtual pip package.
if you can 't find one, you can try it out on xcode iphone simulator just as i did.
note sure if there is similar behavior on other machine/devices
provide a reproducible test case that is the bare minimum necessary to generate the problem.
if i load and unload inception v3 graphs one by one, there is no memory leak, ram usage stays below 1gb, does not matter how many v3 graphs i load.
t [[{{node during handling of the above exception, another exception occurred: traceback (most recent call last): file "test.py", line 46, <module> x = sess.run(next_element, feed_dict={handle: training_handle}) 676, run 1171, run 1270, run raise 693, reraise raise value 1255, run return self._sess.run(*args,
phofcode how can fix this error?
imho, tensorflow 2.0 should eliminate this sort of inconsistency, it would be more pythonic: phofcode
only the `word-test` suggestion should be proposed, i.e.
i would have expected that autotune could pause and not "start from scratch" (and would stay running quickly) so long as i kept the session object around.
index 1,2 give error same as no index
<details> <summary>example code snippet</summary> phofcode </details>
it 's odd algo is identifying the os as ubuntu 18.04, whereas 'uname -a ' is phofcode
this is generally a mistake; consider storing variables in an object attribute on first call
phofcode provide a reproducible test case that is the bare minimum necessary to generate the problem.
instructions for updating: colocations handled automatically by placer.
if you use no weights there is no error during fitting, but if you fine tune the model, re-compile it and fit again the same error pops.
algo installer runs without errors.
phofcode however, if set the above flag, i can loss being logged (similar to that of api v1) phofcode
not found: resource does not exist.
i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 fma i xla service executing computations on platform host.
expect the load to be shared by all the available cores and the performance to scale up with increasing number of cores.
when running a simple custom-op on cpu adapted from the tutorial with multiple threads using work sharder it can find only 1 thread and segfaults.
1. install strapi 2. create model 3. build admin using node_env=production npm run build 3. start in production mode using command node_env=production npm run start 4. you should get error saying error typeerror: cannot read property 'replace' of undefined
the loss keeps fluctuating up and down, and the weights of my network stay constant throughout training process (i've checked this using tensorboard).
n/a - vim version phofcode - os: per `dockerfile` above - terminal: per `dockerfile` above n/a
add any other context about the problem here.
when mirroredstrategy is used there is still access to "model" object however its type is not `tf.keras.models.model` rather it `distributedmodel` when training such a model tensorflow generates following warnings - phofcode inside the custom callback in this scenario self.model.layers set to [] i.e.
1. add the settings.json file at 2. make a change in the settings.json file 3. restart the server 4. regenerate documentation 5. check for changes
print( '###evaluate with training data### ') print( '###evaluate with testing data### ') use an array instead of a generator and print prediction array see what happens all prediction arrays gets similar value from different inputs.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 fma <class traceback (most recent call last): file line 7, in <module> out = net(input) file line 660, in __call__ outputs = self.call(inputs, *args,
on a 18.04 vm local install, during the initial install, the `./algo` script fails on `task [strongswan : revoke non-existing users]`
model.fit should not throw any error.
just add some jni code to the official demo would got the same issue.
large logs and files should be attached.
(galaxy series ( >= 6), xiaomi redmi note series)
i feels like the doesn 't support bn layer from tf.keras.
it shouldn 't disconnect silently, it shouldn 't disconnect at all
force=true)` ### tried solution 2. passing graph object in worker phofcode in worker: phofcode ### tried solution 3.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
2. install nvm phofcode 3.` npm install -g strapi@beta` 4. the installing process is killed phofcode
well, increasing ccuracy and no error basically
the mobilenet v1 1.0 phofhyperlink model in the guide phofhyperlink contains a squeeze operation that isn't supported by gpu, but the mobilenet v1 1.0 phofhyperlink at tensorflow/models phofhyperlink does.
- have i written custom code: yes, i am using my own training model based on mobilenetv2.
passing a tensor to `sample_from_datasets` which depends on the global step causes a sigsegv.
this is not true for `~`.
the trainable weights before and after saving were different.
however, if only one machine tries it then it works.
1. slows down the first epoch of the training process by around ten-fold.
i checked the code with (cpu version), it was ok. (= shows deterministic result) (3) using custom dense layer + tf.keras.layers.relu() was ok also.
at at at at the important line, think, is
even though this is not a public api, it seems like a step backwards to miss this information with respect to cmake system
given the initial high gpu volatile utilization rate, i was expecting the high util-rate to persist, so it was a bit strange to see that after the initial 3k steps, the training process only occasionally used gpu, and mostly relied on cpu.
- os: windows 10 - terminal: cmd.exe and gui phofurl phofurl
i expect no error when i call tf.summary.scalar() in tf2.0 in both gpu and cpu mode.
phofcode during the second initialization (the depth branch), tf complains about shape of the logits layer as if it was never removed: `valueerror: shape of variable ((3,)) doesn 't match with shape of tensor ([1000]) from checkpoint reader.` the problem doesn 't occur when i initialize only one branch, and is tied only to the first of the branches defined - if i change order branches, above error would change to
all usernames in my config just use lowercase a-z.
only one node should be shown for each key.
1. turn on `enable email confirmation` 2. register via `/auth/local/register` 3. click on link inside incoming email
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.
1. open this google colab phofhyperlink 2. run the code with alpha0 choose version phofimage 3. make note of the training history plot.
i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 fma i xla service executing computations on platform host.
also the fact that the conv layers with dilation can handle any input as long as its the first i would expect that no additional padding would be needed.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
... steps to reproduce the behavior: 1. write a request interceptor that modifies the url of loadspec requests 2. load the page 3. wait for spec to load 4. see that the spec selector box has old url
if i look at the tensor before and after the depthwise_conv2d: phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes, the code is attached below - os platform and distribution: arch linux - tensorflow installed from (source or binary): pypi - tensorflow version (use command below): aka the latest nightly python version: 3.6.8 cuda/cudnn version: 10.0 gpu model and memory: gtx 1080ti
- tensorflow version: 1.12 created a hack around this issue by building the merge from scratch with the opbuilder, but would be best to have the generated code to be correct as it provides a neat interface :) phofcode phofcode
use the metadata in tensorboard
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04):centos - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): the problem is in the fact that `sequential.layers` does not return the `inputlayer`.
if including tracebacks, please include the full traceback.
'model'))` should load the data without problems
- os: ubuntu 18.04.2 - terminal: xterm(330) n/a
phofcode and try to load it with brave browser.
large logs and files should be attached.
this may be by design but i couldn 't find anything that confirmed behavior one way or other so raising as a bug for confirmation one way or other.
large logs and files should be attached.
this is a small code snippet for reproduction.
- vim version : - os: windows 7
when using `tf.function` decorator for a function iterating over a dataset in multi-gpu, the colab notebook crashes and gives the following logs: phofcode it works perfectly without `tf.function` decorator
conv, conv2d, and backend.depthwise_conv2d support dilation_rate, but depthwiseconv2d does not.
phofcode fwiw, i get the expected result if i remove `.shuffle(...)`.
on the indexedslices the check in phofurl evaluates to true and in line 5965 .graph attribute is called which in eagerexecution causes error mentioned above.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): pip tensorflow version (use command below): 1.11 / 1.12 python version: 3.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: include any logs or source code that would be helpful to diagnose the problem.
if including tracebacks, please include the full traceback.
large logs and files should be attached.
the first 3 methods do.
using the file tmp.py with the following content: import numpy as np def mul(a, b): z = a @ b then run: .
--- i can not reproduce if c prior to compilation c i configure vim with just `--with-features=huge`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
if including tracebacks, please include the full traceback.
when passing multi-dimensional time input into a phased lstm cell, i am presented with an arithmetic error; phofcode this appears to be caused by the following line in `rnn_cell.py`, which seems to make the assumption that `time` has one dimension; phofcode
i really need help to fix this problem.
it is only reshuffled at each iteration within same epoch.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac high sierra 10.13.6 - mobile device (e.g.
i am attaching to the container produced from this particular image, and downloading algo then following the instructions.
i can understand that eager execution would create a slight overhead, however here it proves huge, while no mechanism whatsoever requires it.
... steps to reproduce the behavior: 1. go to ` phofurl 2. add a required header to the post /pets endpoint.
'program ' is not recognized as an internal or external command: quoting issues... phofimage
phofcode output: forwardpass took before outer start inner end inner; took: after outer; took: backwardpass took forwardpass took before outer start inner end inner; took: after outer; took: backwardpass took ``
image_string = tf.read_file(filename) image_decoded = # this will convert to float values in [0, 1] image = tf.float32) return image # create a dataset dataset = .map(parse_fn, .prefetch(1)) iterator patches iterator.get_next() sess tf.session() res sess.run(patches) print(res.shape) misc.imshow(res) provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - mobile device (e.g.
python import numpy as np import tensorflow as tf information = { "a": 1, "b": 2 }
large logs and files should be attached.
but it fails to run in the latest tf2.0 beta.
instead, the first and second value are always repeated every fourth element.
the form should not clear.
this can of course happen, even with code based on an example, the issue is that this only appears with beta0 and beta1 builds, not with alpha0 (see below) when using the exactly same code and training data.
on a side note, code in guide phofhyperlink seems to forget to use `reset_states` on metric at beginning of each epoch.
- have i written custom code: no, i 'm using mnist tensorboard example provided by google.
this is an issue difficult to reproduce.
* foo3 perfect, it even reduced the whole graph to single constant, congrats.
1. train a toy estimator using have its input function return a dataset.
`/expr-!=#` 5. observe: a match is found but the cursor position is wrong: it 's not at the beginning of the match (the 'e ' char) but 3 chars further (on the 'r of matched string).
1. create model with json fields 2. create entry with some json data 3. update created entry (important: do this through generated api, not through admin panel) 3. look at the response
return axis=axis)) def seg_sum(sp, axis=1, ordered=false): """ args: sp: rank 2 sparse tensor axis: int, axis along which to sum ordered: if true, other axis indices are assumed to be ascending.
should accept a tuple of `dimension` objects for shape, as other initializers do.
i want to make a custom decoder layer with a stateful lstm and a pre-net which is a simple dense, but i do not want to use sequential or functional api, because i want to control with reset_states() when the lstm states will be reset.
n/a - os & version: windows 10 1903 - redis-server version: mixed n/a
i can access the articles unauthenticated (using incognito browser) at phofurl 2. scroll down to the section in tutorial to update index.js articles list first, we want display list of articles.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):
call tape.jacobian on a conv2d network of a moderate size with
the custom ops themselves should work with tpus so i can train on tpus without compromising the performance of my data pipeline.
large logs and files should be attached.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04):ubuntu 18.04 - mobile device (e.g.
if including tracebacks, please include the full traceback.
derivatives of non-holomorphic functions should becorrect.
phofcode 1. so % 2. the
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - tensorflow installed from (source or binary): pip3 (binary) - tensorflow version (use command below): 1.13.1 python version: 3.6.7 i have ~ 1500 samples in those two tfrecord files, repeating it 6 times is 9.5k samples, so i dont think my generator ran out of samples.
no warning and building a graph successfully.
sometimes, the `'cursorline'` option is set in a window, but the text line of the cursor is not highlighted.
for the first time, tried print just values (x,y) couples of points ( landmarks ) that can be drawn on the eye region contours
), it becomes very much (~40 times) slower than floating point 32. another unexpected behavior is getting "core dumped" with most of the filter sizes, that only happens in case of separable conv with fp16, but not the other combinations.
almost same as example below: phofurl we 'd happy to attach our tflite model but github says it 's too large the beginning of our model is like this: <img width="1136" alt="` 16 43 52" src=" phofurl
if including tracebacks, please include the full traceback.
according to the docs: phofcode the data should not be included in the data.
this is a small machine that amazon has on it's free tier (t2.micro).
here 's the code to reproduce the error: phofcode
--> 613 return self.run_local() 614 615 # distributed case.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
getting error "tensorflowlite: failed to load native library: library tensorflowlite_jni not found; at /system/lib64".
i before removing unused ops: 135 operators, 224 arrays (0 quantized) i after removing unused ops pass 1: 127 operators, 212 arrays (0 quantized) i before general graph transformations: 127 operators, 212 arrays (0 quantized) f check failed: bn_op->inputs[1]) && bn_op->inputs[2]) && bn_op->inputs[3]) batch normalization resolution requires that mean, multiplier and offset arrays be constant.
i installed both local and ec2 instances from scratch, used anaconda on all of them, and installed the tensorflow 2 beta 1 binary as per instructions on the website.
i expect this script to finish normally.
to avoid this issue, the ode function has to be defined using either pure tensorflow functions, or the `call` method of the subclass instead of `__call__`.
if including tracebacks, please include the full traceback.
the loss value returned from `keras.model.evaluate()` is an arithmetic average over losses for each batch.
1) download the tensorflow mobile demo app from the link from above 2) compile the tensorflow 1.12 version from source using the mentioned cuda settings 3) download model from model zoo 4) execute bazel commands phofurl and phofurl # download and extract ssd mobilenet model wget phofurl tar -xvf # strip out problematic nodes before even letting toco see graphdef bazel run -c opt -- --input=$detect_pb --output=$stripped_pb --frozen_graph=true --alsologtostderr # run toco conversion.
alternatively, believe that it could great enable using "old-style" (not eager) tensors as layer weights through, _e.g._, a boolean option, so that in the settings when accessing those weights (as eager tensors) is not required (which, believe, is a majority of cases, especially when some keras methods allow effectively pull out the weights as numpy arrays), no overhead would implied by a (seemingly) useless eager declaration.
pressing `<c-j>` while on a directory in the wild menu should "open" that directory and restart wild mode completion.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
phofcode this example works with tensorflow 1.11 but fails with 1.12. it does not matter whether the shapes of `idx` and `update` are set explicitly or `none`.
4. do * not * place a path parameter in the operation.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 2.0.0-beta0 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.0, 7.5 gpu model and memory: 11gb, gtx1080ti you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with: 1. tf 1.0: `python -c "import tensorflow as tf; print(tf.git_version, tf.version)"` 2. tf 2.0: `python -c "import tensorflow as tf; tf.version.version)"` phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): debian 9.0 stretch - mobile device (e.g.
in __init__(self, dataset) 531 self._resource = 532 --> 533 534 self._resource) 535 # delete the resource when this object is deleted output_shapes, name) 71 return 72 name=name, ---> 73 ctx=_ctx) 74 except as e: 75 if name is not none: output_shapes, name, ctx) 89 "expected list for 'output_types ' argument to " 90 op, not %r."
i got graphic depictions of the model using `tf.keras.model.summary` and and discovered that `sequencefeatures` gets decomposed in million sub-layers type `tensorflowoplayer`, as opposite to the clean to the point depiction `embedding_column`.
bigmac-87:~ manou$ cd /users/manou/algo-master bigmac-87:algo-master manou$ python -m ensurepip --user looking in links: requirement already satisfied: setuptools in (41.0.1) requirement already satisfied: pip in (19.2.2) bigmac-87:algo-master manou$ python -m pip install --user --upgrade virtualenv deprecation: python 2.7 will reach the end of its life on january 1st, 2020. please upgrade your python as python 2.7 won 't be maintained after that date.
unit tests in fail when these dtypes are added
removing the batchnorm layer resolves the issue.
1. go to phofurl 2. add a line like `import 'leftpad ';` to the default `<script>` tag 3. click the icon for "download zip file"
when is set to `1` the `iterations` variable in nadam optimizer from always starts at `1` when the optimizer is executed; when is higher than `1` then `iterations` sometimes starts as `0`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu linux 18.04 - mobile device (e.g.
n/a - vim version phofcode - os: ubuntu 19.04 - terminal: xterm(330) add any other context about the problem here.
for tf 2.0 or 1.x phofcode for tf 1.x or cntk with keras phofcode with 8gb vram gpu, tf 1.x and cntk works successfully, and tf 2.0 code are failed due to resource exhausted exception
here is the model link phofhyperlink .
so it looks like that this place phofurl contains the bug.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
it restarts from beginning of dataset.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
(i have also tested it on mac os, and it works fine.
python with tf.graph().as_default() as graph: precision = tf.metrics.precision() 0, 1], [1, 0, 1]) is graph) # true f1 = f1() f1.update_state([0, 0, 1], [1, 0, 1]) is graph) # false ``
i will be run another cell in jupyter notebook, but i can't.
success source and transform nodes - 0.223 s success building schema - 0.196 s success createpages - 0.004 s success createpagesstatefully - 0.044 s onpreextractqueries 0.004 update schema 0.023 error graphql error encountered 1 error(s):
requirement already satisfied: markdown>=2.6.8 in (from (3.1.1) requirement already satisfied: setuptools>=41.0.0 in (from (41.0.1) requirement already satisfied: h5py in (from (2.8.0) installing collected packages: tensorflow successfully installed tensorflow-1.14.0 (base) python 3.7.3 (default, apr 24 2019, [msc v.1915 64 bit (amd64)] :: anaconda, inc. on win32 type "help", "copyright", "credits" or "license" for more information.
phofcode this causes the start up to hang for 25 seconds.
code runs for a while and then suddenly generates nans.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.14.5 - mobile device (e.g.
def decay(epoch): if epoch < 3: return 1e-3 elif epoch >= 3 epoch < 7: return 1e-4 else: return 1e-5 run(): ### get data init_gpus( ) strategy = strategy strategy strategy print( 'number of devices: setting with_info to true includes metadata entire dataset, which being saved here info.
please report this to the autograph team.
(train_images, train_labels, test_images, test_labels) = datagen = width_shift_range=0.125, cval=0.)
the results should be the same as in tf1.x
- os platform and distribution (e.g., linux ubuntu 16.04): win10 x64 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0.0-alpha0 - python version: 3.6.6
' ' ' (base) futurewarning: conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated.
want the `generate_np` method of the saliencymapmethod object to return an adversarial example for the wrapped keras model.
translations: a vector representing [dx, dy] or (if images has rank 4) a matrix of length num_images, with a [dx, dy] vector for each image in the batch.
maybe related: #17439. i noticed some slow variable creations.
large logs files should be attached.
when importing from tf.contrib a warning is raised to do with the sunsetting of tf.contrib, as expected.
it should not run the code within its "with" block when `global_step % n != 0`, or documentation should be added that describes the current behavior.
import time import math import tensorflow as tf import numpy as np tensorflow as keras from tensorflow.keras layers from tensorflow keras tensorflow.keras.backend as k from tensorflow.keras.utils plot_model from scipy .
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from pip3: tensorflow version 1.13.1 python version: cuda/cudnn version: / 7.4.1 gpu model and memory: nvidia k80; 11gb traceback (most recent call last): file line 12, in <module> kernel_size=(1, 4), strides=(1, 1)))(test_td_input)) file line 538, in __call__ file line 1603, in _maybe_build self.build(input_shapes) file line 216, build 1811, build if is none: attributeerror: 'tuple ' object has no attribute 'dims ' ``
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): reproduced on os x and ubuntu 16.04 - mobile device (e.g.
however, running predict() throws the error above.
all 592 examples in my dataset should be processed (with the counter being 592 afterwards).
3. in ui create relation field questiontype.questions (many : 1 ) question.questiontype.
phofcode (in these test scripts, `vim` is a special command that communicates with an already running `vim`/`gvim` instance over a channel.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
... steps to reproduce the behavior: 1. in your swagger ui configuration, import `swagger-ui` as `swaggerui`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): python version: 3.7 cuda/cudnn version: 10/7.4 gpu model and memory: tesla m60 on aws g3.8xlarge traceback: python failedpreconditionerror traceback (most recent call last) in <module> 24 model.fit(x_train, y_train, 25 batch_size=1024, ---> 26 epochs=2) in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing,
- have i written custom code (as opposed to using a stock example script provided in tensorflow): i would consider this an anaconda problem if only it happened at all times.
... steps to reproduce the behavior: 1. render yaml with type: string and format: date 2. open an example value in the browser
would also be willing to bet issue not specific `dnnclassifier` and occurs many different pre-canned and custom estimators.
i expect the batch norm not to make it so much slower and not to get core dumped with different kernel sizes.
compiling concatenated networks with keras metrics causes an invalidargumenterror in the input of the second network (you must feed a value for placeholder tensor 'dense_5_target ' with dtype float and shape [?,?]).
2. enter insert mode and type " phofurl 3. go to normal mode and type `gx`.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.6.8 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory:
running tf.range(b), tf.range(c), tf.range(d), indexing= 'ij ')`` with the same amount of resulting elements can have high variations, for example sometimes 1 second, sometimes up to 8 seconds).
this results in following warning in the console (two times for one evaluation).
import tensorflow as tf import numpy as np from import resnext101 from import preprocess_input, decode_predictions from keras_preprocessing image model = layers=tf.keras.layers, models=tf.keras.models, utils=tf.keras.utils) img_path = 'image.jpg ' img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x preprocess_input(x, utils=tf.keras.utils) preds model.predict(x) print( 'predicted: ', top=3)[0])
3. navigate around in the file for a few seconds.
- vim version phofcode - os: phofcode - terminal: ssh remote by xshell software in ms-windows build vim: phofcode
image phofimage code: phofcode error: phofcode
setting the `steps_per_epoch` parameter in `model.fit` (`tf.keras`) to for example 10 should only perform 10 updates per epoch.
i'm using tf_agents library phofcode
provide a reproducible test case that is the bare minimum necessary to generate the problem.
this may lead to a quantized model that produces erroneous results; however, i currently cannot confirm if the said behavior is causing erroneous quantization.
there should not be an error.
'x ' button close 19. available authorizations value edit box 20. inspect "parameters" & "responses" tables 21. try it execute curl 22. models order expand/collapse carrot 23.
epoch 1/2 invalidargumenterror traceback (most recent call last) in <module> 33 batch_size=2048, 34 epochs=2, ---> 35 shuffle=true) in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing,
i don 't understand this error, because i do provide the `validation_data` argument to the `fit_generator()` function.
when window size is 80 columns or less.
i tested both jcenter downloaded (0.0.0-nightly branches) and manually built from r1.14 branch tensorflow-lite and tensorflow-lite-gpu.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
my entire codebase will be shifting tf2.0 api once thats out alpha.
with partitioner dividing embedding matrix to each ps, the training process should also converge quickly.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
large logs and files should be attached.
its a little complicated to share ready to run code due to privacy.
the tag selection menu should appear, showing one tag (the function from `test.sh`).
this issue is about the error message, not the error itself (i should have called the variable 's assign() method).
this was not the case with the in-memory database.
during (see the tensor feeding a `relu` operator shows that its minimum value is 0 instead of a negative number.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubunut 16.04 - mobile device (e.g.
maybe this is intentional, but i figured i should report it if it isn't.
my `script.py`, reproduce with `python3 script.py` phofcode
gpu model and memory: ?
when fitting a model with `loss="poisson"`, i would expect reported `loss` and `poisson` values to be identical.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from: binary tensorflow version: tensorflow-gpu python version: python 2.7.12 bazel version: n/a gcc/compiler version: n/a cuda/cudnn cuda 9.0 / cudnn gpu model and memory: (geforce gtx 1080ti / 11172mib) x 2 error logs from up code running in v1.12.0 runconfig with distribution strategies.
after upgrading to tf 1.14.0 from tf 1.13.1, i 'm getting the error below: phofcode the training script continues w/o any further issue despite the error.
lstm model would be saved in the savedmodel format to be exported into a google cloud bucket to work with google 's ai platform.
type casting `tf.cast` from `tf.int32` to `tf.uint32` will make the tensor become 0.
options provided in the pipeline.config phofcode have no effect on this.
it seems that the graph is successfully created, but evaluation of `tf.nn.conv2d` call (which is implicitly placed on gpu) is not possible.
a1: - qualcomm msm8953 snapdragon 625 - octa-core 2.0 ghz cortex-a53 - adreno 506 a2: - qualcomm sdm660 snapdragon 660 octa-core (4x2.2 ghz kryo 260 & 4x1.8 ghz kryo 260) adreno 512 *this is similar to other devices, e.g.
all seams to work perfectly but when i try to update strapi-plugin-graphql the console exit with code 1.
if including tracebacks, please include the full traceback.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - tensorflow installed from (source or binary): binary (anaconda) - tensorflow version (use command below): 1.13.1 python version: 3.7.3 cuda/cudnn version: gpu model and memory: nvidia titan x (pascal), 12 gb here are the input tensors used in the example: input_tensors.zip phofhyperlink .
python from __future__ import absolute_import from __future__ import division from __future__ import print_function import tensorflow as tf x0 = tf.get_variable('x0', shape=(), dtype=tf.float32) x1 = tf.constant(3.)
despite installing python3 and running the install certificates command per the documentation (downloaded python3 per docs + ran the install certs command + commands for prereqs)
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below): python version: 2.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.0 gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with: 1. tf 1.0: `python -c "import tensorflow as tf; print(tf.git_version, tf.version)"` 2. tf 2.0: `python -c "import tensorflow as tf; tf.version.version)"`
for this, i added a custom loss function, which feeds the model the adversarially perturbed input and adds the cross-entropy to the loss.
running the same input code with a standard estimator on cpu/gpu works just fine.
for some reason, when i am working on that machine, i 'm not getting that error.
if you uncomment the two lines that define the `build()` method, then everything works fine.
if i do not modify any file error does not happen.
v1 models should load correctly, or present the user with a way to migrate the model to a more compatible format.
phofurl phofcode this code dies at model.compile with `recursionerror: maximum recursion depth exceeded while calling a python object`.according to the stack trace, it seems like it falls into an infinite loop with `hasattr` and `layer.trainable = value`.
1. create a model `customer` with a relation to `users-permissions` : phofcode 2. use `curl` and replace `xxx.xxx.xxx-xxx` by your own bearer token (in admin mode) and try to create a new customer : phofcode 3. check your database and see if the user is set correctly image phofimage 4. retry many times... 5. in my case, i need to retry 4 times before the last is set successfully image phofimage
here is trace: i your cpu supports instructions that this binary was not compiled to use: avx2 traceback (most recent call last): file flow/programs/java examples/cats vs dogs/cvd.py", line 128, in <module> model.fit(x_train, y_train, epochs=2, steps_per_epoch=500) file line 1509, in fit file line 993, in _standardize_user_data class_weight, batch_size) file line 1029, _standardize_weights self._set_inputs(x) 426, _method_wrapper method(self, *args,
after loading graph created a handler image input, previously input during training was getting handled by queuerunner, but because it 's just inference created handler myself with appropriate modifications it as can be seen below.
* needs to be run on all up-to-date software due to possible security issues; it won't be the case after python -m virtualenv --python=`which python2` env && > source env/bin/activate && > python -m pip install -u pip virtualenv && > python -m pip install -r requirements.txt running virtualenv with interpreter /usr/bin/python2 new python executable in also creating executable in installing setuptools, pkg_resources, pip, wheel... done.
* tflite micro 's fastint32tobufferleft phofhyperlink phofcode * tensorflow 's fastint32tobufferleft phofhyperlink phofcode
phofcode using this code piece, tensorflow throws an error: `attributeerror: 'nonetype' object has no attribute 'shape'`
steps to reproduce the behavior: see log below.
w0705 from the name tf.logging.set_verbosity is deprecated.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12.0 python version: 3.5.2 cuda/cudnn version: 9.0.176 / 7.4.1 gpu model and memory: titan x, 12189mib i ran the above script with 0 or 1 gpu, both producing: `python test.py image phofimage `python test.py --use_get_variable=true` image phofimage
[+] conda create -n kerasenv python=3.5.0 [+] conda activate kerasenv [+] pip install tensorflow [+] git clone phofurl cd keras sudo python setup.py install
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - mobile device (e.g.
here is a colab notebook that demonstrates the issue: phofurl
the loss after 1 or 2 epochs is supposed to be around 0.3 the validation loss is supposed to be a little less than 0.3. the graph below this code was produced using keras and a tensorflow 1 backend.
it should be able to import tensorflow.
phofcode output i your cpu supports instructions that this tensorflow bi nary was not compiled to use: avx512f 32bit -- 0 16bit -- 1 32bit -- 2 16bit -- 3 32bit 4 16bit 5 32bit 6 16bit 7 8 9 10 11 12 ``
when i use primitive type objects during building model with tf.keras functional api.
however when i try to run it with gpu delegate option i observe following error:
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary, pip install tensorflow==1.12.0 tensorflow version (use command below): 1.12.0 python version: 3.6.4 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: no usage gpu model and memory: no usage
bin (512): ttotal chunks: 0, chunks use: 0.
... steps to reproduce the behavior: 1. go to url 2. click on users 3. see error
while using a `tf.estimator.estimator` class, each separate call of `.train`/`.evaluate` results on extra cpu ram permanent usage.
pop phofimage - vim version : - os: centos 7 - terminal: gnome terminal it would be much appreciated if the pop up window can be focusable.
the second issue is due to a combination of several factors.
if tensorflow is imported before torch, it will work as well.
`gradienttape.jacobian` is showing warnings about <function pfor.<locals>.f at could not be transformed and will be executed as-is.
large logs and files should be attached.
when `minwidth` and `maxwidth` are set to the same value, i would have expected the popup window to keep that value at all times.
i have a custom metric `def y_pred): return - y_true)))` while training i get the following output: > train on samples, validate on 40385 samples > epoch 1/5 > - 132s - loss: 3.6781 - root_mean_squared_error: 3.6781 val_loss: 3.6463 3.6463 > epoch 2/5 129s loss: 3.6528 root_mean_squared_error: 3.6528 val_loss: 3.6287 3.6287 epoch 3/5 143s loss: 3.6352 root_mean_squared_error: 3.6352 val_loss: 3.6210 3.6210 epoch 4/5 134s loss: 3.6223 root_mean_squared_error: 3.6223 val_loss: 3.6369 3.6369 5/5 after running val_label, verbose = 2)` i get the following output: for `model.metrics_names` [ 'loss ', after running `from sklearn.metrics import mean_squared_error batch_size=256))))` i get
large logs and files should be attached.
when using `strategy = is crashing during training with ` runtimeerror: method requires being in cross-replica context, use
info:tensorflow:saving checkpoints for 0 into info:tensorflow:saving checkpoints for 1 into info:tensorflow:calling model_fn.
this later on results in vim waiting for a message that will never arrive.
> $ python3 test.py [info|test.py:29] >> hello world.
- os platform and distribution (e.g., linux ubuntu 16.04): windows10 - mobile device (e.g.
installing setuptools, pkg_resources, pip, wheel... complete output from command /root/env/bin/python2 - setuptools pkg_resources pip wheel: collecting setuptools downloading (573kb) collecting pkg_resources exception: traceback (most recent call last): file line 215, in main status = self.run(options, args) file line 353, run file line 749, build file line 380, prepare_files 554, _prepare_file require_hashes 278, populate_link self.link = upgrade) 465, find_requirement all_candidates = 423, find_all_candidates for page project_name): 568, _get_pages page = self._get_page(location) 683, _get_page return htmlpage.get_page(link, session=self.session) 795, get_page resp.raise_for_status() 935, raise_for_status raise response=self) httperror: 404 client error: not found for url: ...installing setuptools, pkg_resources, pip, wheel...done.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macos - tensorflow installed from (source or binary): pip - tensorflow version (use command below): 1.13.1 python version: 3.7.3
is it possible that having the urls.primaryname in my url is causing a problem?
includes the header data in the dataset
since the shape of `x` is known at compile time, the constant folder should be able to fold away these ops to create a single const for the new shape for reshape.
my code is provided below.
large logs and files should be attached.
- os & version: [e.g.
large logs and files should be attached.
`tf.scatter_update` keeps failing with typeerror
this is the model in question: import tensorflow as tf import tensorflow.keras.layers as layers from tensorflow.keras import model class mrf_block(model): def __init__(self, out_channels, num_inputs, use_deconv=true, kernel_size=(3,3), strides=(1,1), padding='same'): super(mrf_block, self).__init__() # add the conv/deconv layers.
thank you very much, gilad
is 'cursorline ' local to window or local to buffer?
phofcode next inspect the saved model: phofcode notice that the `method_name` is empty.
the code below won 't generate any errors.
runtime on honorplay(cor-al00) is 3 times higher than pocof1.
func() but in fact, i get: fu!
every epoch should roughly take the same time
large logs and files should be attached.
loading 1.4mil 100dim embeddings into the graph dim:100
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no.
--- i think that height of last which entered `6` because it 's height of old command-line (`4`) + `2` for status line and single text buffer line which was above.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0.0.b0
func() return "*/ <plug>(up) " endfu the `<plug>` mapping is ignored in any terminal whose `term` value is `xterm` (or some derivative).
i think converter should be support scalar shape placeholder, because tf users usually need scalar shape placeholder when they set dynamic params (hyper params, dropout, ect...) in neural network.
large logs and files should be attached.
if including tracebacks, please include the full traceback.
i would expect to be able to train an lstm using `sample_weight` and `batch_size > 1`.
tf.io.decode_image() and tf.image.resize() should work without any exception in dataset.map().
i 've boiled down the problem to a mwe.)
afaik, squeeze isn't a supported operation, but both of them contain at least one.
we will be working on improving this in future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get best performance.
i don 't have problem in anaconda.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): amazon linux ami - mobile device (e.g.
1095 x, _, _ = -> 1096 x, check_steps=true, steps_name='steps', steps=steps) 1097 1098 if (self.run_eagerly or (isinstance(x, in x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle) 2380 feed_input_shapes, 2381 check_batch_axis=false, # don't enforce the batch size.
i reinstall the certifi, openssl, cryptography, ca-certificates.
on the other hand, if the model is built using functional api, everything works as intended.
the response headers should be very similar since the call is made to the same endpoint
using theano i get 28 seconds by iteration.
1. create a model (a) with string field called "value" 2. add an entry in this model 2. create another model (b) which has a one-way relation with the previous one 3. and an entry in this model and set a value for the related a 4. try to save 5. an error should appear on top of the page and a `error error: invalid input syntax for integer` should appear in the logs
large logs and files should be attached.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
in run: ckpt_dir="ckpt" python train.py --logtostderr --data_dir=$data_dir --architecture=resnet --imagenet_norm=true --joint_encoder=false
when the install script is run as a user who is an administrator all is good (elevation prompt and everything works).
ctc has only a cpu implementation, and the gpu backed offloads it to cpu, why isn 't this also case for tpus ?
i know tensorflow has tried to avoid leaking logging macros (#7480), but phofhyperlink is leaking them again.
once the function is done executing the models are no longer needed.
detailed steps to reproduce the behavior: 1. run `gvim` with no custom vimrc or vimfiles.
can view the data in string or bytes format, not crash.
the performance is 2-3 times slower as the keras model.
- vim version - os: windows 10 1803 - gvim
maybe it should give an exception.
the test program throws an exception retval[0] does not have value` in session run.
while training my model i noticed that my system would be out of memory after a few minutes.
... steps to reproduce the behavior: 1. open the operation.
r options(digits=8) fake <- function(shape_) { # arbitrary but reproducible %% 2.71 - 1.04, shape_) } library(keras) shape <- c(30,5) model <- keras_model_sequential() %>% layer_lstm(units=2, input_shape=shape) %>% 8)), fake(c(2, 8)), fake(8))) n <- 11 # not a multiple of 4 x array(rep(fake(shape), each=n), c(n, shape)) # n copies of identical input p model %>% predict(x) # all predictions should match p but last n%%4 rows differ #> [,1] [,2] #> [1,] #> [2,] #> [3,] [4,] [5,] [6,] [7,] [8,] [9,] [10,] [11,] (t(p)-p[1,]) * 2
i do get an ipv6 address using wireguard on my android device (kernel-level [rooted] wireguard) and my ipad (ios wireguard client).
input_arrays = ["reshape_1"] # this is the name of the input node output_arrays = ["labels_softmax"] # this is name of ouput node converter = input_arrays, output_arrays) converter.inference_type input_arrays {input_arrays[0] : (227., 0.92)} # mean, std_dev tflite_model converter.convert() the resulting .tflite file.`
lstm computes phases and `shifted_time` for * each * parameter separately (in this case, this would require `shifted_time` and other relevant variables to be the same size as the number of features in the input - e.g.
sigabrt phofimage terminal 1: vim terminal 2: gdb some upper and lower lines (like command line) were cut due to capture area restriction.
`:bp`, switched to a.txt, cursorline is displayed.
phofcode you can see many [i] log using but with no logs
steps to reproduce the behavior: 1. choose to deploy a server on vultr 2. prompt says:` enter the local path to your configuration ini file ( phofurl ` 3. that url give a 404 not_found error
when i try to run one of the distributed examples, the model fails using the default nccl all reduce cross devices ops: <details> <summary> nccl all reduce error </summary> internalerror traceback (most recent call last) in <module> ----> 1 model.fit(train_dataset, epochs=50, callbacks=callbacks) in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing,
[info|test.py:30] >> this is message 1 i0314 test.py:30] this is message 1 when i try without importing tensorflow, then it works properly.
now only user-permissions folder was created.
1. install latest strapi 'npm install strapi@beta -g ' 2. create new project 'strapi new test ' 2.1. choose manual method 2.2. use mysql server 3. go to the test directory and run server 'strapi develop ' 4. everything works 5. stop server 6. add graphql plugin 'npm install starpi-plugin-graphql --save ' 7. try to run strapi again 'strapi develop ' 8. get error "typeerror: cannot read property 'routes ' of undefined"
this is my cli command that works: phofcode and this is my attempted python file.
when run with more then one worker it fails: phofcode the negative value in check seems to depend only on `drop_remainder` argument to `padded_batch` method and not on: `num_inputs`, `batch_size`, length of dataset, `epochs`.
in the original notebook example, it prints out evaluation loss without error
while my issue arises with custom code using model.fit_generator, i was able to replicate the issue using model.fit with documentation code provided at phofurl phofcode
* if you can demonstrate bug using phofurl please do.
however, when i directly feed in the same tensor to the keras model it gives output as expected.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: honor 10 view (with kirin970 and npu) - tensorflow installed from (source or binary): pip install tensorflow version (use command below): tf: 1.5.0 python version: 2.7.15rc1 bazel version (if compiling from source): gcc/compiler version (if compiling from source): gcc 7.3.0 cuda/cudnn version: none gpu model and memory: none i compiled demo app twice using two different bazel commands: case 1. bazel build -c opt --cxxopt='--std=c++11' case 2. bazel build --cxxopt=--std=c++11 --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
provide a reproducible test case that is the bare minimum necessary to generate the problem.
when training an estimator which is got from a tf.keras.model instance, the method fails to log anything on the stdout.
have i written custom code (as opposed to using a stock example script provided in tensorflow): no os platform and distribution (e.g., linux ubuntu 16.04): centos mobile device (e.g.
not hang at the middle of training, the final train log are phofcode
... 1. open phofurl phofhyperlink 2. edit the get path /pet/{petid} to allow enums instead of input (full file here swagger-bug.txt phofhyperlink ) 3. try out by selecting id 1 and executing ==> correct result 4. change enum value to id 2 and execute ==> still trying to get id 1
'[on]' : '' endfu set stl=%{status()} then, start vim like this: vim -nu none -s /tmp/vimrc finally, press `i` to enter insert mode and `c-b` to invoke the mapping.
on a related note, phofurl mentions that java is planning to stay `session` centric.
the characters do get echo'd screen (as note says about gnome terminal), but as far as i can tell it does seem respond at all, and thus causes vim wait at quit
provide a reproducible test case that is the bare minimum necessary to generate the problem.
python import tensorflow as tf from tensorflow.keras.layers import input print(tf.__version__) input_ = input((128, 128, 1), dtype='float32') print(input_) output = tf.stack(input_, axis=1) ``
`:e b.txt`, no cursorline 4.
now however corresponding bazel build rule in only provides build type (`"cuda"` or `"cpu"`) and recently also `msvcp_dll_name` on windows.
in my code i train my model like so: checkpoint = monitor="val_acc", save_best_only=true) steps_per_epoch=100, epochs=20, validation_steps=100, callbacks=callbacks) however, with that code i do get the following warning message: warning:tensorflow:can save best model only with val_acc available, skipping.
whether previous function call raised assertion error or not, function result should be independent of previous function call
when running the tflite model through python the results are very different to the original model when no quantisation has been applied.
a snakeviz flamegraph for prediction of our network in community keras, showing no unnecessary overhead: community keras phofimage
i am looking for advice if using different allocator would help, for example.
tensorflow doesn 't throw oom errors.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04):linux ubuntu 16.04 - mobile device (e.g.
the documentation could be a little expanded on that matter by the way.
if applicable, copy/paste the text or add screenshots to help explain your problem.
i froze it using function and tried to convert it to tflite formate for quantization.
approx 2 sec tf.0.12 approx 70 sec tf1.12.0 the creation of graph tf.1.12.0 is progressively getting slower for each extra added layer..
info:tensorflow:running training and evaluation locally (non-distributed).
@registry.py:133] conv0 output: [none, 64, none, none] @registry.py:125] pool0 input: [none, 64, none, none] @registry.py:133] pool0 output: [none, 64, none, none] @registry.py:125] group0/block0/conv1 input: @batch_norm.py:164] wrn [batchnorm] using in training.
build a small lstm with keras for sequence prediction.
when running `estimator.train()` in a loop n times, there will be n `tf.graph()` instances allocated in memory.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i have done this several times before so i am not sure why i am all the sudden getting this error.
vim keeps scrolling the pum for several seconds.
i have trained a 2 classified resnet18 when i use it for inference on different machine, the time is as follow: about 2.0s on my conputer about 1.6s on my workmate 's computer (win10, tensorflow-gpu of version 1.9, python 3.6.6, cuda9.0, nvidia geforce gtx 1080 ti 11264mib) about 0.65s on a server (ubuntu16.04, tensorflow-gpu of version 1.9, python 3.6,2, cuda9.0, nvidia geforce gtx 1080 ti 11171mib), about 0.22s another server (ubuntu16.04, tensorflow-gpu of version 1.8, python 2.7.13, cuda9.0, nvidia geforce gtx 1080 ti 11171mib).
there is a mismatch in the name of and the name.
i am unable to find actual time to execute those operations in streams.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:na - tensorflow installed from (source or binary):no tensorflow version (use command below):1.10.1 python version:2.7 bazel version (if compiling from source):na gcc/compiler version (if compiling from source):na cuda/cudnn version:9.1.85 gpu model and memory: tesla p4, 7gb
the stock example of rnns with multiple inputs from here phofurl produces an error if you set `stateful=true`.
1. select your database 2. try to expand a large group of keys.
i am able to create the saved_model in my colab using the following code: phofcode this created a folder with the pb file and assets and variables folder.
... steps to reproduce the behavior: 1. go to '...' 2. click on '....' 3. scroll down to '....' 4. see error
notice `at step = 6` in first epoch: phofcode
large logs and files should be attached.
large logs and files should be attached.
if i create the dataset inside the annotate function code works as expected.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the `training` flag should be passed to and to each stacked cell.
returns: rank 1 dense tensor equivalent to tf.sparse.reduce_sum(sp, axis=axis) """ if sp.shape.ndims != 2: raise notimplementederror other_axis 0 if axis in (1, -1) else 1 if ordered: return sp.indices[:, other_axis]) else: sp.indices[:, other_axis], compare(dense_shape, axis=1, ordered=false,
large logs and files should be attached.
importing tensorflow should not influence the behavior of other toolkits.
tf.load_op_library is working correctly with tensorflow 1.8 on my configuration.
it returns the same error even when lr() is used.
pkgutil now fails when trying to find tensorflow.
so this simple graph should not have any quantize/dequantize at tf.split ops?
i want to use the model in mobile phone i converted the model using following code phofcode
note that this fails even when tf.map_fn operates over convolution filters.
--> please include the following block of text when reporting issues: algo running on: ubuntu 19.04 (virtualized: vmware) created from git clone.
this can result in oom errors when running prediction afterward on gpu.
large logs and files should be attached.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i think the behavior of training data and validation data in keras `model.fit` should be consistent.
- - ubuntu 19.04 - gnome terminal, tmu
detailed steps to reproduce the behavior: 1. run `vim --clean` (or `gvim --clean`, etc.)
see the notebooks in this dedicated repository phofhyperlink for the complete code.
1879 """ -> 1880 zero = _to_tensor(0., x.dtype.base_dtype) 1881 inf = _to_tensor(np.inf, x.dtype.base_dtype) 1882 x = zero, inf) _to_tensor(x, dtype) 612 a tensor.
i used tablet pc with windows home 10 with last stable vim.
random output should be different on each call.
the example code should allow easy reproduction of issue.
error: unsupported type for iota t [{{node = _retval[t=dt_float, index=9 phofhyperlink ]] t [{{node = targs=[dt_int64, dt_int32, dt_float, dt_int64, dt_int64, dt_int32, dt_float, dt_float, dt_float, dt_float], tconstants=[dt_int32, dt_int32, dt_int32, dt_int32], _xlanumconstantargs=14, _xlanumresourceargs=0], phofhyperlink ]] t [{{node = tensor_type=dt_int32, phofhyperlink : file line 446, in <module> tf.app.run(main=main, argv=[sys.argv[0]] + unparsed) 125, run _sys.exit(main(argv)) 356, main flags.bleu_threshold) 274, train_schedule 354, train loss hooks, saving_listeners) 1207, _train_model return hooks, saving_listeners) 1241, _train_model_default saving_listeners) 1471, _, loss estimator_spec.loss]) 671, run 1156, run 1255, run raise 693, reraise raise value 1240, return self._sess.run(*args,
am doing something wrong or is this a bug?
i'm caching a costly database query using redis, and trying to view the entry within rdm.
to be sure, removed all lines referring to them: < phofurl < phofurl then recompiled vim, so that output of `:set termcap` same regardless of whether `t_rv` set.
have observed this issue does _not_ occur optimizers _do not_ save state (see below).
if it is local to window, the behavior should not be local to buffer.
when printing the variable `i`, it 's clear that the loop just never stops, i.e.
... steps to reproduce the behavior: 1. open the swagger-ui in a browser with the `configurl` query parameter 2. refresh the page to verify swagger-ui will load either pet store example or `urls` openapi definition.
training in c++: phofcode prediction in c#: phofcode
phofurl this code does not record anything in eager execution mode.
the model runs as described in the tutorial.
i could not find reproducing code.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): ubuntu 14.04 - os platform and distribution (e.g., linux ubuntu 16.04): - mobile device (e.g.
the score of _precision_ should agree with that of _tf.metrics.precision_.
after computing logits, i want to compute softmax activations : `probabilities = tf.nn.softmax(logits)` with logits of shape which shouldn 't be a problem since softmax internally reshapes it to a 2d tensor.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
phofcode below code to convert to coreml.
the official keras api is documented at keras.io, and it shows an example using `model_to_dot()` ( here phofhyperlink ).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04):linux ubuntu 16.04 - mobile device (e.g.
i was trying to compile a simple adding function: phofcode until this line, both keras and `tensorflow.keras` allow me to run without errors or warning.
phofhyperlink this zip file contains debug.pb (tf freeze graph model) and debug.tflite (tflite converted model from frozen model).
i double-checked this was correct.
i would be happy to know about a workaround, in case one exists.
invoking `xla.compile` in the function passed to as in the code below raises the following exception (full traceback below): phofcode
1. i create model using keras functional api.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
warning: logging before flag parsing goes to stderr.
but they don't contain the same values.
... you can use the second example to see this behavior.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
... steps to reproduce the behavior: 1. create an asp.net core web api or web application project 2. install swashbuckle.aspnetcore via the normal nuget.org instructions 3. configure a custom route prefix (changing the options per the linked documentation) 4. navigate to swagger ui endpoint _without_ final slash (so you hit built-in redirect)
- have i written custom code (as opposed to using a stock example script provided in tensorflow): _yes, a minimum repro is below_ - os platform and distribution (e.g., linux ubuntu 16.04): _linux ubuntu 16.04_ - mobile device (e.g.
the second time, you can launch ./run_training.sh 1 25 directly in order avoid downloading training data
(note example is not updated to xml - this is issue #5460).
set tflite interpreter gpu delegate on androidit 's output is different without gpu delegate, and it seems that whatever data i fed, the model output is the same, is it something wrong with my code nd i have checked my code, it 's output is correct without set the gpu delegate.
perhaps due gil, this info was not printed when process hanged, but when i interrupted process with ctrl+c.
with tensorflow 1.14.0, the tutorial works just fine, but with 2.0.0 it runs out of memory.
if including tracebacks, please include the full traceback.
installation proceeds with ipv4 only and succeeds.
batchnorm should not affect a model 's ability to make good predictions.
error: unsupported type for iota w op_requires failed at xla_ops.cc:408 : internal: error evaluating input 0 as a compile-time constant.
phofcode the `itertest()` call fails because `runtimeerror: dataset.__iter__() is only supported when eager execution is enabled.` while the `loopiter()` call fails because function is not defined.
receive an attribute not found error on
np.matmul(a, b) should be very close to tf.matmul(a, b) when a, b are both float64s.
additional info: the expected behavior occurs when creating `gradienttape` with persistent=true and replacing last line of code with `g.jacobian(y, x, however, that workaround is extremely slow with real data.
calling after `tf.executing_eagerly()` fails with: valueerror: must be called at program startup.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
below are the accurate arguments used in `tf.dataset` creation and its consumption by `tf.keras.model`.
min-leak-example-mkl.txt phofhyperlink phofhyperlink i 've been working on a solution i can propose.
it is not possible to provide code for every issue.
it should display the same information regardless the number of calls.
should be simple to reproduce.
i 'm getting an error while training.
i am trying to quantize a model that is made using tfslim library.
`set nocursorline` in vimrc 1. vim a.txt 2.
i wrote a simple demo using tflite opengles delegate to run deeplab models from model zoo phofhyperlink .
the problem is during epoch.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - mobile device (e.g.
i 'd rather not post this code publicly, any way for me to share it with the dev 's without pasting it here?
i try to convert model with tfliteconverter, but get errors of this kind: `array conv2d/biasadd does not have minmax information, and is not a constant array.
info: elapsed time: critical path: 93.66s info: 3606 processes: 3606 local.
my code is structured as <br /> phofcode here each of my `images` is a numpy array with a size of [10, 1536, 1536, 2].<br /> while the code running, the memory usage is growing fast -- every image set it read in stayed there in the memory.
- os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - tensorflow installed from (source or binary): pip3 installation - tensorflow version (use command below): 12.0 - python version: 3.6.7
`x=[[[1,2,3]], [[4,5,6]], [[7,8,9]], ...], and it still doesn't work) then, build my model: phofcode and then train it: phofcode it correctly reports the number of train validation samples, then progress bar pops up... but that's where success stops.
below is python code to export the graph: phofcode below is go code to access flatten operation: phofcode executing above produces following output: phofcode
android library fails to load inceptionv3 model converted to tflite.
unless i misunderstand, the gradient when using peephole connections is incorrect.
on other machines i have used older versions (tf 1.10.0 with cuda 9.0) and it worked fine.
a `valueerror` should be produced in response to the incorrect input shape, whether or not eager execution is enabled.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): centos 7.5 - mobile device (e.g.
version 1.10.1, 1.10.0, and 1.9.0 do not segfault.
the random seed set via should be set in the context in which the functions passed to `tf.data.dataset#map` are invoked, at least for the single thread case.
phofcode - debian unstable - gnome terminal and iterm2 i believe this happens because, when switching to a tiny vertically split window, vim helpfully makes it bigger so you see what you type.
training and evaluation should happen fine.
large logs and files should be attached.
now, my issue is that i actually need to implement a function that works on keras symbolic tensors and uses a `tf.while_loop`, which seemingly proves impossible (apart from disabling eager execution, which workaround i am comfortable to use in long term, but i does not feel like an actual solution).
when i try to run on android, i saw below issue.
probably easiest to load the following text into a buffer... set shell=bash -c set shellcmdflag= set shellxquote=' set shellquote= shellredir=> noshelltemp shell?
when using multi_gpu_model (i.e., in tensorflow 2.0 to distribute a job across multiple gpus (4), only one gpu appears to be used.
compile the above, and look at the invalid code that's generated.
large logs and files should be attached.
the estimator apparently does not learn the data, which can be observed by a constant loss (in evaluation) and a stagnant loss during training.
`tf.keras.layers.dense` should be correctly instantiated when defining a model using keras model subclassing api, just like how sequential and functional apis behave.
open any .vim file and enable syntax highlighting.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): using tensorflow in anaconda tensorflow version (use command below): 1.12.0, 1.13.1, tf-nightly python version: 3.6.xxx as asked by @jdduke , i filled this issue.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): linux centos 7 - mobile device (e.g.
reproducible in colaboratory with tf 1.14.0 phofurl phofcode
i am following this tutorial: phofurl and using a model i created in tfv2.
the master and ps works properly.
num_filters1 16 there are 16 of these filters.
1. configure a custom admin path in i.e., "/panel" 2. rebuild strapi admin 3. go to ' phofurl 2. click on 'forgot your password?.... '
* install cuda 10, cudnn 7.4.2 * install tensorflow-gpu 1.13.1 (or and keras-retinanet 0.5.0 into a virtualenv from pypi * download pre-trained coco model from here phofhyperlink * load the model using `model = (will generate first blob of output above) predict bounding boxes on an image via axis=0))` (will generate second blob of output above)
i am convinced there's something wrong with either code posted in tutorial or with `tf.nn.nce_loss` function, as i don't see any problem with code wrote.
the aws s3 implementation can 't handle weird path
but don't get why we would want squeeze it in first place.
empty space due to translation will be filled with zeros.
if i convert the lists to tf arrays as input, it runs without error.
in tensorflow 1 i was able to create lots of nodes in a graph using a for loop, when using `tf.function` autograph replaces this with a `tf.while_loop` which seems to be slower in some cases.
`docker run --cap-drop=all -it -v /users/sam/vpns:/data trailofbits/algo:latest`
use valid tensor id for bias) passes `allocatetensors` and `invoke` without any problem.
text runtimeerror traceback (most recent call last) in <module>() 1 resolver = ----> 2 3 tpu_strategy = in 89 # pylint: enable=protected-access 90 ---> 91 with 92 output = 93 args=[], device_ordinal=0, tout=[dtypes.string], f=func_name) in 41 [x for x context.list_devices() if "device:tpu:" x]) 42 if not tpu_devices: ---> 43 raise runtimeerror("could not find any tpu devices") 44 spec = 45 task_id spec.task runtimeerror: could not find any tpu devices ``
if including tracebacks, please include the full traceback.
<c-u>")` should work just like the other scrolling keys.
i was going to implement the android repository(i edited the androidmanifest.xml a little so only the detectoractivity is going to launch) in android things when it failed to open the camera
the documentation phofhyperlink of the `tf.saved_model.save()` function shows a code example to save a custom model with a `serve()` method, but it fails: 1. when subclassing the `keras.models.model` class, it is compulsory to define a `call()` method (or else you get `notimplementederror: when subclassing the model class, you should implement call method.`.
when filing bug, set verbosity 10 (on linux, `export autograph_verbosity=10`) attach full output.
if including tracebacks, please include the full traceback.
however feedable iterator api does not support tf.varlenfeature(namely `sparsetensor`), varlenfeature is useful for recommend system.
i want to rotate the image and the points together.
found 1 target... target up-to-date: elapsed time: 3.214s, critical path: 0.01s 0 processes.
in tensorflow keras, the `input_tensors`, `output_tensors`, `output_shapes` of `class node` was a list in tensorflow 1.13.1, even if it only contains one tensor.
`./algo` gets stuck at "gathering facts" after creating the server.
the `:help expr-!~?` command should work right after vim start (step 2).
expected: train first run: 4 steps -> features 0 to 3 save checkpoint model-ckpt and input-ckpt 4 restore model-ckpt and input-ckpt train second run: steps -> features to 7 save checkpoint 8 phofcode
this is how batch normalization is implemented (within a convolution block): phofcode below is the code to train and save the model.
i expect tf.gather to work for the case in which the params argument is an attribute of the tf.keras.layers.layer, just as it does for the other cases mentioned.
note that it works correctly after doing a browser refresh after server url has been changed.
3. press "explore" 4. it gives the following error: > # failed to load api definition.
the names of the outputs should not change.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): (this is the tf 2.0-preview) python version: 3.6.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: here is the output of the program above: tf.tensor([1.
all tests use adam optimizer, sparse categorical loss.
the error comes from the declaration of a tensor::tensorshape supposed to contain dimension of my input tensor, prior to filling a tensorflow::tensor with my data contained in a cv::mat.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below): python version: python 3 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" 1.8.0
traceback (most recent call last):` `file equest.py", line 1318, in do_open file line 1239, request url, body, headers, encode_chunked) file line 1285, _send_request self.endheaders(body, file line 1234, endheaders file 1026, _send_output self.send(msg) 964, send self.connect() 1400, connect 407, wrap_socket _context=self, _session=session) 814, __init__ self.do_handshake() 1068, do_handshake 689, do_handshake ssl.sslerror: [ssl: certificate verify failed (_ssl.c:833)` `during handling of the above exception, another exception occurred:` `traceback (most recent call last): "<stdin>", 1, <module> 324, new_func return func(*args,
description should appear scopes infos should not appear if i don't want to.
(issues related to the runtime files should be reported to their maintainer, check the file header.)
if we replace `tf.while_loop` with `while_v2.while_loop` then we end up getting an error: `in op input types ([tf.float32, tf.float32, tf.float32]) are not compatible with expected types ([tf.float32_ref, tf.float32, tf.float32])` since this shows that gradient descent is being fed a list of length 3, but the placeholders have a length of 2, i think it might be something around counter getting added.
then run `make test_eval.out` in the testdir.
first 88 operations will run on the gpu, and the remaining 5 on bias->type != input_type (10 != 1)node number 90 (conv_2d) failed to prepare.
if `set_param` is used because key `context` was not found in `params`, then is invoked before setting parameter.
number of `call` invocations plotted against memory allocated in gigabytes phofimage
[[{{node e component function execution failed: failed precondition: error while reading resource _anonymousvar4 container: localhost.
after quantization with phofcode , the interpreter fail to `allocate_tensor()`.
steps to reproduce the behavior: 1. git clone phofurl 2. cd algo 3. sudo apt-get update && sudo apt-get install build-essential libssl-dev libffi-dev python-dev python-pip python-setuptools python-virtualenv -y 4. vim config.cfg 5.
the two snippets of code above should be equivalent, that is, they should both run successfully.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macos / ubuntu 16.04 - mobile device (e.g.
cluster a is accessed via nat port 6379 : internal port 6379 cluster b is accessed via nat port 6380 : internal port 6379 upon first connection to cluster b, we see the correct keys in rdm.
i trained a model with quantization aware training, froze the model and converted the file to a tflite file and then when trying to compile the file one the coral tpu website it fails to compile.
was expecting to see higher throughput with tf-trt.
my opinion is that the following code: phofcode should return the default value which is 1 and should return phofcode , but for some reason the value is phofcode and the tf.seed changes for next batches of train data, which cause instability of training procedur
unfortunately tensorflow docs do not explain well how to use tf.function and admitted operations.
when applying a keras layer 3 or more times using within the run config, training with a tensorflow estimator hits an exception when applying the layer at graph build time: file line 314, in __call__ output = super(layer, self).__call__(inputs, *args,
there is no mention of `'hidden'` in `textprop.txt`.
when there are no x11 headers available at build time, the `clipboard` feature is diabled, but the `mouse_xterm` feature is enabled.
i would expect nnapi at least not to cause performance degradation.
stride2 1 the stride sliding window with num_in_channel w tf.get_variable( 'w ', dtype=tfgraphnumbertype, shape=[filter_size2, filter_size2, num_in_channel, num_filters2], seed=rngseed)) w) b tf.get_variable( 'b ', dtype=tfgraphnumbertype, shape=[num_filters2], b) layer tf.nn.conv2d(pool1, w, strides=[1, stride2, stride2, 1], padding="same") layer += b conv2 tf.nn.relu(layer) padding2 padding(conv2, 'pad2 ') pool2 tf.nn.max_pool(padding2, ksize=[1, strides=[1, padding="same", name= 'pool2 ') with layer_shape pool2.get_shape() num_features layer_flat tf.reshape(pool2, [-1, num_features]) fully-connected layer.
i believe it 's an issue with how paths are created in tensorflow.
- os platform and distribution (e.g., linux ubuntu 16.04): colab.
it seems like a natural thing to be able to do based on the the tf.module rfc phofhyperlink
i suggest that the `.in` files should not contain any translations; they should contain only the source strings, and only the `.po` files should contain the translations (#3085).
also i was finally able to see the start of the training, it took about 40 minutes!
it should be true for both
build fails due to `xgettext`/`msgfmt` incompatibility in rhel 6.5 after patch
provide a reproducible test case that is the bare minimum necessary to generate the problem.
... steps to reproduce the behavior: 1. open public/bad.html 2. click to expand get request 3. click "try it out" 4. enter a value in the "limit" field of the form and submit 5. see that the "limit" field has been cleared
however, the parameter is passed as `none` when it should be `true`.
7. close the "authorize" dialog.
same thing for all other mappings using alt modifier: <m-t> <m-p> <m-n> <m-d> <m-u> <m-o> <m-i> <m-f> <m-b> <m-g> after commit, these mappings are printed twice; once with expected `m-` modifier, and once with these unexpected characters (` `).
import tensorflow as tf scale=1.)
a minimal reproducible example is below.
feel free to: 1. go to phofurl 2. run the query: phofcode the speed is normal.
i expect that memory should be cleaned.
i can load parameters from checkpoint phofhyperlink (download password: bpy9) and inference with the model correctly.
i train a model using slim in tensorflow models with the flowers dataset and the large nasnet.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): win10 - mobile device (e.g.
[[{{node w failed precondition: error while reading resource variable _anonymousvar4 container: localhost.
the codes are largely copied from phofurl with slight modification.
n/a - vim version phofcode - os: ubuntu 19.04 - terminal: xterm(330) add any other context about the problem here.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): '1.14.0 ') python version: 2.7.15 cuda/cudnn version: 7 gpu model and memory: gtx 1070, 8g
2. run `node_env=qa strapi start`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 and 18.04 - tensorflow installed from (source or binary): source - tensorflow version (use command below): 2.0 and 1.10 python version: 3.5.2 and 3.6.7 bazel version (if compiling from source): 19.0 16.0 gcc/compiler version (if compiling from source): 7.4 7.3 cuda/cudnn version: 10.1 9.1 gpu model memory: nvidia rtx gb) nvidia gtx 1050ti(4 gb) include any logs or source code that would be helpful to diagnose the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
it looks like around that: phofcode i got the following error: phofcode i tried to wrap `__pass_critic` in `tf.function`, but it didn't help.
-iproto -dhave_config_h -dfeat_gui_gtk -i/usr/include/gtk-3.0 -i/usr/include/pango-1.0 -i/usr/include/glib-2.0 -i/usr/include/fribidi -i/usr/include/harfbuzz -i/usr/include/freetype2 -i/usr/include/libpng16 -i/usr/include/uuid -i/usr/include/cairo -i/usr/include/pixman-1 -i/usr/include/libmount -i/usr/include/blkid -i/usr/include/libdrm -i/usr/include/atk-1.0 -i/usr/include/dbus-1.0 -pthread -d_fortify_source=2 -march=x86-64 -mtune=generic -o2 -pipe -fno-plt -u_fortify_source -d_fortify_source=1 linking: gcc -l. -fstack-protector-strong -rdynamic -wl,-export-dynamic -wl,-e -l/usr/local/lib -wl,--as-needed -o vim -lgtk-3 -lgdk-3 -lpangocairo-1.0 -lpango-1.0 -latk-1.0 -lcairo-gobject -lcairo -lgdk_pixbuf-2.0 -lgio-2.0 -lgobject-2.0 -lglib-2.0 -lsm -lice -lxt -lx11 -lxdmcp -lsm -lice -lm -ltinfo -lelf -lnsl -lacl -lattr -lgpm -ldl -wl,-e -fstack-protector-strong -l/usr/local/lib -lperl -lpthread -ldl -lm -lcrypt -lutil -lc -l/usr/lib -ltclstub8.6 -ldl -lz -lpthread -lm ``
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): nightly jan 15, 2018 (protobuf built from head jan 15) python version: n/a bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): gcc 7.3.0 cuda/cudnn version: n/a gpu model and memory: program received signal sigsegv, segmentation fault.
outside of `fit`, e.g in a custom training loop, results in an error.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):yes - os platform and distribution (e.g., linux ubuntu 16.04):windows 10 pro - mobile device (e.g.
this did not happen a few days ago in tf 2.0-preview.
e363 error occurs when opening json file containing long lines.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 pro n, 64-bit - mobile device (e.g.
the issue persists even after opening all folds with `zr`.
large logs and files should be attached.
the application should not crash when calling (almost) any function in the tensorflow api.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary):binary tensorflow version (use command below): python version: 3.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - tensorflow installed from (source or binary): pip - tensorflow version (use command below): 1.13.1 python version: python 3.6.7 cuda/cudnn version: 10.0/7.4 gpu model and memory: geforce gtx 1060, compute capability 6.1, 6gb ram
should pass and not raise exception.
there is no usage of op named _
- os & version: [e.g.
the request url should be phofcode
fyi, we cannot omit tensor(i.e, `len(inputs) = 2`) since in such case we 'll hit another assertion: phofurl
phofcode when i reach phofcode it shows unsupported feed type` but after i change the label to floats by df[ 'review '] = df[ 'review ']*1.0 phofcode the train function works again.
- os platform and distribution (e.g., linux ubuntu 16.04): windows 10 home - tensorflow installed from (source or binary): using pip3 - tensorflow version (use command below): reproduced on both 1.12.0 1.11.0 - python version: python 3.6.7 :: anaconda, inc.
the expected output of the code below should be `<tf.tensor: id=4, shape=(), dtype=string,
the last top and and bottom border chars are not shown.
tensorflow version: i think nuget automatically gives me v1.4 with tensorflowsharp v1.11?
provide a reproducible test case that is the bare minimum necessary to generate the problem.
large logs and files should be attached.
adam and adagrad, both gives different losses, when initialized once (globally) and initialized every time train function is called.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
the pointer should not be null.
instructions for updating: if using keras pass *_constraint arguments to layers.
while this issue is very visible on large graphs, i tried to compile a small example to consistently show the effect: phofcode
when `gvim` is started from a terminal, its default behavior is to fork, thereby "detaching" from the terminal (this can be overridden with `-f`).
it shouldn't print control characters
if i run list_local_devices() in order to get the number of gpus available while almost all of memory is already allocated on all the gpus, the process crashes due to there 's no problem at all if at least one gpu has substantial amount of free memory.
i am training a straightforward keras stacked lstm model, using model.compile and model.fit.
tensorflow 2 code to reproduce files =
677.2kib 624.8kib 600.0kib (65536): 4, 4.
i used tf.keras built a facenet model and try to deploy it in android tflite, the processing of the transforming(from tf.keras to tflite) was smoothed, but when i try to deploy it in android tflite, whatever i feed any image, the interpreter always send me the same result
we should be able to create a tensor with buffer
i write some preprocess code which can only be executed eagerly in map function for dataset formating, but i find that the code can 't be executed properly.
the app should run just fine with the paths specified.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
wrapper should succesfully apply to layer; previously worked in tf 1.11.0.
steps to reproduce the behavior: 1. follow the steps in the documentation for ubuntu based installation on google compute engine 2. download the cacert and user.p12 files and follow manual windows client setup 3. click connect from windows 10 (64 bit) settings-> network&internet -> vpn.
- do have to do something special to run android?
running with gpu makes eager mode significantly faster and the time difference more noticeable.
error occurs when trying to compute the jacobian in the following code
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: 2.7.15rc1 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: 10.0 gpu model and memory: geforce gtx 1080 you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
the parametermacro function is executed and set's a default value for the parameter.
large logs and files should be attached.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
/->items/e " now start vim like this: vim -nu none -s /tmp/vim.vim press `cd` to invoke `func()` which searches the next bar and echo the name of the syntax item under the cursor.
if a text file containing a bom is opened and wordcount() polled, the editor crashes, saying "vim: caught deadly signal segv "
using graph mode `tf.data` caching works using distribution strategy and doesn 't throw an error.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
large logs and files should be attached.
if including tracebacks, please include the full traceback.
i have the same results on 8.1.527. this code: string[] m_def = new string[] { "postprocessing.editor", "recorder", }; indents like this: string[] m_def = new string[] { "postprocessing.editor", // unwanted indentation "recorder", }; however, removing the `new` to make it look like a c++ array initializer, works (and it 's legal c#)!
using tf.keras in jupyter (or a python shell) with the `tensorboard` callback, some problems occur if i interrupt training.
i run this following code phofcode it print the good result in eager mode, but it return attributeerror: 'tensor ' object has no attribute '_lazy_read ' when i close the eager mode.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): colab - mobile device (e.g.
the [source code phofhyperlink shows this: phofcode note that my error is `retval 26]` (ive gotten [24], etc.
in tensorflow 's guide about feeding a model with dataset: input tf.data datasets.
flatten the unsupported ops using graph_transforms also failed.
the script is in two parts : first few pictures are taken on phone to re-train a model with overfiting (ssd_resnet_50_fpn_coco) on the server, which produces a .tflite model, then sends it back to the device.
besides, i print the prediction result array.
but if set `num_parallel_calls=4` or assertion failure is soon triggered.
i started seeing this behavior when training (`model.fit()`) but only when providing validation data.
1. create new node project: `npm init -y` at empty folder 2. install svelte and tyepscript: `npm i -d svelte typescript` 3. configure typescript: phofcode 4. create `test.ts` with the following import: `import * as svelte from 'svelte/compiler '` 5. run `npx tsc`
trying to apply a mask when calling a batchnormalization layer fails with: *typeerror: call() got an unexpected keyword argument 'mask'*
after multiplying the input with my first layer i get as an object shape=(1, 16), dtype=float32).
ive been trying to convert this trained model to `coreml` via `tf-coreml` library with no success, with below error: retval 26] does not have value` i 've also encountered: `notimplementederror: unsupported ops of type: switch,merg` i understand that this error states that there is a certain node thats missing a value so the converter can execute the model.
run it a few times and watch the validation accuracy/loss.
the client connects to the server and the vpn connection is established
1. run `vim --clean <file>` 2. type ':set number ' 3. type ':set relativenumber ' screen flashes 4. type ':set norelativenumber ' screen flashes
- try to train a model based on get an - error attributeerror: module 'tensorflow ' has no attribute 'init_scope '.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:none - tensorflow installed from (source or binary): source and binary tensorflow version (use command below): python version: 3.6.6 bazel version (if compiling from source): 0.15.2 gcc/compiler version (if compiling source): 6.4.0 cuda/cudnn version: 9.0 / 7.3 gpu model and memory: 2 x 1080-ti
would get some warnings like below.
running macos 10.14, fails to ask for api token and location
# can you tell me about your tflite_convert command?
large logs and files should be attached.
calling load and freeze in different python functions should work unless there is some hidden assumption in tf 2.0. my guess is that there is a leak or some dependency between the two apis.
any way, here what 's particularly wrong about it.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): osx 10 - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: oneplus5, pixel3, galaxy s10 - tensorflow installed from (source or binary): various tensorflow version (use command below): 1.14.0, r1.14 source build python version: 3.x bazel version (if compiling from source): 0.25.2 gcc/compiler version (if compiling from source): 7.4.0 cuda/cudnn version: gpu model and memory: i tested tflite both from nightly build jcenter and local builds r1.14 branch.
can train large model using without memory issue, like
i was expecting the custom training loop can be run using different optimizers.
python import tensorflow as tf inputs = tf.keras.input((10, 10)) assert inputs.shape.as_list() == [none, 10, 10] # this works assert == [none, 10, 5] # raises valueerror ``
the bug: `autocmd bufread * silent!
you may want to write a small program to make predictions over a subset of data to sanity-check model training and gain more insight into how your estimator makes predictions, without having go through the trouble of deploying your model.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
` import tensorflow as tf from tensorflow import keras fashion_mnist = (train_images, test_labels) = model = keras.layers.conv2d(64, (3,3), activation=tf.nn.relu, input_shape=(28,28,1)), keras.layers.flatten(), keras.layers.dense(128, activation=tf.nn.relu), keras.layers.dense(10, ]) model.fit(train_images, train_labels, epochs=5) '
it looks worse longer operation summary is.
i have spent some effort on debugging, validating the behavior of api.
fenced code blocks should not require additional indentation
profiling is not working when two machines(workers) run with full_trace in a distributed mode(ps).
1. create a new content-type 2. create some records 3. delete content-type 4. check the database and see that the created records are remaining
i looked into it and found that phofhyperlink generates unexpected results with the calculation `multiplier = input_scale * weight_scale / output_scale`.
you can alternate back and forth removing one or other and see that settings are working independently, just not together.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
1. run `vim --clean popup.vim -c "source popup.vim"` phofcode 2. close the popup window with the mouse by clicking on `x`.
with eager enabled: phofcode results in: phofcode without eager: phofcode which results in a model with no output when saved to pb.
- i thought this issue #20843 is related to the problem but i have done a git bisect between the `v1.8.0` and the `v1.9.0` tag and running the script above for every commit and found one that introduce largest drop in performance: d4976f7.
63.2kib 63.2kib 60.0kib (8192): 2, 2.
large logs and files should be attached.
however, autotune seems "forget" it 's session values if i don 't keep running and fed with new data constantly.
phofcode where phofcode the above case works fine with the output: phofcode however, adding just one `embedding` layer so that now is: phofcode leads to the error: i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 fma i cpu frequency: hz i xla service executing computations on platform host.
steps to reproduce the behavior: 1. prepare the virtual environment 2. prepare the gce iam project 3. start the algo installation
"+yy one line 3. ctrl+z (or quit vim) 4. try to paste it in any application (gedit, terminal, ...) 5. oberve that it does not paste anything if you skip step 3, you will see the pasted text in step 5 as expected.
making a prediction from that model gives a shape error.
attributeerror traceback (most recent call last) in <module> 8 epochs=100, 9 ---> 10 11 ) in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch) 1424 1425 shuffle=shuffle, -> 1426 1427 1428 def evaluate_generator(self, in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size,
i am getting an out of memory error during the training of a encoder-decoder model using the subclassing api of tensorflow 2.
it feels like unnecessary complexity
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution: linux ubuntu 16.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12.2 python version: 3.5 cuda/cudnn version: v8.0.61 gpu model and memory: tesla p100-pcie, 12193mib
- tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.10.0 python version: 3.6.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: on cpu gpu model and memory: on cpu include any logs or source code that would be helpful to diagnose the problem.
datagenflow = train_labels, batch_size=batch_size) epochs=epoch_num, callbacks=[change_lr], test_labels))
here is the code below which should work (it works with keras-team/keras).
# no error when creating dir with + "/test_file.txt", "w") as f: # .write throws exception since the <test_path> directory does not exist f.write("hello") ``
i've created a github gist file that can be run on randomly generated data and prints reports about the ops: phofurl usage: python train_tensorflow.py --random_data=true alternatively the preprocessed data can be downloaded from: phofurl you can also easily use it in combination with tensorboard, just ask the command argument parser for some --help.
responsive browsing with little to no lag at all, expected all webpages to load and have no issues with timeouts.
case, should define them a .py file.
so, is there anyone can help me to figure out principle behind this phenomenon.
so things work reliably for first image, but not for following images.
it is correct on cpu, but wrong on gpu.
here is full stacktrace for first example code: pycon runtimeerror traceback (most recent call last) in <module> 9 10 b = b() ---> 11 b.bar(5) __call__(self, *args,
you can try any model under deeplab model zoo for example phofurl phofhyperlink
return a ragged tensor regardless of the use of `@tf.function`
and problematic happening once per tower under a phofcode however, if this error were tied to my mishandling of those control statements many layers of model that get added before would have caused an error.
i successfully opened dynamic library cupti64_100.dll - eta: 12:48 - loss: 2.2374 - accuracy: i collecting 81 kernel records, 14 memcpy rec ords.
`tf.while_loop` only loops more than once
at step 11., something has clearly gone wrong - the contents of buffer are missing.
convert `.pb` to `.tflite` using `tflite_convert`.
return the value like eager mode.
i run in eager execution, but code inside `.map()` is not executed eagerly.
1. run this shell command: `$ printf "foobar foo" >/tmp/html.html && vim -nu none --cmd 'filetype indent on ' + 'set showcmd showmatch matchtime=100 ' /tmp/html.html` 2. enter insert mode at the end of the second line, after `foo`, by pressing `ja` 3. press `{` and `}` to insert `{}` 4. press `x` to insert `x` the `x` character is not inserted in the buffer.
use specified steps_per_epoch instead of len(data) if specified.
i do not know what can be done about it _per se_, but if such an overhead is to be expected, i think it would be nice to be able to disable eager in 2.0. and i mean _properly_ disabling it, not using `tf.compat` instructions that are bound to wiped out at some point.
a sample class like here phofhyperlink , which was instantiated globally.
when logits are computed in for the evaluation of `tf.nn.nce_loss`, the output related to the true labels from `out_labels` might be wrong.
2. when saving the model as shown in code example, i get `valueerror: exporting an object with no tf.saved_model.save(..., signatures=...) argument specified, and with more than one @tf.function-decorated method attached to it: 'serve '].
phofcode macos i am also finding that jumping to quick fix entries isn 't working, so something really bad seems to have happened to quick fix.
can no longer use _bash_ as my shell on windows, as _gvim_ builds the wrong command-line.
the tensors are of shape (4, 192, 192, 2).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): win 10 / colab ( linux ) - tensorflow installed from (source or binary): binary - tensorflow version (use command below): include any logs or source code that would be helpful to diagnose the problem.
`func()` has been wrongly removed.
start from yolo-v3( phofurl meet correctness issue, the tf output did not consistent with the previous version
i can resolve issue with running this sample by setting but this should not be necessary because i already configured this path in and ran `$ ldconfig`.
- current app: phofurl - demo version r1.13: phofurl - object detection app: phofurl
i have no idea what 's the problem.
1. create a simple tags file by running `ctags foo.vim` with the following content: phofcode 2. edit the file `foo.vim` in a separate terminal window to make sure a swap file exists.
as a temporary workaround, we disabled some of these function calls in local tensorflow fork, and the performance regression went away as expected.
"red" : "blue"}"> text </span>
1) #from conda cli base create new environment conda create --name tensorflow2_test 2) #activate environment conda activate tensorflow2_test 3) #install pip conda install pip 4) #install environment tensorflow 2.0 pip install tensorflow 5) #verify install: python -c "import tensorflow as tf"
under certain circumstances, vim will segfault upon closing.
if including tracebacks, please include the full traceback.
5. there is a workaround, but it requires using `win_execute()` and then setting `firstline`: phofcode but for some reasons `win_execute(a:winid, "normal!
2, 3, 4 threads (`-t 2, -t 3, -t 4`) are expected to return the same results as single-thread case.
going below 0.015 make it crash (as it obviously don't have enough vram to run).
you may need to reduce font size to be able to have a terminal with enough columns.
the bug therefore seems to be connected to size of input.
strategy info:tensorflow:saving checkpoints into i successfully opened cuda library libcublas.so.10.0 locally i filling up shuffle buffer (this may take a while): 387 of 512 i shuffle buffer filled.
python import tensorflow as tf import numpy as np def get_dense(input, widths, activations): assert len(widths) == len(activations) output = input for i, (w, a) in enumerate(zip(widths, activations)): output = tf.layers.dense(output, units=w, activation=a) return output nn = lambda input, depth, width: get_dense( input, [width for _ in range(depth - 1)] + [3 ], [tf.nn.relu for _ in range(depth - 1)] + [none] ) tf.reset_default_graph() # works fine when n = 8192, # but breaks with n 8193: n 8193 n 100 x scale=1., size=(n, 1)).astype(np.float32) nx x.shape[1] y x
so the best i can do is to suggest running a minimal version of phofurl * install phofurl * run vim/gvim with `-u` which sources phofurl
see this colab notebook phofhyperlink
if including tracebacks, please include the full traceback.
- have i written custom code: yes - os platform and distribution: ubuntu 18.04 - mobile device: no - tensorflow installed from (source or binary): pypi tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6.4 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: na gpu model and memory: na this seems to stem somehow from the inheritance of abstract methods.
i tried nasty fix which consisted in adding `and all(isinstance(tensor, ops.eagertensor) for tensor in loop_vars)` `executing_eagerly = line at beginning `while_loop` 's body in source code, but this results raising `attributeerror: tensor.name meaningless when eager execution enabled.` within loop constructor (only with symbolic tensors - using ones, everything goes fine).
instructions for updating: use standard file apis to check for files with this prefix.
during handling of above exception, another exception occurred: traceback (most recent call last): file "error_showcase_tb.py", line 50, <module> steps_per_epoch=50 file line 669, fit file line 669, fit 444, model_iteration epoch_logs) 296, on_epoch_end logs) 1612, on_epoch_end self._log_weights(epoch) 1691, _log_weights weight = k.get_value(weight) 3016, get_value return x.numpy() 580, numpy return 633, read_value value = self._read_variable_op() 611, _read_variable_op self._dtype) 575, read_variable_op resource, dtype=dtype, name=name, ctx=_ctx) 613, attrs=_attrs, ctx=_ctx, name=name) 71, quick_execute raise e 61, quick_execute num_outputs) typeerror: an op outside of function building code being passed "graph" tensor.
partially copied from phofurl phofcode output: warning:tensorflow:from check_dropout.py:5: the name is deprecated.
i expected that should be able to load a model created witih keras.
within `govim`, a format-on-save is achieved by adding a `bufwritepre` autocmd that makes a call to `gopls` for any changes that need to be applied pre-save.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): `yes` - os platform and distribution (e.g., linux ubuntu 16.04): `ubuntu 14.04` - mobile device (e.g.
will try to get one soon.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): docker image, details below; phofcode - tensorflow installed from (source or binary): binary - tensorflow version (use command below): git version: version: 1.13.1 python version: 3.5.2 cuda/cudnn version: phofcode gpu model and memory: nvidia gtx 1050 ti to give you some context, i have several sensors generating signals at uneven sample rates.
hi all, i know this problem has arisen on here but i have yet to find a discussion around the problem with regards to r; all i see are python issues and solutions.
the result is that cannot download images in gcs.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
should work out of the box as before
if including tracebacks, please include full traceback.
[save.py:136] skipping full serialization of object <__main__.mymodel object at because an error occurred while tracing layer functions.
what i have found is: the second code snippet if 1.5 times faster than the first code snippet with the same training setting.i want to know why this happens and i want know which code snippet is the tensorflow recommended code snippet.
large logs and files should be attached.
le: by running with debugging symbols, it seems that eigenfortflite is the culprit; is this really a wanted behavior
recently i switched from tf1 wrapped with native keras to tf2 using the built-in `tf.keras` implementation.
this is best demonstrated by a video in which i 've slightly changed code to _always_ call `setqflist(entries, 'r ')`, regardless of whether diagnostics have changed.
upon further investigation, this error was found here: phofurl
when i call a method decorated by `@tf.function`, i get an error if it uses `super()`: `runtimeerror: super(): __class__ cell not found`.
when using c=1 cpu host (tf tflite) android on cpu are kind of the same (less than 10% difference), but android gpudelegate produces drastically different output.
it also works correctly on the gpu in tf 1.12.0, cuda/cudnn 9.0/7.3.1 and python 3.6.8.
when attempting to deploy algo to a local ubuntu 18.04 server using the ./algo script, the script crashes when attempting to restart the dnscrypt-proxy service, and the script states that it is unable to start service dnscrypt-proxy because control process exited with an error code.
as described in the third example in the documentation for keras models phofhyperlink , a boolean `training` parameter can be used in the `call` method of subclassed models.
when running it seems the graph or session is somehow not properly destroyed when `max_steps` is `none` or less than the number of steps in the `input_fn`.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
should execute in a comparable time to a dense tensor.
not found: resource does not exist.
phofcode may be same other asser functions: raise exception with assert_greater phofcode
valueerror: input 0 of node was passed float from prefix/lstm/kernel:0 incompatible with expected resource.
in my output below i have added yet another user config.txt, and error appears twice.
we know that you guys are working on v2 of the while loops, and we're really excited for it.
here is the saved_model: ver1.zip phofhyperlink
program crashes due to insufficient memory issue.
train for 281 steps, validate for 31 steps epoch 1/5 w cannot find shardable dataset, adding a shard node at end of dataset instead.
if i understood correctly, tf keras is supposed to be interoperable with feature column.
result should not depend on eagerly mode on or off.
when i created tensorflow lite model i was forced to provide input shape parameters (eg., per documentation should be able to change the input size using method.
tflite_convert (i can upload the model somewhere, if needed!)
i created a custom keras model (cnn network with resnet layers).
to be precise, this model has the same weights as when the model was first defined.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): '1.12.0') python version: 2.7.12 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: gpu model and memory: v100-pcie-32gb, nvidia driver version with `intel_iommu=on` phofcode phofcode kernel reports ` dma write] pte write access is not set` for one of the gpu.
i would expect that tf.eval() gives me a numpy float/array and therefore the ouput after multiplying with my first layer should result in something like tensor("add_12:0", shape=(1, 16), dtype=float32).
python import tensorflow as tf class def __init__(self): super(debugmodel, self).__init__() self.dense = activation= 'relu ', input_shape=(5,)) def __call__(self, input, *args,
- have i written custom code (as opposed to using a stock example script provided in tensorflow): phofhyperlink - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04-hwe - mobile device (e.g.
`sudo docker run --cap-drop all -it -v where `/home/ubuntu/algovpn` is a folder that includes the `config.cfg` file.
- have i written custom code: yes - os platform and distribution: os x 10.13.6 - mobile device if the issue happens on mobile device: x - tensorflow installed from: binary tensorflow version: 1.12.0 python version: 3.6.7 bazel version:x gcc/compiler version: x cuda/cudnn version: x gpu model and memory: x none.
while running attached python script, it reports following warnings: phofcode and during one my `gdb` sessions, i check i/o min/max values of a `max_pool_2d` operator highlighted by one of warnings above.
- vim version: - os: macos 10.14 - terminal: apple termina
i generated them with fan_in = 100, fan_out = 100 and ploted their histogram.
lcd %:p:h` 4. execute `:vim /whatever/
the first line of the buffer should be immediately visible.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution: linux ubuntu 16.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12.0 python version: 2.7 cuda/cudnn version: 9.0 / 7 gpu model and memory: t4, 16 gb tpu type: v2-32 tpu tensorflow version: 1.12.0 what causes the issues?
i downloaded quantized and float point models from phofurl then run benchmark_model to measure inference time.
if including tracebacks, please include the full traceback.
but it's 2019, who still uses python 2 style?
expected 96, got 88 return f(*args,
this error causes the failure of the sparse_matmul_op_test (found at
however, when i try to do the same on ai platform, it fails: `not json serializable`.
check results are the same - psi_np.ravel())) should print 0.0 note that the speed depends on shape!
2. put a `parameters` object directly under that path 3. place the path parameter in the parameters object.
when its run as a non-admin user with admin creds provided for elevation the certs are installed to an incorrect but hard coded location and misleading error is presented.
i would expect the epochs to to through smoothly, i actually suspect the validation part, because it is failing exactly before it.
[default is to use phofurl build fails with "/bin/bash: cuda_toolkit_path: unbound variable" this has been broken since this commit: phofurl running in docker container: (for ease of setup) export tf_need_cuda=1 "" | ./configure bazel build --config=opt --config=cuda failed with: error: executing genrule failed (exit 1) /bin/bash: cuda_toolkit_path: unbound variable target failed to build use --verbose_failures see command lines of failed steps.
in keras, steps_per_epoch is kept.
bin (256): ttotal chunks: 162, chunks in use: 161.
the reason is save_weights and load_weights handles nested model differently save_weights -> call layer.weights for each layer load_weights -> recursively call model.weights if layer is a nested model
why is this happening and/or how can it be fixed?
error in below is outcome of before and after the optimization.
please find gist here phofhyperlink
the op run normally without tf.function.
the order of operations are as follows: tf.keras fit(): 1.
<img width="734" src=" phofurl - vim version gvim for windows - os: windows 10 1903 - terminal: gui (use gui if you use the gui.)
- have i written custom code (as opposed to using a stock example script provided in tensorflow): none - os platform and distribution (e.g., linux ubuntu 16.04): windows - mobile device (e.g.
the 2nd word of line 1 should have remained blue, and had italics applied on top.
suggests instead, but module attribute 'cudnnlstm' doesn't exist.
as far as i can tell, filtering for sharding now happening twice, once in googletest.py and once in absltest.
note: seems to be inconsistent, indicating a race.
correct progress bar information progress bar reasonably short
2. urxvt appears not support t_crc phofhyperlink ?
sample rate output of decode_wav can be used as input to
provide a reproducible test case that is the bare minimum necessary to generate the problem.
unfortunately, i can 't share the data i 'm using.
first 43 operations will run on gpu, and remaining 7 on cpu.tflitegpudelegate prepare: tensor ref has unsupported number of dimensions: 5node number 50 (tflitegpudelegate) failed prepare.
it gives an error when i am calling the unknown shape tensor into the dynamic_rnn function
steps to reproduce the behavior: 1. install a ct with ubuntu 18.04 on proxmox 2. add proxmox ve test repository 3. follow the steps to install algo vpn
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
<tf.tensor: id=xxx, shape=(2, 1), dtype=float32, numpy=array([[1.
segmentation fault in saving an initializable dataset iterator when entering the context manager.
stack trace: > traceback (most recent call last): > file "xxx.py", line 255, in <module> > result = model.predict(data, batch_size=32) > file line 1864, in predict x, check_steps=true, steps_name= 'steps ', steps=steps) file line 992, in _standardize_user_data class_weight, batch_size) file line 1117, in _standardize_weights 257, standardize_input_data 'expected no data, but got: ', data) valueerror: ( 'error when checking model input: expected no data, but got: ', ..., ..., ..., ..., dtype=float32))
the solution would be: ` out_labels = 1) `
# create a n-dimensional ndarray psi = np.random.randn(*[d for n in psi_np = psi.copy() for _ in range(5): # numpy's transpose is a metadata operation.
note that when only using a cpu, it gives the correct result.
when creating a custom model, simply creating a `mean` metric in the constructor and setting it as one of the attributes leads to an exception when fitting the model.
* i finally figured out why this was happening from an issue on the numbers plugin phofhyperlink which i'm not using.
the code also works fine on mac.
this doesn 't happen in vim, only in macvim: [b@localhost:~]$ --version vim - vi improved 8.1 (2018 may 18, compiled jul 28 2019 macos version included patches: 1-1722 compiled by travis@traviss-mac.local huge version with macvim gui.
... unfortunately i 'm unable to provide a quick test for this as i 'm only involved in the ui portion.
the two exact scripts that i use are do.py phofhyperlink and utilities.py phofhyperlink .
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04):linux ubuntu 18.04 - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - tensorflow installed from (source or binary): pip - tensorflow version (use command below): 2.0.0-beta1 - python version: 3.7.3 traceback (most recent call last): file line 1194, in visit super(astannotator, self).visit(node) file line 132, in visit super(basevisitor, self).visit(node) file "c: program line 262, in visit return visitor(node) file line 47, in wrapped f(self, node, *args,
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): 18.04 - mobile device (e.g.
the tensorflow module loads in python3.
i 'm trying to optimize tf model ' phofhyperlink ' with graph_transforms tools.
instructions for updating: use tf.where in 2.0, which has the same broadcast rule as np.where ``
the model can be created, compiled, and perform the training (by model.fit method) without any warning or error.
steps to reproduce the behavior: 1. setup an aws iam using with mfa activated as instructed 2. run algo and choose
details are provided here phofurl after asking the question i additionaly tried different versions of tf/cuda, no changes.
i expect the behavior from the python file to match the cli behavior.
large logs and files should be attached.
large logs and files should be attached.
the tf.random_normal() of py2 and py3 with same seed should get the same result.
i have already disabled fragmentation on the vpn server as someone else had noted in an issue.
expect to see a normal run since the variables are initialized inside the conv2d function
i have written a face classification code which uses mobilenet (depth 80) model.
the expected print should be 0-9 for both `x` and `seed`.
tflite phofcode label_image needs input image and labels file.
phofcode this takes about 6 seconds in my environment.
phofurl this is the speed with tf 1.13. info:tensorflow:loss = step = 0 info:tensorflow:loss = step = 100 (0.341 sec) info:tensorflow:loss step 200 (0.296 sec) info:tensorflow:loss step 300 (0.292 sec) 400 (0.292 sec) 500 (0.288 600 (0.287 700 (0.304 800 (0.338 900 (0.338 this is speed with tf 1.14. i0722 loss 0 i0722 global_step/sec: i0722 loss 100 (0.354 i0722 global_step/sec: loss 200 (0.306 global_step/sec: 299.99 loss 300 (0.333 global_step/sec: 400 (0.332 500 (0.337 600 (0.334 700 (0.341 800 (0.338 900 (0.337
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): rhel 7.6 - mobile device (e.g.
the `]m` and `[m` motions in java: phofcode this also affects related motions like `]m` and `[m`.
the test_negative test in fails, as the bincount call with negative values does not throw an invalidargumenterror.
should login and execute the app
gpu model and memory: not relevant.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): source tensorflow version (use command below): 0.12.0 python version: 2.7.15rc1 bazel version (if compiling from source): 0.15.0 gcc/compiler version (if compiling from source): 7.3.0 cuda/cudnn version: na gpu model and memory: na from above logs we can see that when we pass a constant (axis) of int64 type to `concatv2`, we get different results on s390x which causes assertion failure.
the problem does not exist when using `fit`
cache won 't run out of memory
download and install cuda %s from " "this url: phofurl % the url directs you to version 9 but the current dll checks for version 10.
i expect this behavior: tf.tensor([ 0 11 0 10 9 0 0 12], shape=(8,), dtype=int32) tf.tensor([ 11 10 9 12], shape=(8,), dtype=int32) tf.tensor([ 11 10 9 12], shape=(8,), dtype=int32) otherwise i need to find unique indices and delete multiples.
dnscrypt-proxy restarts after a reboot.
in alacritty phofhyperlink (under both x and wayland) and wterm phofhyperlink at least, it won 't start.
large logs and files should be attached.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: not applicable - tensorflow installed from (source or binary): tensorflow binary installed from pypi via `pip`.
`(env) ./algo play [ask user for the input]
using tf.keras.layers.rnn does not work in place of tf.nn.dynamic_rnn, or at least not in a way that i can find.
the expected behavior should be the same as simplernn (re-sample dropout masks on each call).
found out that memory leak only occurs if uncomment tf_sessionrun.
in that version, both scenarios works good with dict type inputs.
since traffic can not be hidden back-propagation progress, requirement of bandwidth for hardware, like nv-link server or infiniband network between servers goes up a lot.
bazel run -c opt -- --mean_values=128 --std_values=128 --default_ranges_min=0 --default_ranges_max=6 --allow_custom_ops
when i try to save a trained model (keras) locally with params[ 'output_dir '] + '/savedmodel ', save_format= 'tf ')` it runs like expected.
however this error gets handled, and tpu session remains active even when there 's no more code to execute.
build looks like this: phofcode looks like: phofcode then in our binary cpp the code is called like this (simplified, parts removed for clarity): phofcode the object works if compiled like: `bazel build -c opt --compilation_mode=dbg` but fails if compiled like: `bazel build -c opt
here are the tiny model and the big model .
notimplementederror: layers with arguments in `__init__` must override `get_config`.
if including tracebacks, please include the full traceback.
the example should run as is.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):yes - os platform and distribution (e.g., linux ubuntu 16.04): google colab - mobile device (e.g.
in fact, this behavior is meet when the model is saved with keras format ('model.h5') and loaded.
the mae and the mse should be correlated but the mse value used in the metric is not correlated see this minimal log for instance phofhyperlink
phofcode similar code in the official examples of google colab also not working phofurl
if including tracebacks, please include the full traceback.
laptop cpu only, no gpu used.
the expected behavior is run the code and get the result of the estimation through of the `output `tensor.
class simple(layer): def __init__(self, output_dim,
for example, this is the root cause of #25754. this might be considered a feature request, rather than an issue: i grant you that it's debatable!
here 's a gif reproducing the issue in an interactive usage: gif phofimage
code should work with eager execution mode.
phofcode phofcode - os: macos 10.14.5 - terminal: macos terminal
when i used tf.keras.activations operators in my keras model, serialization of model was failed due to not json serializable error.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - tensorflow installed from (source or binary): source - tensorflow version (use command below): 1.12.0 (tensorflow-gpu, tensorflow-base) python version: 3.6.8 bazel version (if compiling from source): 0.21.0 gcc/compiler version (if compiling from source): 5.4.0 cuda/cudnn version: 7.1.2 gpu model and memory: geforce gtx 1080, 8gb
creating a variable should be completed within acceptable time.
following are two simplified versions of the problem, the first one comes from phofurl phofhyperlink and the second one is my problem.
note that quotes in common lisp files do not cause the same error.
my hypothesis is that there is a race condition between when vim checks to see if the terminal process has died and when the process actually does die.
i presume issue is with c api because training in c++ works on gpu.
cursorline should not be displayed other than the popup window
performance should improve, or at least remain the same, on p3.16 as compared to p3.8, as there are more cpus available.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): pip tensorflow version (use command below): attributeerror traceback (most recent call last) in <module>() 15 for bf, p in zip(batch_features, path): 16 path_of_feature = ---> 17 np.save(path_of_feature, bf.numpy()) attributeerror: 'tensor ' object has no attribute 'numpy
this does not make sense me, especially since when trying apply `y_true = [-1])`, message properly aknowledge that last dimension dimension `1` (recall `can not squeeze dim[1]` above).
i have tried to optimize my custom frozen model to run on tensorrt using however, the output was larger than the original model (my model is around 200mb, but after converting it's more than 2gb).
but if i enabled the batch norm layer, it said that "tf.gradients is not supported when eager execution is enabled.
expected behavior is no segfault.
visit this repl, wait three seconds and click the number: phofurl
whereas for 1.12.0, `ndim_y_true` yields `2` as expected `[64,4]` tensor, 1.13.1 2.0.0 yields `3`.
even if i could i think then i'd need a license.
return tf.sparse.reduce_sum(sp, axis=axis) def sparse_sum2(sp, axis=1): -> to_dense."""
* get a clean installation of ubuntu 18.04 with all updates * follow the instructions at phofurl to install algo and its dependencies * at step 4. install algo's remaining dependencies, you'll get deprecation: python 2.7 will reach the end of its life on january 1st, 2020. please upgrade your python as python 2.7 won't be maintained after that date.
and i found that if i uncomment mirror strategy part, code is working good phofcode and another found is that myself only have 2 gpus however, when tensorflow init, it prints "adding devices" 3 times which i thought should be 2: phofcode
given that it is possible to convert tensorflow models such as inception to tensorrt as evidenced by phofurl the conversion should similarly work for models in
this is the example given in the docs with error message returned when ran: phofcode python attributeerror traceback (most recent call last) in <module> 1 m = ----> 2 m.update_state([0, 0, 1, 1], [0, 1, 0, 1]) 3 # cm = [[1, 1], 4 # [1, 1]] 5 # sum_row = [2, 2], sum_col = [2, 2], true_positives [1, 1] in decorated(metric_obj, *args,
there is no issue if (help) window has more than 10 lines.
we can conclude from the python api that 'cursorline ' is local to window: phofcode as per the doc, phofurl 'cursorline ' is local to window.
the memory allocated by an invocation of a`call` method using `tf.linalg.expm` should be freed.
any code that passes feeds totaling > 2gib to tf_sessionrun connected over a grpc connection.
i'm trying to feed a cloud tpu (v3) some data.
2. execute for example `:!ls` 3. receive the error message as shown in the screenshot
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no - os platform and distribution (e.g., linux ubuntu - mobile device (e.g.
the aws s3 implementation should be able to handle weird path
unless i 'm missing something
- have i written custom code: no - os platform and distribution: ubuntu 16.04 - tensorflow installed from (source or binary): binary pip3 - tensorflow version (use command below): python version: 3.5.2 cuda/cudnn version: 10.0/7 gpu model and memory: titan v 12gb
so far i could not come up with a small testcase to reproduce the issue.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.14 mojave - tensorflow installed from (source or binary): binary (pip) - tensorflow version (use command below): 1.11.0 python version: 3.6.4
ideally, a signature inference centric api would be a good way to solve this issue and would be good addition the existing java api.
whether i run a sample x as a batch size of 1 or with other samples, its output should be exactly same.
two different runs should always have the same output.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
so error highlight must be painted from "f".
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): conda installation tensorflow version (use command below): 1.9.0 python version: 3.6.5 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: cuda version 9.0.176 gpu model and memory: v100, 16g n/a
1. upload many files to server for an entity 2. refresh and check all files are ok 3. upload new files to server for the same entity 4. refresh and check all older files and new are ok 4. see than all previous are unlink from _upload_file_morph_ relations table
this way, the sigmoid which applies independently to each label will try to produce a `y_hat` for each (positive) label close to 1 instead of `1/num_true`.
unfortunately, after ~900 steps, the training fails with `runtimeerror: variable creator scope nesting error: move call to out of `with` scope.`
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
i write a dynamic library with the tensorflow c sdk which can be called normally by a c++ program but when i wrap it as a jni and call it from a java program, it will crash: `a fatal error has been detected by the java runtime environment: sigsegv (0xb) at pid=26065, jre version: openjdk runtime environment (build java vm: openjdk 64-bit server vm mixed mode linux-amd64 compressed oops) problematic frame: c const*, ` what 's the reason for that?
i expect to get the same variable names in a layer no matter how layer is used.
last commit: remove pycrypto from the gce role (#1489) python 2.7.15+ runtime variables: algo_provider "local" algo_ondemand_cellular "true" algo_ondemand_wifi "true" "x251bgw=" algo_windows "true" algo_dns_adblocking "false" algo_ssh_tunneling "false" wireguard_enabled "true" dns_encryption
from import resnet50 #from keras.applications import resnet50 image_size = 224 model = image_size, 3), include_top=true, weights='imagenet') weights = print(weights) weights = print(weights)
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes, my own example code.
`import tensorflow as tf ` 'label.pbtxt ', overwrite=true)`
phofcode output: > 0.0 > 1e-08 expected output: > 1e-08 > 1e-08
-iproto -dhave_config_h -dmacos_x -dmacos_x_darwin -g -o2 -u_fortify_source -d_fortify_source=1 dition de liens clang -l. -fstack-protector-strong -l/usr/local/lib -l/usr/local/lib -o vim -lncurses -liconv -lintl -framework appkit -l/usr/local/opt/lua/lib -llua5.3 -fstack-protector-strong -l/usr/local/lib -lperl -lm -lutil -lc -lpython3.7m -framework corefoundation -lruby.2.6 ``
the efficiency of the code should be same.
i believe it's related to phofurl but that one was closed and the problem is still occuring for me.
run this in google colab : phofurl for the code to execute without error (until the save), you 'll need to replace phofcode with phofcode
same output as running on colab: image phofimage
- have i written custom code (as opposed to using a stock example script provided in tensorflow): simple keras model combined from examples - os platform and distribution (e.g., linux ubuntu 16.04): macos x 10.14 - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code: only for testing - os platform and distribution: linux ubuntu 16.04 - tensorflow installed from (source or binary): from pypi via pip3 install --upgrade tensorflow==1.11.0 - tensorflow version: 1.11.0 python version: 3.5.2 cuda/cudnn version: cuda 9.0, cudnn 7.0 gpu model and memory: nvidia geforce gtx 1080, 8gb
when training `tf.keras.fit(dataset)` with eager execution, where dataset is a `tf.data.dataset`, i am finding a ~10x performance loss as compared to turning off eager execution.
insert anything from any model.
please include the following block of text when reporting issues: algo running on: alpine linux v3.9 (virtualized: docker) zip file created: python 2.7.16 runtime variables: algo_provider "gce" algo_ondemand_cellular "true" algo_ondemand_wifi "true" "x251bgw=" algo_windows "false" algo_local_dns "false" algo_ssh_tunneling "false" wireguard_enabled "true" dns_encryption "true" task [display the invocation environment]
quitting a tab changes the size of a maximized window in another tab.
indicate the source of the problem/possible resolution.
phofcode note that this code is inspired by phofurl p.s.
the model should work without mandating `@tf.function` decorator.
running the following code for the first time show no warning.
i'm running tensorflow.keras through a docker container and displaying the output in a jupyter notebook that is outside of the container (using amazon sagemaker notebooks).
how could this be solved?
although we clearly shouldn't be passing an empty tensor to this method, i would expect an exception to occur (in python) explaining that the input is invalid, rather than a segfault.
the text selection should continue to the end of the line.
(for this and the previous step, i use `:set autochdir` and edit file on the linux side.)
when doing transfer learning using pre-trained keras model (xception per say) with the imagenet weights and adding a classification layer there is an error when fitting.
`/expr-!=#` + `0n`) leaves cursor in correct position.
phofcode quickly opens with syntax as expected.
adding visible gpu devices: 0 device interconnect streamexecutor strength 1 edge matrix: 0: n created tensorflow device (/device:gpu:0 5139 mb memory) -> physical gpu (device: name: geforce gtx 1060 6gb, pci bus id: compute capability: 6.1) info:tensorflow:device is available but not used by distribute strategy: /device:cpu:0 info:tensorflow:device is available but not used by distribute strategy: /device:xla_cpu:0 info:tensorflow:device is available but not used by distribute strategy: /device:xla_gpu:0 nccl all-reduce.
i 'm wondering if the warnings could have to do with it?
interestingly, if number of elements in tensor is less than four, gradient is returned correctly (inf instead of nan) here is output failed test: > fail: (__main__.betainctest) > traceback (most recent call last): file line 160, in np.isnan(grads_x)) file line 1556, in assertallequal b, err_msg=msg) file line 855, assert_array_equal verbose=verbose, header= 'arrays are not equal ') file line 779, assert_array_compare raise assertionerror(msg) assertionerror: arrays are not equal (mismatch 2.0%) x: array([false, false, false, ..., false, false, false]) y: array([false, ..., true])
will fail with tensorflow 2.0.0-beta1
this check fails when using the cuda malloc allocator because virtual allocator method getstats was never overridden.
it was observed that modifying momentum appears to have no effect.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
this is about searching for a string in a window that has less then 11 lines and the line of the match contains characters that are concealed via syntax or via matchadd().
i don't know whether the change is intended, but i bisected it to
the same rotation, zooming, etc.
at this point i can 't build new or train any existing ones.
place the cursor at the asterisk in the following code sample: namespace foo { class bar { void qux() { // press [m here -> * } } }
1 is detailed in that issue 2 is with autoindent or from line start or select something(w/o mouse) then backspace then c-s 3 always there
when i attempt to load my .tflite model on the android (after applying the gpudelegate as described here phofhyperlink ), it fails saying that 'mean' is an unsupported operation (see traceback below)
- have i written custom code (as opposed to using a stock example script provided in tensorflow): include any logs or source code that would be helpful to diagnose the problem.
the expected value of a gamma-distributed random variable with shape parameter a and inverse scale parameter b is a/b, so the empirical mean should be close to a/b.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macosx 10.13.6 - mobile device (e.g.
should be able to continue using existing wireguard clients, without having to download new config files (unless of course the associated user was removed)
usage for batch sizes 128 through 227 is as above, while for batch size 228, it suddenly 4495mb.
a stack of dense layers, the parallelism is visibly improved.
->navigate parameters table use caps+arrow keys read table values 9. expand and collapse put/pet operation 10. models inspect element: group "" 11. navigate responses table use caps+arrow keys read row2,column2 12. pet expand operation 13.
if including tracebacks, please include full traceback.
however they 're shown to be 8k only.
no error is raised if i use one of the following fixes: 1) use 2) remove the `conv2d` layer 3) remove the `batch_size` and `epochs` arguments from the `.fit` call however, context setter seems to have no effect and training is happening on gpu anyways (i can tell by how fast it 's training) behavior seems weird, can anyone explain what 's going on?
the inference speed of model is 90ms which corresponding to `inter=1, intra=1` but not `inter=1, intra=8`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): this appears to be caused by one of the inputs to the betainc function being set to 1, as changing the definition of space_x from t > space_x = np.linspace(1e-16, 1 c 1e-16).tolist() t > space_x = np.linspace(1e-16, 1 c 1e-8).tolist() removes the nan.
here is the code i set the tflite interpreter options: phofcode and here is code i used to inference(where input is a four dim array, and output a two dim array, corresponding to model input and output dimentions): `tflite.run(audiodata, output);`
the 60 elements of the output of matmul(x, w) should match exactly the first 60 outputs of matmul(tf.tile(x, [1, 2, 1]), w)
then i got the following error messages.
from keras.callbacks import modelcheckpoint, earlystopping from model import load_model import numpy as np import argparse args=parser.parse_args() frames=x_train.shape[2] #need to make number of frames divisible by 10 frames=frames-frames%10 y_train=x_train.copy() epochs=args.n_epochs batch_size=1 if __name__=="__main__": t model=load_model() t callback_save = save_best_only=true) t callback_early_stopping = patience=3) t print( 'model has been loaded ') t t t batch_size=batch_size, t t t epochs=epochs, t t t callbacks = t t t )
`sysctl -p` fails in similar way on my previously working ipv4 only server, so perhaps i disabled ipv6 after installing algo and didn 't run into this failure before.
my build of doesn 't have this problem.
scalar example: phofcode vector example phofcode executes sucessfully, but the arithmeticoptimizer fails with a warning: -2 w failed to run optimizer arithmeticoptimizer, stage node strided_slice.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macosx 10.13.6 - mobile device (e.g.
it should run smoothly as described in the documentation.
if you make any errors with input signature on autograph / tf.function, the error message is essentially useless: neuromax.py:396 train * change, total_change, episode, episodes_this_dataset = run_episode(type, n_atoms, target_positions, positions, features, masses, quantum_target, target_features, change, total_change, episode, episodes_this_dataset, gif) __call__ return
- both terminal and gui
... steps to reproduce the behavior: 1. start internet explorer 11 2. go to phofurl 3. example value is ` [ null ] ` 4. click on model 5. model is ` [ { } ]`
memory consumption should be bounded for any number of iterations.
note: with the code below, there is no error message.
the current issue is that autograph doesn 't recognize `tqdm(dataset)` as a `tf.data.dataset` (which is normal).
then run this: phofcode use saved_model_cli to inspect the saved_model
1. mongo dump a strapi database after set permissions on public 2. mongo restore on another database 3. create new strapi on this new database 4. look permissions on new strapi, they are empty, but in database the entries are in the right document
`tf.numpy_function` works are wrapping numpy / python functions with tf 1.14 in eager and normal mode, but shape is lost phofcode
with mask_zero = true, anytime there is an input of 0 in one of the time steps, the output should be same as the previous time step 's output.
i think that bug happens when number of columns is >= 203.
5.0.1] add any other context about the problem here.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the code under heading "1.7 stacking recurrent layers" also runs incorrectly in tensorflow 2.0.0 using tensorflow.keras.
1. the result of running on the gpu of the server should be the same as the result of gpu running on tx2.
phofcode after this a variables folder with files saved_model.pb and assests folder created in newdir.
the "global" security requirement should work for all endpoints for the defined endpoint.
but the issue seem to come from one of the custom layer which has a kernel defined like so: phofcode and a call function defined as so: python def call(self, x,
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
- os platform and distribution (e.g., linux ubuntu 16.04): win10 - tensorflow installed from (source or binary): pip - tensorflow version (use command below): 2.0 alpha gpu - python version: 3.7
class def self.batch_size=8 self.num_outputs=256 self.alpha=alpha # (batch_size, def call(self,inputs): print("inputs shape:",inputs.shape) output_up=self.up(temp) return here is my selfdefine layer ,it can run,but i found change to my selflayer my eval loss and acc don 't change anymore
futurewarning: conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated.
it should support multiple inputs models.
it should start training, or more probably throw some other error honestly, everything related to tpus on colab is in aplha, so i was kinda expecting bugs after bugs and so it happened, nothing personal this is the final bug i got stuck on
i expect custom cells to behave like custom layers: the first time the cell is used, all of its internal layers should be automatically built.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i follow "quantization aware training" tutorial in tensorflow to retrain my already trained floating point model.
which is weird as exact same line works juste fine on local machine : phofcode (sparing you variable names).
this behavior exists when using python2 and when using python3.
produce kind of the same results for the same models (no matter c=1 or c=3) for cpu host tf and tflite and android tflite on cpu and gpudelegate.
performance should actually improve with intel mkl instruction set .
nano does not have this problem.
i can work around this issue by using: `from import model_to_dot`, but i believe importing from `tensorflow.python` is frowned upon and may break in the future.
according to the documentation provided, model.predict should also be able to take a tensorflow tensor as input.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i expect that an output tensor representing the class of the input handwriting digit is returned from the serving.
discriminator model code: phofcode generator & dircriminator body phofcode
tflite outputs is expected to match tensorflow outputs
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
this error does not happen if i do not register sessionrunhook to evaluation_hooks.
python class decoder(model): def __init__(self, dim): super(decodercell, self).__init__() self.input = sequential([ input(batch_shape=(32, 1, num_mels)), dense(128, activation='relu') ]) self.rnn = lstm(256, stateful=true) def call(self, x): x = self.input(x) x = self.rnn(x) return x ``
it should only evaluate all outputs defined in keras model, not only pick one.
when calling `c_api.tf_sessionrun` from 10 parallel threads in an infinite loop for a minute or so - some sort of corruption occurs that causes tensorflow terminate with the following error code `code which is: > stack buffer overflow / overrun.
`vim.eval` changes all numeric keys/values into string.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):yes - os platform and distribution (e.g., linux ubuntu 16.04):win 10 - tensorflow installed from (source or binary):binary - tensorflow version (use command below):2.0.0-alpha0 python version:3.6 the value_estimate is calculated by the session and model.value.
make several tests but fail, in particular, with atomic assignments.
i mean, it should just work, shouldn 't it?
provide a reproducible test case that is the bare minimum necessary to generate the problem.
and it is impossible to achieve this using `firstline` only.
... steps to reproduce the behavior: 1. go to '... ' 2. click on '.... ' 3. scroll down to '.... ' 4. see error 1. start the swagger petstore demo ** locally ** 2. navigate to phofurl 3. scroll down the modal-section 4. click on a modal-element (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary, using pip tensorflow version (use command below): 1.13.0-rc0 python version: 3.6.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
is there another way how one can remove it?
i am unable to change the learning rate on the fly.
it outputs the mapping only once: c <c-a> * xxx
1. go to content type builder 2. click on any already existing content type 3. click on add new field 4. select data type, insert name and default value 5. save
edit: since writing this, i have tried run more code in notebook.
phofurl (see ssr js output)
when `set nohidden` is used and a text property is applied to a hidden buffer, the text property is not persisted and various errors can occur.
expected behavior is that the nnapi time should be much shorter than cpu
- have i written custom code (as opposed to using a stock example script provided in tensorflow): maybe.
if including tracebacks, please include the full traceback.
undoing a change to end of buffer (`cg`) with a `listner_add` callback function that itself includes a function call fails with: phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.6.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: 10.0 gpu model and memory: yes, 11gb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:no - tensorflow installed from (source or binary): pip install tensorflow version (use command below):conda list 2.0.0a0 python version:python 3.6.8 :: anaconda custom (64-bit) cuda/cudnn gpu model and memory:gtx 1050 ti
after debugging i figured out that last thing that ansible does is running ansiballz_pip.py && sleep 0. so i think its ansible pip related issue while it's the last module called.
i attached example and memory profiler logs.
image phofimage - vim version [e.g.
when i add multiple tensors with shape (), tf.keras.layers.add complains that > file line 680, in __call__ file line 1905, in _maybe_build self.build(input_shapes) file line 299, in wrapper output_shape = fn(instance, input_shape) file line 95, in build batch_sizes = [s[0] for s input_shape if s is not none] 95, <listcomp> batch_sizes = [s[0] for s input_shape if s is not none] indexerror: tuple index out of range
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution: ubuntu 18.04 x86_64 gnu/linux) - mobile device (e.g.
base_model = include_top=false) x = base_model.output y = z = activation='relu')(y) predictions tf.keras.layers.dense(4, activation='softmax')(z) model outputs=predictions) for layer in base_model.layers: layer.trainable false model.fit(traindata, shuffle=true) model.save(mypath) outvalues model.predict(testdata) model #error here
i don 't think my program has a bug that is responsible for the exception but even if it does, the throwing of the exception should be consistent and not depend on `episode_length` being big enough.
nvidia driver: 390.87 bash kyle@debian:~$ nvidia-smi fri nov 23 2018 | nvidia-smi 390.87 driver version: 390.87 | | gpu name persistence-m| bus-id disp.a | volatile uncorr.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): - mobile device (e.g.
installation of keras from source fails by doing the following.
to illustrate this further, data layout similar this: file 1: file 2: file 3: 4: ... ... where `-` denotes irrelevant `#` denotes relevant points time series.
would expect the program to finish without a warning.
the tensorflow.dll we use is not built on debug, in order to use a custom tensorflow.dll with a .pdb, replace the one in et461 or netcoreapp2.2 ` and then run `calceventstfs.exe` simply build the solution and run the only project there is.
a clear and concise description of what you expected to happen.
my network is shown below def model(x, y, is_training, is_quantize): with slim.fully_connected], biases_initializer = is_training, 'epsilon ':1e-5, 'scale ': true, data_slice = tf.slice(x, [ 0, 0, 0, 0], [ train_batch, 64, 60, 3]) conv1 = slim.conv2d(data_slice, 16, [3, 3], scope= 'conv1 ')
large logs and files should be attached.
i would expect the output to be random.
might be wrong but want be sure that proceed find fix for right thing.
when i generate the profile file by using the low level approach of runoptions, runmetadata, and then writing the string generated by i can load it with tfprof and analyze it just fine.
the request is executed and includes use of the required header.
... steps to reproduce the behavior: 1. have the server return a 200 with an empty response body to have the spinners spin forever.
i also have seen other people online having issues with tf.session, so i 'm wondering if that could be it?
is this something we can avoid?
`max_len_text=275` `max_len_summary=28` `from keras import backend as k ` `k.clear_session() ` `latent_dim = 500 ` `encoder_inputs = ` `enc_emb = embedding(x_voc_size, `encoder_lstm1 = `encoder_output1, state_h1, state_c1 encoder_lstm1(enc_emb) `encoder_lstm2 `encoder_output2, state_h2, state_c2 return_state=true, return_sequences=true)` `encoder_outputs, state_h, state_c= `decoder_inputs input(shape=(none,)) `dec_emb_layer embedding(y_voc_size, `dec_emb `decoder_lstm lstm(latent_dim, return_sequences=true, return_state=true)` decoder_back_state state_c]) `attn_layer `attn_out, attn_states decoder_outputs])` `decoder_concat_input concatenate(axis=-1, attn_out])` `decoder_dense `decoder_outputs `model model([encoder_inputs, decoder_inputs], decoder_outputs) `es mode= 'min ', verbose=1)` 1)[:,1:] 1)[:,1:]))` provide a reproducible test case that is the bare minimum necessary to generate the problem.
the trained model should achieve similar level of accuracy regardless of virtual devices (0.9938) or physical devices (0.9698).
(i 've prepared pipeline.config, classes.bptxt and one tfrecord - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu (linux smok #48-ubuntu smp tue jan 29 utc 2019 x86_64 x86_64 x86_64 gnu/linux) - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary):source tensorflow version (use command below): 1.12.0 python version: 3.6 bazel version (if compiling from source):n/a gcc/compiler version (if compiling from source):n/a cuda/cudnn version: 9.0 / 7.1 gpu model and memory: gtx 1050 ti
if including tracebacks, please include the full traceback.
python version: 3.6 cuda/cudnn version: no gpu used include any logs or source code that would be helpful to diagnose the problem.
when using python internal "logging" module with tensorflow, a logging module emits below warning message.
if the saved function (decorated with tf.function) returned a dict {'output_a': a, 'output_b': b} names 'output_a' and 'output_b' are in saved_model.
to return a dataset iterator
`:h expandcmd()` says that the function expands special characters anywhere in the string.
i am using tensorflow 's c api to train models created with tf.keras in python.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):yes - os platform and distribution (e.g., linux ubuntu 16.04):ms windows10 x64 1809 build 17763 - mobile device (e.g.
i am not using macports or brew, i attempted to run brew install curl-ca-bundle but no package was found.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 python version: python 3.5.2 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory:
the last line `a b` is unexpected.
crash when viewing the value.
interrupt training after a few epochs have completed, then re-run the script with `initial_run=false`.
fix the rng bug in parallel mode maybe?
two tabs in the tab line: b and c
for tensorflow 1.13.1 2.0.0 situation is more unclear couldn't find a workaround this time.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):
steps to reproduce the behavior: 1. open ios settings -> vpn 2. tap on right side of algovpn algo ikev2 string 3. behold there is no delete(remove) button
wvi \'y" -c "execute (@@ == 'here ' ? '
specify signature functions explicitly.` however it works if i define `call()` method and i specify signature explicitly.
phofcode the aforementioned code runs in an image generated by the following dockerfile dockerfile from centos:7 env source_directory /tmp/tf-connect4 env python_version 3.7.4 run yum -y groupinstall -y "development tools" && yum -y update && yum -y install openssl-devel zlib-devel libffi libffi-devel wget && wget phofurl && tar -xjf cd python-$python_version ./configure make make install pip3 install --upgrade pip pip3 install tensorflow==2.0.0-rc1 numpy falcon jsonschema ``
code runs forever although nothing is to be trained (no parameters, no data).
>>> s = "hello world" >>> r" ", r" t") invalid rewrite pattern: t <tf.tensor: id=4, shape=(), dtype=string, numpy=b 'helloworld '> ``
2. also get an exception if interrupt training, then delete the logs directory, then try to use `tensorboard` callback on same logs directory again: phofcode this one is more severe: sometimes it recovers by itself after a while.
just follow the exact command in your document on a fresh ubuntu 18.04 (follow exactly from installing npm,....)
the contents should be identical.
feature size is around fm cross_emb_size is 4. i have checked the graph from tensorboard and timeline hooks.
the result of rping shows that rdma configure is good.
create a hasmany relation like this: "type": "hasmany", ... "properties": { "type": "type" }, "options": { "invertproperties": true }
provide a reproducible test case that is the bare minimum necessary to generate the problem.
for my research work i need to build lots of different convolutional gan models and train them.
specifically, in our case, should it allow a user to all gather two size `7` tensors into a size `14`?
`python3 -m virtualenv --python="$(command -v python3)" .env && source .env/bin/activate && python3 -m pip install -u pip virtualenv && python3 -m pip install -r requirements.txt` 5. add a few users to `config.cfg` 6. run `./algo`
to compute the jacobian without complaining.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): custom code - os platform and distribution (e.g., linux ubuntu 16.04): windows/ubuntu - tensorflow installed from (source or binary): tried source and binary - python version: 3.6 cuda/cudnn version: 10/7.3 gpu model and memory: rtx 2070
i am trying to extract the features generated by the model 's densefeatures layer by creating a new model based on the inputs of the original model and outputs of densefeature layers.
training a model defined on the cpu raises a when using a machine with a gpu in tf 2.0 nightly.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the github bug in this issue has bad troubleshooting steps and is going down the wrongconclusion
- have i written custom code (as opposed to using a stock example script provided in tensorflow): y - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
\'quit \'" or, using the example from #5183: vim --clean :set selection=exclusive<cr> i foo 'bar ' baz<esc> 0wvi '
stack trace: `traceback (most recent call last): file line 109, in <module> tf.app.run() file line 125, in run _sys.exit(main(argv)) file line 105, in main train_spec, eval_specs[0]) file line 471, in train_and_evaluate return executor.run() 611, run return self.run_local() 712, run_local 358, train loss = hooks, saving_listeners) 1124, _train_model return hooks, saving_listeners) 1154, _train_model_default features, labels, self.config) 1112, _call_model_fn model_fn_results =
below is the model definition: phofcode
i have written a dummy custom c++ op where i 'm simply printing input tensor and returning.
i expect to be able to use the `tpuestimator` with my own `hparams`.
i then noticed how the two systems handle the `placeholder` and `variable` are different even though they are using the same backend.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): 16.04 - mobile device (e.g.
zoom out to find lots of stuff uninitialized: x/64x 0x7fffee4ddb00 0x7fffee4ddb00 0x7fffee4ddb10 0x7fffee4ddb20 <_zstl8__ioinit>: 0x7fffee4ddb30 <_zstl8__ioinit>: 0x7fffee4ddb40 0x7fffee4ddb50 <guard variable for 0x7fffee4ddb60 <guard variable for const*, int, 0x7fffee4ddb70 <_zstl8__ioinit>: 0x7fffee4ddb80 0x7fffee4ddb90 0x7fffee4ddba0 0x7fffee4ddbb0 <guard variable for 0x7fffee4ddbc0 0x7fffee4ddbd0 0x7fffee4ddbe0 0x7fffee4ddbf0
after building the model, `tf.keras.fit_generator` phofhyperlink should accept `(inputs, targets, sample_weights)` as inputs.
however, the actual training data still fits into ram and i want to make sure it is prefetched even when using the `tfrecorddataset`.
when the different `tf.io.gfile` functions are called inside the `tf.function`, like `makedirs`, it raises an error indicating that it requires a binary or unicode string as input: phofcode if try to use `.numpy()`, it is not available, as expected.
the displayed curl request should not be undefined.
i am seeing differences of 0.3 at times (see my output below).
the problem doesn't happen with horizontal splits as at least one line is left visible and vim doesn't need to expand the window when you switch it
> * `axis`: a 0-d int tensor representing the axis in `tensor` to mask from.
tested in arch linux terminal vim (built from git) and windows 10 gvim (from vim-win32-installer phofhyperlink
detailed steps to reproduce the behavior: 1. open nautilus, hit ctrl+l to get the url bar, and type in "ssh://your.server", and hit enter.
launched another p3.16 instance with a single thread per cpu, so that number virtual cpus is identical p3.8 instance.
the depthwise conv2d performance should be same in the official model and the replicated model.
3. the data is scaled to 0..1 range.
the evaluate will happen after 600 secs or training is finished.
a spellfile path cannot contain a @ character.
passing data with variable batch sizes to a training graph (as created by `tf.function`) causes the graph execution to be significantly slower than just executing the training in eager mode.
phofcode i am not quite familiar with python multi-thread.
show popup windows at the right top and disappear after 5s
row is added, however app should not close (crash)
it is only available in `tf.math.log()`.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04.4 lts - mobile device (e.g.
thanks, venkatesh.s include any logs or source code that would be helpful to diagnose problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary):colab tensorflow version (use command below):1.13 python version:python 3 bazel version (if compiling from source):unknown gcc/compiler version (if compiling from source):unknown cuda/cudnn version:unknown gpu model and memory:unknown you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
build gpudelegate failed on android p java demo
1. subscribe channel on console window 2. receive very long messages (in my case, over 5~6,000 column) 3. can not drag received text block, and can not copy to clipboard and horizontal scrollbar not working.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.13.6 - mobile device (e.g.
if i understand correctly, the library is smart enough to choose between gpu and cpu.
if applicable, copy/paste the text or add screenshots to help explain your problem.
when putting something into the system clipboard using e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - windows 10 - tensorflow installed from pre-built binary - tensorflow version: 1.13.1 python: 3.6.6 cuda/cudnn version: 10.0/7.1 gpu model and memory: gtx 960m/4gb cpu: intel i7 6700hq
large logs and files should be attached.
`attributeerror: 'mirroredstrategy ' object has no attribute 'read_var '` when trying to use mirroredstrategy.
line 76 in except oserror: raise importerror( "could not find %r.
i am almost crazy with tf 's multiple gpu.
it seems like it 's being called once or a few times, but then a "cached" value is repeatedly used in updates.
all the working examples do not have that.
file only folded in bottom window c because `:badd` run in `b.txt` buffer: $ vim -nu none +'e /tmp/a.txt|setl fdm=marker|new|badd /tmp/a.txt|b a.txt' i could be wrong, but think this a bug because don't understand why a `:badd` command would affect how a window-local option set.
in our models we have them in reasonable range and still get wrong behavior.
- vim version: phofcode it looks to me like we 're double-counting the `off` offset in the following calculation in `textpos2screenpos` in `move.c`: phofcode the offset is added into `col` and also taken away from `width` so later, when we do this check: phofcode we 're off by 1x `off`.
after training and freezing, in toco, i have following code: `graph_def_file = "my_frozen.pb" //this is the .pb file.
registered devices: cpu], registered kernels: > <no registered kernels> > > [[{{node = sin[t=dt_float, _device="/gpu:0" phofhyperlink ]]
`tf.data.dataset::cache` generates a single file with size 45mb.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: oneplus5, pixel3, galaxy s10 - tensorflow installed from (source or binary): various tensorflow version (use command below): 1.12.0, 1.13.1, 1.14.0, r1.14 source build python version: 3.x bazel version (if compiling from source): 0.25.2 gcc/compiler version (if compiling from source): 7.4.0 cuda/cudnn version: gpu model and memory:
recently, we have designed a tool to monitor the communication of distributed training process in microsecond.
either, tf 2 should behave similarly to tf 1, ie use up the entire validation set.
i would expect the shape of x to be: dimension(3), dimension(4)])
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): osx - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): docker image from - mobile device (e.g.
i already commented --data_url but i got positional argument error.
4. run `:q` to close the command-line window.
for the command line window to be closed by the autocmd.
iterating over the elements of a `raggedtensor` results in an after the last element.
for example, if `(static_cast<float>(x) + 0.5f) * scale` equals 1.4, minus 0.5f or not will brings completely different result, no matter there is or is not `std::floor`.
model has both inputs after loading.
both systems allow two threads for each cpu.
i am trying to load a 1.4gib binary saved model and am getting an error saying that the protobuf message is too big i.e.
is there a script anywhere in the tflite repo that prints out all the operations contained in a .tflite file?
detailed steps to reproduce the behavior: 1. ensure the terminal displays some 24-bit color, e.g.
i 'm using buildfrombuffer() api in model.h using a mmapped buffer and not getting a valid model back.
phofurl **fetch error** possible cross-origin (cors) issue?
`l`, the screen is redrawn and the cursor is drawn in correct column.
no cursor line should be displayed if `cursorline` is off.
attempting to receive multiple outputs from a lambda layer.
when a layer has multiple nodes (shared layer) in calling the method `layer.output` or `layer.output` returns only the first node created.
i am trying to optimize the speed of my code when training on imagenet and started by looking at the data pipeline.
this bug also appears in neovim version 0.4.2. see issue #11300. phofhyperlink
phofcode it seems that two odd newline character was mixed into rng.h phofhyperlink as follows.
specifically in my test case (see attached model below), in the original freeze graph model, `matmul_6` takes a product of 32x12 matrix and 12x1 vector then `add_7` adds a 32x1 vector to it as a bias.
large logs and files should be attached.
image phofimage image phofimage image phofimage - windows 10 17763 - 4.0.2.
therefore, it did not make sense for `:resize` accept `0` argument; closest height it could accept was `1`, so automatically turned `0` into `1`.
it takes minutes to generate the model input1 = x_a = input1 for i in range(6): x_a = tf.keras.layers.conv1d(8 * (2
- have i written custom code (as opposed to using a stock example script provided in tensorflow): custom - os platform and distribution (e.g., linux ubuntu 16.04): run in colab phofhyperlink - tensorflow installed from (source or binary): pre-installed in colab phofhyperlink - tensorflow version (use command below): 1.12.0 pre-installed in colab phofhyperlink python version:3.6.7 (default, oct 22 2018, gcc 8.2.0] cuda/cudnn version: gpu model and memory: tpu you can collect some of this information using our environment capture [script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" 1.12.0`
the `netrw_gitignore` plugin doesn 't work with the pattern `.
so the api `function` defines the input should be a list of `placeholders`.
this is the code for exporting savedmodel: phofcode the conversion command: phofcode
unfortunately i cannot give a concrete code example to reproduce this issue since memory leaks appear anytime in between 10min to 12h of training.
- vim version phofcode - arch linux - terminal: rxvt-unicode
1. go to strapi admin 2. go to files upload 3. drag and drop or browse and select file greater than 2mb 4. wait for upload to fail and see error
provide a reproducible test case that is the bare minimum necessary to generate the problem.
tflite output different result with pbfile when using only one convolutional layer ?
the two issues can be reproduced using this colab notebook phofhyperlink .
see attached pipeline.config (would need to specify some training/testing tfrecord)
from __future__ import absolute_import, division, print_function import numpy as np import pandas as pd from ipython.display import clear_output !pip install tensorflow==2.0.0-alpha0 tensorflow as tf tf.random.set_seed(123)
if a model output is not part of the loss function, then it should be ignored when casting targets.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): productname: tmac os x buildversion: t18c54 - tensorflow installed from (source or binary): pip install ... - tensorflow version (use command below): 2.0.0-beta0 python version: 3.6.8
i am also able to invoke the compiled graph, producing correct prediction results.
a simple solution for this (in <pre> header_size *width *height bpp </pre>
i was following the keras tutorial on tf website and i was trying out my custom layer, which was basically the same as in the tutorial.
shape: unknown_rank # <= what is this?
- some way to build models so that the gpu memory they occupy gets automatically released when they go out of scope would also be appreciated.
invalidargumenterror traceback (most recent call last) in get_attr(self, name) 2408 with c_api_util.tf_buffer() as buf: -> 2409 name, buf) 2410 data = c_api.tf_getbuffer(buf) invalidargumenterror: operation 'raggedtile/tile ' has no attr named '_xlacompile '.
it seems that tensorflow will increase memory usage when i compiled keras model serveral time.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: not test - tensorflow installed from (source or binary): binary (nighty) tensorflow version (use command below): python version: 3.6.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: nvidia-smi 418.56 driver version: 410.79 cuda version: 10.0 gpu model and memory: tesla t4 <p> phofcode </p> </details
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): pip3.7 tensorflow version (use command below): 1.13.1 python version: 3.7.2 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: running the cpu version gpu model and memory: na you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" i was trying using a variable to track a seed number in dataset.
for experimentation, i use colab, so here 's the colab notebook phofhyperlink .
provide a reproducible test case that is the bare minimum necessary to generate the problem.
then jump a couple thousand lines down.
use `--use_hparams` argument in the demo script.
i expect tensorflow to continue to run successfully when using the --runtime=nvidia arg, i.e.
i used tflitelstmcell defined in this file: phofurl i manually inserted fake quant nodes in the graph and generated a .pb file.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
somewhere in-between and highlighting calculation are done wrong and now visual selection has normal foreground colors even it doesn 't define any.
1. update to strapi beta.7 2. try to connect with facebook 3. an error 500 occurs phofcode _i 've overload the auth.js but with the "original" version, the bug still happen._
... steps to reproduce the behavior: 1. go to editor.swagger.io and upload the swagger above 2. in the gui pane notice that description, content type data is rendered outside the table.
when i run this code, it is super slow.
convert into a singe depthwise_conv_2d v2(dilation=2)
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): win 10 - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:no mobile device related - tensorflow installed from (source or binary):binary tensorflow version (use command below): '1.12.0 ') python version:2.7.15 bazel version (if compiling from source):installed from binary gcc/compiler version (if compiling from source):installed binary cuda/cudnn version:no cuda gpu model and memory:no computing gpu include any logs or source code that would be helpful to diagnose the problem.
calling `tf.gradients` with returns scalars for unconnected resource variables.
it will always generate the error when you compile.
nor does it occur when using example phofhyperlink in a similar capacity.
all the following functions should return almost identical graphs.
just as it should be.
list of operations with kernel queue times reported, but no execution times can be found here.
however, assigning name to it using tf.identity works.
using tf.io.decode_image() and tf.image.resize() in dataset.map() generates valueerror: 'images' contains no shape.
in a terminal vim session of phofurl balloons quickly disappear after being shown, but in gvim they remain until the mouse is moved: * terminal: phofurl * gvim: phofurl
18.71mib 10.71mib 10.71mib 5, 3.
parameter or path variable is in generated curl before execute captura phofimage after captura2 phofimage
the file `a.txt` is folded in the top window, just like it is folded in the bottom window, because: - `a.txt` is already displayed in the bottom window - `a.txt` is folded in the latter - when editing a buffer that has been edited before, options from window that was last closed are used again (see `:h local-options`) - closing window not a requirement anymore since phofurl ; i.e.
1. go to '... ' 2. click on '.... ' 3. scroll down to '.... ' 4. see error
warning:tensorflow:from colocate_with (from is deprecated and will be removed in a future version.
phofcode this produces this output, after going through the training fine, it crashes on the validation set phofcode
tensorflow_transform throwing error when tf 2.0.0 version is installed.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
in tensorflow 2.0.0 supposed the model.save model default to be saved in savedmodel format.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): 18.04 - mobile device (e.g.
`gvim -n -u .vimrc -u none -i none` where `.vimrc` contains a single line `set 2. execute `:term`
expect to be able to train with any optimizer from keras' options.
`set_model` keras_team keras fit_generator(): 1.
i am trying to build the tf lite detect app phofhyperlink with using nnapi, but when i set usennapi to
steps to reproduce the behavior: 1. go to '...' 2. click on '....' 3. scroll down to '....' 4. see error
always clear the highlighting without needing to run `:redraw!` or `<c-l>` afterwards, even when the focus is on another window.
use `if t is not none:` instead of `if t:` to test if a tensor is defined, and use tensorflow ops such as tf.cond to execute subgraphs conditioned on the value of tensor.
i freeze a ckpt file into a pb file in python and want to load it in c++.
if including tracebacks, please include the full traceback.
this doesn 't make any sense actually, because modification at (netrw.vim: 2220 phofhyperlink ) masks previous careful work on modifying file path by testing whether user is in cygwin mode or not (netrw.vim: 11518 phofhyperlink ).
i created a model phofhyperlink with only tf.keras functional api.
`attributeerror: 'sparsetensor ' object has no attribute 'tocoo '`
i have trained a model and then i 2 versions of frozen graph .
true are weights empty after training?
please use for better performance on gpu.
to do this, i am manually setting the `@/` register after escaping the appropriate characters.
i have verified my netgear router configuration and there is nothing there that drops any vpn related packets.
the cause of the difference between the dataset performance and iterator performance is likely at `training_utils.py:1314` where keras creates new iterator for each `predict` loop.
the issue originates from ___please note the `readonly` attribute___ phofcode
they fail as expected if sharding is disabled.
wireguard-log.txt phofhyperlink fulllog.txt phofhyperlink play [ask user for the input]
provide a reproducible test case that is the bare minimum necessary to generate the problem.
with warmstartsettings: from: variable: dense/kernel; prev_var_name: unchanged variable: dense/bias; prev_var_name: unchanged variable: dense_1/kernel; prev_var_name: unchanged variable: dense_1/bias; prev_var_name: unchanged dense_2/kernel; dense_2/bias; adam/iterations; adam/lr; adam/beta_1; adam/beta_2; adam/decay; training/adam/variable; info:tensorflow:create checkpointsaverhook.
get the following error when using tf.case and slim.l2_regularization.
not to confuse with issue #21893
apologies if this irrelevant to tensorflow and indeed an anaconda issue.
tested on jupyter notebook jupyter --version 4.4.0 and google colab
when i call it, tensorflow complains that >attributeerror: tensor.op is meaningless when eager execution is enabled.
i'd like to avoid these frequent allocations, but more importantly i am having hard time figuring out why am getting this behavior in first place.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
large logs and files should be attached.
a model created with can be loaded by using "./model")
i use sample code in tensorflow 2.0.0 rc: classify images
for example `:py prints `{ '0 ': '0 '}`.
<pre><code> e error polling for event status: failed to query event: an illegal memory access was encountered f unexpected event status: 1 e could not synchronize on cuda context: an illegal memory access was encountered ::
the python interpreter crashes with sigsegv (segmentation fault); according to gdb the fault occurs in phofcode .
i run the code on my local machine : the tf 2.0 alpha train your first neural network: basic classification.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
manually placing dataset + all processing steps on cpu with also fails with same error when training tpuestimator.
i have a command requesting the internet.
... steps to reproduce the behavior: add a summary and description under a path
all code results in the issue.
as a workaround and a proof i disabled layout optimizer with following config: phofcode and most of performance were back again in `tensorflow 1.9.0-devel-gpu-py3`: and in latest release `tensorflow 1.12.0-devel-gpu-py3`: here phofhyperlink is a full log of my bisect script.
cpu and gpu delegate should produce the same masks
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ml engine instance (debian) - tensorflow installed from (source or binary): source - tensorflow version (use command below): 1.10.0 python version: 2.7.14 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a here 's what i see in the ml engine logs: `14:24 epoch 1/4` `15:00 starting evaluation` with no logging in between
a colab notebook that reproduces the issue is here: phofurl specifically, a dummy dataset with 256 samples with validation_split = 1/8 should result in 224 training samples and 32 validation samples, but currently shows 256 training samples and 32 validation samples.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 / 1.14.dev / 2.0.0-alpha0 python version: 3.7.3 cuda/cudnn version: gpu model and memory:
exception raises when trying to feed sparse/ragged tensors to keras model.fit().
when i use a tensorflow tensor created using `tf.constant` as input to keras `model.predict` it gives an error.
cannot proceed with quantization.` look at my graph (the one modified by and don't see min/max nodes for biases.
the plot of derivatives are provided in the image .
but, if i disconnect the power plug in my laptop, it's works.
- os & version: [e.g.
this could mean that the variable was uninitialized.
i implemented waveglow model, in the model it contains dilated convolution, so in early time i used tf.nn.conv2d to implement the dilated convolution model- -- wavenet, i used the default data format nhwc, but after many experiments, i found the model does not convergence.
include any logs or source code that would be helpful to diagnose the problem.
if including tracebacks, please include the full traceback.
this was spotted in the context of `govim` phofhyperlink - i haven 't managed to find a smaller reproduction.
phofcode the output value is: phofcode notice that we get the same sequence of random numbers every time, ignoring the value of the global random seed.
image: screen shot at 8 29 54 am phofimage
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): (note: this is the 2.0-preview) python version: 3.6.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: here is the output of this program: phofcode notice that the evaluation progress bar (last line) does not go up to 100% (it stops at moreover, there is no newline at end, so function 's returned values 0.8358]`) are printed on same line.
the code below shows that before execution, tensors do have right shape.
i was able to do the inference successfully in the same without any issues.
sometimes output can be as large as infinity or some very big numbers.
the issue is that there isn 't a complete guide on how to use correctly tf.function.
codesandbox example: phofurl (specifically the todos.svelte file)
since we only know the end() is called before closing the session.
using the following code snippet to create an interpreter with gpu delegate phofcode calling the run() of the interpreter with following lines of codes: phofcode if these two code snippets are called in two different threads, the thread which calls interpreter.run() will be blocked.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): master branch from python version: bazel version (if compiling from source): 0.18.0 gcc/compiler version (if compiling from source): 5.4.0 cuda/cudnn version: 9.0/7 gpu model and memory: 2x tesla p100 16 gb fails on both x86 and ppc64le, tried with both bazel 0.15.0 and 0.18.0
the distribution to the pod fails with 1.
editing a simple help vim file (help filetype) with vim-airline, after 3 o 4 saves with ":w", some errors are printed in vim output, and in the next save, vim crash.
python3 import tensorflow as tf import numpy as np fast_optimizer = learning_rate=1e-3) slow_optimizer = learning_rate=1e-3 * 1e-9) @tf.function def grads, vars): grads = [grads] vars)) def apply_grads(use_fast, grads_per_model, vars): for i in range(2): if use_fast[i]: grads_per_model[i], vars[i]) else: grads_per_model[i], vars[i]) def compute_loss(w, x, y): r = (w * x - y)
from the above repro case: <img width="1139" alt="screenshot at 12 20 50" src=" phofurl another example: <img width="913" alt="screenshot at 11 55 23" src=" phofurl vim - vi improved 8.1 (2018 may 18, compiled aug 24 2019 macos version included patches: 1-1915 compiled by ben@benmbp.local huge version without gui.
@tf.function def compute(): arr = 1, dynamic_size=true) def body(i, arr): real_logits = tf.random.normal([5, 1]) arr = arr.write(tf.cast(i, tf.int32), real_logits) i += 1 return i, arr def cond(i, arr): return i < 10 _, arr = tf.while_loop(cond, body, (0, arr)) c arr.concat() shape:', c.shape, 'rank:', c.shape.rank) return c @tf.function def compute_ds(): 1, dynamic_size=true) body(state, _): i, state real_logits tf.random.normal([5, 1]) arr.write(tf.cast(i, tf.int32), real_logits) i += 1 return i, en_ds _, en_ds.reduce((0, arr), body) c arr.concat() shape:', c.shape, 'rank:', c.shape.rank) c print('
however when using the same code with the same tensorflow version/ tensorflow api version, etc.
- vim - os: macos 10.14.4 - terminal: iterm2
if including tracebacks, please include the full traceback.
import tensorflow as tf phofcode
datasetb is expected to correspond to the order of shuffled dataset a.
the `lcd` should not print messages during an autocmd when `silent!` prefixes it.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
some of the gpu supported tflite ops, does not run in gpu properly.
error: tflitegpudelegate prepare: program not properly linked: l0005 the number of compute uniform components (1261) greater than maximum number allowed (1024).
this works as expected in tensorflow-gpu==2.0.0rc1 however.
the following toy example illustrates problem: phofcode i get following error: `valueerror: this input must be scalar but has rank 1 for 'topkv2_20' (op: 'topkv2') with input shapes: [3,4], [3] and with computed input tensors: input[1] = <2 1 2>` diving into source code, it would seem that topkv2 op is _precisely_ intended for varying values of k, however, something at a higher level is preventing my code from utilizing this functionality.
the `build_info` module should also contain information about cuda version among other things.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary, pip install tensorflow version (use command below): python version: 3.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: cudatoolkit cudnn 7.6.0 gpu model and memory: 4x nvidia geforce gtx 1080 ti
steps to reproduce the behavior: 1. select key that you want to populate 1. click "add row" and enter value and score 3. click "ok" to add and then application crash
in the official model avg time was around 28 ms; whereas in our own model it was around 103 ms. the only difference between the two float models being quantization levels i.e ours was 0-255 and official has -1 to 0.99.also, there is a difference in size(official:2.7mb vs. ours:3.3mb).
this problem is amplified when the upsampling is repeated.
i am trying to use a pre-made estimator with a dataset fetched from tensorflow_dataset.
i attached a screenshot of the nvidia smi which shows that it utilize 1.7gb only of gpu while use 16gb of ram.
i interrupted kernel after 4 epochs.
unfortunately, it seems that steps/epochs counters are not restored.
for the script below import tensorflow as tf import tensorflow.keras.layers as nn from import get_graph from import to_snake_case from unique_layer_name module = tf.keras.models.model # class module(tf.module): # def __init__(self, name=none,
i expected a valid flatbuffermodel to be returned.
3. run `:3,6source`, and then close popup window with mouse.
compiled without this flag: run session successfully tensor<type: float shape: [] values: 6> output value: 6
version with gpudelegate is a lot slower ms per run vs ms) than cpu.
% --> 126 (arg_name, repr(v))) 127 i v.as_datatype_enum 128 return i typeerror: expected datatype for argument 'output_types ' object at ``
when i convert a keras model to a tflite model all outputs are named identity.
what ive done so far: - i know creates operations `switch` and `merge`, which are not coreml compatible - ive tried converting `tensorflow lite` with similar issues - follow `facenet` (this model uses same `tf.bool` logic for training, validation, testing) conversion process no success - tried [`graphtransforms` phofhyperlink library tried scripts remove / modify control flow created separate graphs avoid extra ops no success _note: abstracted big part of code and network post issue._
1. go to < phofurl 2. click the three things it tells you to click 3. see that the blue and purple (if and else parts of the if/else in app.svelte) are both showing if you remove `transition:slide` from `scaffold.svelte` the problem is no longer there.
i am using keras bundled with tensorflow.
however, when i update my tensorflow version to 1.12.0, the second version(faster) could cause some error("tensorflow graph is invalid, contains a cycle").
before removing unused ops: 113 operators, 173 arrays (0 quantized) before general graph transformations: 113 operators, 173 arrays (0 quantized) after general graph transformations pass 1: 20 operators, 37 arrays (1 quantized) after general graph transformations pass 2: 19 operators, 35 arrays (1 quantized) after general graph transformations pass 3: 18 33 (1 before pre-quantization transformations: 18 33 (1 after pre-quantization transformations pass 1: 12 27 f array slice, which is an input to the conv operator producing the output array conv1/relu, is lacking min/max data, which is necessary for quantization.
info:tensorflow:loss for final step: ... # run 2 info:tensorflow:calling model_fn.
build tflite + opengl delegate successfully with cmake and perform correctly on my aarch64 board.
tf-conv-nhwc-issue.log phofhyperlink here is a zipped of the python example phofhyperlink not sure if this is important, but it seems that all tensorflow ops from function which is called by `tf.data.dataset.map` are not showing up in device placement logs
* data input_fn: ================ input file(s): batch size: 1024 epoch count: 1 mode: eval thread count: 32 shuffle: false ================ ( 'target_dtype ', <tf.tensor 'decodecsv:9 ' shape=(?,) dtype=int32>) info:tensorflow:starting evaluation at parameters from info:tensorflow:finished evaluation at info:tensorflow:saving dict for global step 0: accuracy = auroc = global_step = 0, loss = precision recall parameters info:tensorflow:assets added to graph.
i run a keras adam optimizer with a cnn network.
the code should run as expected, instead, we get the error message below.
simply run python interepreter with attached `.tfite` file.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
my guess is there is some type of connectivity problem and it just gets caught in a tight loop before failing.
based off of the network, found at, phofurl and after some additional modifications, i added layers before and after each convolutional layer that bitcasts the initial tf.float32 to a tf.int32 and back to a tf.float32.
there is so many changes with tf 2.0, that it could be that i am did a wrong implementation but i managed to migrate all old components to keras layer, loss, metrics ... i couldn 't find an equivalent for for tf 2.0 (without using the compatibility module v1)
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.11.0 python version: 3.6.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: nvidia quadro m2200
`converter.convert()` call finishes without errors
i want to be able to compute the top k results for different values of k in a batched evaluation metric.
`:call 4. highlight buffer 1: `:call prop_add(1, 1, 5.
do you wish build tensorflow with xla jit support?
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux opensuse leap 42.3 - mobile device (e.g.
so this works: b, c] but not this: b, c]
i have noticed that mixing-and-matching keras and tensorflow.keras imports can cause some issues.
following attached code continuously increases memory usage.
total #images for training: @train.py:578] total passes of training set is: 2.0466 @input_source.py:220] setting up queue for cpu prefetching ... @training.py:109] building graph training tower 0 on device /gpu:0 ... @registry.py:125] conv0 input: [none, 3, none, none] @batch_norm.py:164] wrn [batchnorm] using in training.
it failed on importing tensorflow
the same was working with tf 1.x
using it converts and gives me a .tflite file with this error: init node conv2d/kernel/assign doesn 't exist in graph` now, when i load this tflite file and try to make predictions on the same input images, it always predicts 'zero ' which is the first class label and with probability = the rest of classes are always 0.00, i get same results when loading my tflite model in android image classification example app from tensorflow repo 's.
i built a keras model with only a `tf.nn.relu`, but the gradient seems to be `none` after being decorated by `@tf.function`
these commands start to work only after doing a `:substitute` command.
vim in the terminal doesn 't seem to support the `focusgained` or `focuslost` events.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): colab.google.com (linux) - mobile device (e.g.
`runtimeerror: params->depth_multiplier * sizeofdimension(input, 3) != sizeofdimension(filter, 3) (0 != 6)node number 7 (depthwise_conv_2d) failed to prepare.` the code runs fine normally, just not after exporting to tflite.
+ weight*b_only_loss, var_list=[b])`, produces wrong results (i.e.
17. available authorizations value edit box 18.
a few libraries implement this functionality, such as databrick 's spark deep learning phofhyperlink , however, they are implemented using `tf 1.13.1` or lower.
the circumstances required appear to be: * vim configured with: `--without-x --enable-gui=no` * `autocmd terminalopen * call term_setkill( '% ', 'hup ')` * at least two terminals open, with at least one on an inactive tab page terminals running bash the segfault is not 100% reproducible; i get it about 70% of the time.
or, is there a bug in latest version of tf docker?
we see the keys from cluster a. this means it is dangerous to work with rdm because sometimes we might think we are working with cluster b, but really we are working with a.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): package based pip installation tensorflow version (use command below): 1.12.0 python version: 3.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)"
adding does silence the error
shape inference used to work as expected in
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: none - tensorflow installed from (source or binary): `community tensorflow version (use command below): `1.13.1` python version: `3.7.2` bazel version (if compiling from source): none gcc/compiler version (if compiling from source): none cuda/cudnn version: `10.1 / 7.5` gpu model and memory: `geforce gtx 1080 ti 11gb` you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" here is the full error log, including device placements.
- have i written custom code: * yes * - os platform and distribution: *arch linux* - tensorflow installed from (source or binary): * binary * - tensorflow version (use command below): *2.0.0 beta 1* python version: 3.7.3 cuda/cudnn version: / 7.6.0 gpu model and memory: *nvidia gtx 1060, 6gb* warning produced when using `tf.linalg.expm`: warning: logging before flag parsing goes to stderr.
this is how i did it: insert following at line 345 in config = tf.configproto() = tf.optimizeroptions.on_1 run_config = tf.estimator.runconfig( session_config=config) after inserting the above, change line 350 to include run_config: estimator = tf.estimator.estimator( model_fn=model_fn, config=run_config, params=params) 3. change path /research/transformer to wherever you cloned transformer.
i create a simple generator by subclassing from tf.keras.utils.sequence: phofcode i then create a simple model phofcode i set up my training and validation generators and run fit_generator phofcode i get the following output: phofcode if change workers from 0 to 1, then `on_epoch_end` is triggered (i.e.
my tensorflow model is mobilenet.
i have followed the steps in the read me to a t..
w defaultlogger tensor datatype is determined at build time tensors not marked as input or output.
create `repro.vim`: phofcode then open vim via: phofcode then attempt completion on: phofcode then use `<c-n>` to select an item in the list of completion candidates.
if including tracebacks, please include the full traceback.
i then copy image data into the buffer for inference, which runs in an asynctask with a serial executor, so the interpreter and buffer are only used by one forward pass at time.
problem emerges when i try to run the inference with a newly taken image and the .tflite model as in associated tflite object detection example script : caused by: internal error: failed to run on given interpreter: - num_classes <= 1) was not true.node number 228 failed invoke.
link statusline errormsg ino <expr> <c-b> func() fu func() let g:on = !get(g:, 'on', 0) redraws return '' endfu fu status() return get(g:, 'on', 0) ?
the estimator function is serialized, sent to each worker and then executed to create the estimator, create the graph and start the training.
(i know this because i `tf.print` 'd everything inside of nadam optimizer.)
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the expected behaviour would be that the output number for conv2d_layers is at the same position as for usual conv2d_layers.
note: code worked fine with tensorflow 2.0 beta1.
if including tracebacks, please include the full traceback.
dataset input to estimator is broken
it should handle learning phase internally.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): none tensorflow version (use command below): 1.11 python version: 2.7.12 bazel version (if compiling from source): 0.15.0 gcc/compiler version (if compiling from source): 5.4.0 cuda/cudnn version: 9.0, 7.0.5 gpu model and memory: titanxp, 16gb processes are hanged after the log of "successfully opened cuda library libcupti.so.9.0 locally".
a clear and concise description of what the bug is.
i would expect the normalization ops to be folding into the weights, so that only one const/read node remains.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): - mobile device (e.g.
manually: * `vim -nu none --clean --no-plugin` * `set columns=73` * `set number` * `66ix<esc>` `:echo screenpos( win_getid(), 1, 66 )` expect: screen position ( line 1, col 71 or so) actual: col: 0 see this test: phofcode fails with: phofcode
the expected behavior are 1. training process gets slower by at most two-fold using this implementation 2. does not further slow down after every epoch 3. no error thrown.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): - mobile device (e.g.
each epoch takes longer and longer.
i have an input pipeline that contains tf.py_function as one of its processing steps when training a model with tpuestimator.
if you pass an iterator the performance will continue to degrade at fifth to tenth the rate.
-iproto -dhave_pathdef -dwin32 -dfeat_cscope -dfeat_terminal -dfeat_job_channel -dwinver=0x0a00 /mp -dhave_stdint_h /ox /gl -dndebug /arch:avx2 /zl /mt -ddynamic_iconv -ddynamic_gettext -dfeat_tcl -ddynamic_tcl -dfeat_lua -ddynamic_lua -dfeat_python3 -ddynamic_python3 -dmswinps -dfeat_huge /fd.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): linux redhat - mobile device (e.g.
running example app with retrained model as shown in the guide phofhyperlink
i am not quite sure which parts lead to the hanging.
anyone has any ideas to solve it ?
i 've been able to distill the issue into ~150 lines of code and package it in a colab.
but that is an ideal situation; it is understandable to some extent that in cases such as mirroredstrategy a custom callback may require extra conditions.
also, it does not end with a newline.
... steps to reproduce the behavior: 1. create request body where schema is in separate file 2. make sure that one of the property of that object is required 3. launch swagger ui
phofcode - os: ubuntu 19.10 - terminal: gui
case 2: 1. run `vim --clean -c 'set number ' tmp.vim` 2.
dataset in tf2.0 lack of key properties and methods which already in tf1.13, for example: properties: output_shapes, output_types methods: make_one_shot_iterator
every time the layers `call` method is invoked more and more memory get 's allocated.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): win 10 - mobile device (e.g.
expect `+python3` flag to be set.
the model can be saved successfully.
the one-hot columns in the dataset should not be normalized.
this shows that the issue is linked to autograph not recognizing `super()`, only `super(b, self)`: phofcode i can also work around this by calling `super()` outside of the method, e.g., in constructor: phofcode tried to work around it using a `tf.init_scope()`, but could not get it work, not sure why.
large logs and files should be attached.
conversion of tf2.0 function containing `reshape` and `one_hot` ops to tflite format fails with the following runtimeerror.
setting: `:set doesn 't make the "@" character (code 64) to be recognized by the completion (e.g.
devices: i streamexecutor device (0): <undefined>, <undefined> converted call: <function <lambda> at args: (<_variantdataset shapes: (), types: tf.int64>,) kwargs: {} not whitelisted: <method-wrapper '__call__ ' of function object at default rule not whitelisted: <function <lambda> at default rule entity <function <lambda> at is not cached for key <code object <lambda> file line 4> subkey object frozenset()) converting <function warning: logging before flag parsing goes to stderr.
hello, i am trying to parallelize a machine learning algorithm by using multiprocessing for my tensorflow session.
when using a `for` loop in a @tf.function (with autograph), such as `for i in tf.range(10)`, `i` is undefined after the loop ends, unless it is defined in the global scope, in which case it takes this value.
unnecessary access to gcp metadata endpoint, 10 retries, and lots of logging.
please see if it 's intended.
3. notice how the width changes during scrolling, right before wrapped lines appear/disappear.
let me provide an example.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below): import tensorflow as tf python version: bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
include any logs or source code that would be helpful diagnose problem.
in the tensorflow 2.0 preview, the and wrappers are incompatible with it raises an "unknown layer" exception.
test.vim phofcode 1. so % 2. scroll the mouse wheel
during my checking progress, i intentionally use the training data to evaluate the model, and i can see evaluation result is quite different from training progress shows.
my use case is running tensorflow as a c-library using go interface.
converting it to an estimator and then training it throws an exception because it assumes the input names are 'input_1 ' and 'input_2 '.
running the tflite demo phofhyperlink (the link is the head of master branch as of now) on , average inference time for 1 thread, _mobilenet v1 quant_, on cpu is
no assertion, as i computed the values by hand and expect them to be right.
this is borrowed from the stackoverflow link above: phofcode
i 've trained it and froze it (with quantization set to true) following the official tutorial phofhyperlink .
provide a reproducible test case that is the bare minimum necessary to generate the problem.
tfliteconverter converts the first convolutional layer of tensorflow model into depthwise convolutional layer if the input to the model has single channel (in my case grey scale images with channel =1).
info:tensorflow:calling info:tensorflow:done calling calling info:tensorflow:starting evaluation at traceback (most recent call last): file "train.py", line 146, in <module> main() file "train.py", line 142, main use_multi_gpu) file "train.py", line 83, train_and_evaluate train_spec, eval_spec) file line 471, train_and_evaluate return executor.run() 611, run return self.run_local() 712, run_local 358, train loss hooks, saving_listeners) 1122, _train_model return hooks, saving_listeners) 1185, _train_model_distributed input_fn, hooks, saving_listeners) 1287, saving_listeners) 1407, _, loss estimator_spec.loss]) 676, run 1171, run 1270, run raise 693, reraise raise value 1255, return self._sess.run(*args,
2. run command `let g:netrw_sizestyle = 'h'` 3. open a directory with a few files.
- debug is not at fault here, because in my project, the `index` is no longer passed onto nested loops which breaks my code.
docker image just fails on startup using example startup command from phofurl the mounted directory contains the `config.cfg` copied from the repo root.
i found that setting the same base_random_seed for tensorforest generate different classification performance.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): source tensorflow version (use command below):1.12.0 python version:3.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version:9.0 gpu model and memory: gtx1070, 8gb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)"
i 'm wondering how could this happen?
there should be no error.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below):1.8.0 java python version: 3.5.6 bazel version (if compiling from source): none gcc/compiler version (if compiling from source): none cuda/cudnn version: none gpu model and memory: none you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
this is the block of code that produces this error phofcode strangely shapes of y_pool, y_1 are correctly inferred , complete code to reproduce issue available below phofcode
phofcode the two models are combined with following code: phofcode the issue seems to occur no matter what input values are.
a clear and concise description of what you expected to happen.
however, if i perform check before model runs, mismatch is already present.
after patch phofhyperlink , `test_term_gettitle` fails when vim built with `+autoservername` feature.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
export tf_need_cuda=1 "" | ./configure bazel build --config=opt --config=cuda
one parent, and one child (in my case it was one-to-many relation) 2. create many child models (1k +) 3. make sure child is displayed for parent's edit page 4. try to edit parent entry 5. see how it stuck
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): (this is the tf 2.0-preview) python version: 3.6.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: below is the output of this program.
however, before calling included call at starting of code by mistake.
- os platform and distribution (e.g., linux ubuntu 16.04): window10 - tensorflow installed from (source or binary): source - tensorflow version (use command below): 1.13.1 - python version: 3.6 include any logs or source code that would be helpful to diagnose the problem.
import argparse import numpy as np import tensorflow as tf from import signature_constants from tag_constants from convert_to_constants def get_dataset(batch_size, input_size): features = np.random.normal( loc=112, scale=70, size=(batch_size, input_size, input_size, 3)).astype(np.float32) features = np.clip(features, 0.0, 255.0) features = "features", dtype=tf.float32, dataset = dataset dataset.repeat() return dataset def def load_model(): saved_model_loaded tf.saved_model.load( saved_model_dir, graph_func return graph_func graph_func load_model() # replace load_model function and its call with the following to make it work # saved_model_loaded tf.saved_model.load( # saved_model_dir, # graph_func frozen_func def wrap_func(*args,
phofurl the problem can be reproduced without the two lines workaround.
as far as i understand it, `sample_weight` can be used for weighting the loss function, and according to the docs phofhyperlink it can be "a flat (1d) numpy array with the same length as the input samples" - i.e.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): downloaded .aar file from phofurl - mobile device (e.g.
i receive an assertion error when creating a forward pass for an unrolled multi-layer lstm while using a mask.
`argc` and `argv` should not be highlighted as cmacroname.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
... steps to reproduce the behavior: 1. click on any route with required parameter.
i think the second evaluation should also work, because i use padding="same"
cursorline should not be displayed other than the popup window
- os platform and distribution (e.g., linux ubuntu 16.04): macos 10.14.4 - mobile device (e.g.
<nomodeline> % ' endif endfu nno <silent> <s-f18> :<c-u>call nno <silent> <s-f19> :<c-u>call augroup restore_focus_events au!
you also have icons for certain types of files(pdf, zip, jpg), i think its worth adding a default icon to other files
there are a lot of operations which are placed on gpu and they have just kernel queue/launch times recorded in timeline in compute` streams.
not sure it will be useful.
`import tensorflow as tf ` `image = 'r ')` `image.read()` my tf2.0 version below has the same problem with the beyond.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macosx 10.13.6 - mobile device (e.g.
the response headers: { "cache-control": "no-cache, no-store", "content-disposition": "attachment; filename="example.zip"", "transfer-encoding": "chunked", "content-type": } this is the download url that it displays in ui.
this would have been great, except there's a few things that are very obviously incorrect.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
navigate to phofurl ...and click on the + / - / reset buttons.
`:bn`, switched to b.txt, no cursorline
foo1 identity add_9 add_8 add_7 add_6 add_5 add_4 add_3 add_2 add_1 add x add/y add_1/y add_2/y add_3/y add_4/y add_5/y add_6/y add_7/y add_8/y add_9/y foo2 identity while/identity_2 while while/loop_counter const_1 x maximum_iterations range range/start maximum const maximum/y range/delta foo3 identity const foo4 identity while/identity_2 while while/loop_counter const_2 const maximum_iterations range range/start maximum const_1 maximum/y range/delta ``
when i run python code with real input (an image) it detects objects.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
subscribecopy phofimage - os & version: windows 10 1809 - redis-server version 3.2.100 add any other context about the problem here.
w0625 deprecation.py:323] from checkpoint_exists (from is deprecated and will be removed in a future version.
it will consume the result so it is not output to the user/terminal, but will remain `status_sent`.
however, if i multiply the `sample_weights` by 10000, the loss doesn't change.
not found: resource does not exist.
we now do an equivalent tranpose, but sandwiched by reshapes to reduce number of dims.
the following model uses a custom initializer, and a custom regularizer, and a custom constraint, it works fine, saves fine, but cannot be loaded.
... steps to reproduce the behavior: add a description to a openapi 3.0.0 response object try to view it in swagger-ui v3.23.4 or greater
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux version (gcc version 4.8.5 (red hat 4.8.5-36) (gcc) ) - mobile device (e.g.
all `list` methods should be supported for defining nested list of layers.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): arch linux - tensorflow installed from (source or binary): binary - tensorflow version (use command below): python version: 3.6.8 cuda/cudnn version: cuda version gpu model and memory: geforce gtx 1080 ti
phofcode prints phofcode when run in a graph, the output is the same.
`call popup_create("hello", {"borderchars": [ ' ', ' ', ' ', ' ', ' ', ' '], "border": [1,1,1,1])`
i say sqlite is used b/c redeploying wiped out any data, such as new users, entered in the admin then the start new screen is shown after every deploy.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary):binary(use pip) tensorflow version (use command python version:3.6.7 bazel version (if compiling from source):n/a gcc/compiler version (if compiling from source):n/a cuda/cudnn gpu model and memory:nvidia geforce rtx2080ti 11gb x2 include any logs or source code that would be helpful to diagnose the problem.
4. the json has a bug.
if including tracebacks, please include the full traceback.
when running provided code it fails only when fit_generator executed in graph mode.
probably some insufficient character escaping or missing unicode support or something
it doesn 't happen when a list of strings is passed.
*according to issue 36175: identity of bound methods - python tracker phofhyperlink this is supposed to be expected behavior in python.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
minimal model: phofcode * steps when `save_model()` is used: phofcode * or steps when is used: phofcode
when i use tf.gradienttape to compute gradients, it will use all gpu memory even though i use gpuoption in session.
input_tensor_pairs, {output_tensor_name}, {}, &output, nullptr);`
detailed steps to reproduce the behavior: 1. run `vim --clean` (or `gvim --clean`, etc.)
- os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 lts - mobile device (e.g.
steps followed for inference on my trained model: 1. method #1: freeze graph *.pb file using modified [freeze_graph.py][1] and restore it.
- vim version: it looks like the rule is translated as ` ..*` which matches anything that
`:set ttymouse=sgr` 4. click the test winbar menu
`vim --version`: phofcode - os: ubuntu 18.04.2 - terminal: bash
inference should not crash, as it does not in other ios devices
it's as if the `random_shuffle` is ignoring the random seed.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): this is a custom written yolov3 model written primarily with the tf.layers api which is soon to deprecated.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): binary tensorflow version (use command below): tf version = tf git version = python version: 3.6.4 bazel version (if compiling from source): na gcc/compiler (if compiling from source): na cuda/cudnn version: na gpu model and memory: here is the stack trace... warning: logging before flag parsing goes to stderr.
i want to see gpu0 and gpu 1 when run the device list command
provide a reproducible test case that is the bare minimum necessary to generate the problem.
`tf.keras.model.fit` should apply `class_weight` when passed a `tf.data.dataset`.
train model distributed without error.
firstly, this will not generate cache hits if the function is called twice with same parameter values, because object ids will not be same even though their values are.
1. build a multipart form manually with some binary file, without the `content-type` part header 2. post to /upload
steps to reproduce the behavior: 1. run algo installation script 2. dnscrypt-proxy works as expected.
5. observe that `[modified value]` is now displayed in examples dropdown.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: / - tensorflow installed from (source or binary): _source_ tensorflow version (use command below): 1.12.0_ python version: _3.6.7_ bazel version (if compiling from source): _0.18.1_ gcc/compiler version (if compiling from source): _4.8_ cuda/cudnn version: _10.0, 7.3.1_ cpu model: _intel(r) xeon(r) bronze 3106 cpu @ 1.70ghz_ gpu model and memory: _nvidia geforce gtx 1080 ti 11gb_ download phofimage this figure shows the average execution time of _conv3d_ and _conv3d_transpose_ operations on both cpu on gpu with a log scale for both x and y axis.
this is just a sample.
the window is `26` lines high, because of the autocmd running `wincmd _` on `winenter`.
using the keras model subclassing api, dense layer cannot be instantiated at any point in `__init__` after instantiation of conv2d layer.
the title of the preview popup window exceeds the popup window width for long file names.
gist link 1 phofhyperlink gist link 2 phofhyperlink i am trying to use tfe.pyfunc to combine eager block in the operation graph (see link 1).
not sure why adding sendrecvops causes data corruption.
it should work, and run faster than eager mode.
running docker and answering steps to create on google compute engine.
phofcode where at somepoint in `preprocess_fn` `np.rollaxis` is called, which in turns calls `np.transpose`.
]], dtype=float32)>, 1 <tf.tensor: id=108, shape=(2, 12), dtype=float32, numpy= array([[1., 1.
detailed steps to reproduce the behavior: 1. use the popup menu(open with vim tab or edit with vimvim) for the right mouse button to open 2. :set tags 3. ctrl + ] in function start_armboot to jump to definition in line 333 4. after 3s it would jump definitionit takes too long if run gvim.exe with left mouse button then drag the start.s gvimthen do process 2 it would jump definition very fast whats difference between right mouse button and left mouse button
i use tf.dataset.cache to cache data and it will run out of my computer 's memory
consider the dummy training code below.
the podfile would need be modified locate your own build cocoapods because tensorflowlitec pod uses framework in question and it's not yet publicly available
w0624 deprecation.py:506] from calling (from with constraint is deprecated and will be removed in a future version.
running the tflite demo phofhyperlink , average inference time for 1 thread is
- - os: nixos - terminal: xterm, terminolog
add database connections to one database per environment.
expected to fit the model without errors.
the default is used for multi gpu training.
import tensorflow as tf testing_tensor = testing_output = testing_tensor - what_grad_auto = image phofimage
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - tensorflow version (use command below): 1.12.0 and 0.12 - python version: 3.5 (for tf1.12.0) and 2.7 (for tf0.12)
"e730: using list as a string" when using `popup_setoptions()`
... steps to reproduce the behavior: run chrome accessibility assessment or siteimprove extension.
if put lstm/kernel to black list, the pd file can be load later on, however, cannot be used in c++ and c #.
if including tracebacks, please include the full traceback.
devices: i streamexecutor device (0): host, default version initialize grpcchannelcache for job worker -> {0 -> localhost:65535, 1 -> localhost:65532} started server with target: grpc://localhost:65535 is not passed in.
enter some text and select to the end of the line: gvim1 phofimage after inserting a tab at the start of the line (note eol marker is not selected): gvim2 phofimage turning on line numbers makes it worse: gvim3 phofimage - vim version is - os: windows 8.1 - terminal: gui selecting downward with mouse selects whole line.
when i tried the following code from the sample: phofcode it failed with error: phofcode according to the change of numpy in phofurl the default value of allow_pickle was changed from true to false.
if so, would it be a better idea to have it noted in guide?
1. clone strapi 2. run yarn setup
provide a reproducible test case that is the bare minimum necessary to generate the problem.
in order to do this i am using the below script : phofcode as an example, i am using ssd mobilenet architecture.
false are weights empty when resetting model?
if using the code expects non keras optimizer, if using "standard" behavior (compiled graph) the code runs fine.
... steps to reproduce the behavior: 1. go to any post operation 2. click on try it out 3. type in the request body field 4. the ui will hang for a while for each keystroke, the more keystrokes the longer
when using non-ascii character in the function with @tf.function, autograph failed.
- have i written custom code: no - os platform and distribution: android - mobile device: google pixel, google pixel 2 and xiaomi 6 - tensorflow installed from: binary `implementation tensorflow version: 1.13.0 python version: 3.6 cuda/cudnn version: gpu model and memory:
uncommented the line aboveand comment the one above it 1) run npm run lint from the command line 2) edit the app.svelte and save file
for the minimal example script provided below, here is the output: fwd:: non_stateful: [ 1.
3. narrator (screen reader) should read reference of column header (i.e.
and when replace default all reduce alg with `nccl/xring` , distributed training run 120 steps, but tensorflow hang all the time.
t0 time.time() psi_np np.transpose(psi_np, (*list(range(2,n)), 0, 1)).copy() print(time.time() - t0) around 0.07s on my system psi_tf @tf.contrib.eager.defun make this a graph, just in case def f(p): return tf.transpose(p, (*list(range(2,n)), 0, 1)) for _ range(5): t0 time.time() psi_tf f(psi_tf) print(time.time() - t0) around 0.5s on my system (almost 10 times slower!)
reproduce with `python3 issue.py` issue.py.zip phofhyperlink issue.py is the following (also zipped above): import sys import tensorflow as tf import numpy as np from import compile hidden_size = 2048 filter_size = 8196 num_heads = 32 d_k = hidden_size // num_heads d_k_root d_k
when using `@tf.function` a function with a ragged input it will return the dense equivalent of the ragged tensor.
steps to reproduce the behavior: 1. install wireguard (unreleased) app 2. scan the qr code 3. tab connect
the latest vim has updated to a newer version of netrw, which seems to have broken the `gx` functionality.
using `defun` in eager mode, this very simple function seems to cause havoc.
the window width depends on the number of characters in `showbreak`.
1. run `vim --clean popup.vim -c "source %"` phofcode 2. scroll the popup window with `j/k`, close with `q`.
to measure inference time i decided to call the object manually by supplying (256, 256, 3) image as input to the overridden call() method.
(according to tf r1.13) phofcode output: `<dtype: 'float32_ref'>`
as can be seen, 5 `tf.graph` instances remain in memory, along with all `tensor`, `tensorshape`, `dimension`, and other objects that go along with it.
to be able to save a model on ai platform.
when connected via openvpn it works without issue, but with algovpn it for some reason is not passing the traffic correctly.
(and i confess that even * * is quite bit slower than expected.)
t w internal: [_derived_]inconsistent [64].
commit phofurl regressed this order, and also removed test case that was intended to catch regressions.
i want to change the input tensor size freely
but seems that more complex models takes for `evaluate` increases at a higher rate each gets called
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): amazon linux - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): android 9 - mobile device (e.g.
the following error appears: toco failed.
however, whenever the total feed size goes above 2gib, i get the following crash inside libtensorflow: phofcode
- install phofurl - execute the example about web camera: `python .
the output of `tf.nn.conv2d` should not be different w.r.t batch sizes.
only the first line is highlighted with cursorline.
in all cases, the dll of custom operator generated with provided import library in tf 1.8 depends on whereas dll generated with my own import library for tf 1.12 depends on which is a non existing file.
- os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0.0-alpha0 - python version: python 3.6 cuda/cudnn version: 10.1 gpu model and memory: titan x
regardless of tfl converter optimization, prediction on
` 'list '` is not remembered in the first command (unexpected): gif1 phofimage ` 'list '` is remembered in the second command (expected): gif2 phofimage i know that `:h local-options` says: > when editing a buffer that has been edited before, the options from the window > that was last
this problem seems very similar to #24622, where the same issue was reported for sequential objects.
`export 2. start vim `vim --clean`.
now, i 'm trying to deploy this model on the edge by converting the saved model (.h5 file) to a .tflite file.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary, from intel's build: phofurl tensorflow version (use command below): 1.11 python version: observed in 3.5 and 3.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: n/a include any logs or source code that would be helpful to diagnose the problem.
i am trying to use tf.summary at the same time as using distribute strategies.
if applicable, add screenshots to help explain your problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): use default tensorflow on colab ( 1.12.0 ) i installed 1.12.0 binary for my mac with pip install tensorflow tensorflow version (use command below): 1.12.0 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: using cpu on mac gpu model and memory: gpu on colab.
1. create a new mongo cluster 2. install a new strapi application with a mongodb database.
expected behavior is that the debugger does not crash and i can examine the tensors.
i 'm using keras for the cnn model.
during the train of the xgboosted tree, the kernel will abort and halt.
- vim version - os: all
large logs and files should be attached.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
traceback (most recent call last): file line 3296, in run_code exec(code_obj, self.user_global_ns, self.user_ns) file line 41, in <module> comparison = file line 313, predict_classes proba = self.predict(x, batch_size=batch_size, verbose=verbose) file line 1122, predict batch_size = steps, x) 1780, raise valueerror( 'the `batch_size` argument must not be specified when ' valueerror: the `batch_size` argument must not be specified when using dataset as an input.
the "true" label is `price < price_cutoff`, with `price_cutoff` set so that the resulting label mean is `0.0146`, making it significantly unbalanced.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
creating a dataset using tensorflow_dataset and passing it to a function decorated with `@tf.function` gives the error: phofcode
i found this behavior nowhere documented.
`model.fit` should train the model, and model weights should be saved in desired files.
this only happens with some cursor positions, not all.
this code calculates a matrix exponential using a taylor series.
float[][] outputvector; int[] intvalues; static ppmitem ppm_tfrun = "runalex", string.class); bytebuffer imgdata; interpreter tflite; /
i am going through francois chollet 's book "deep learning with python" and running the code in his jupyter notebooks in tensorflow 2.0.0 adapting the code to "import tensorflow.keras" instead of "import keras".
if is case, you should define them in a .py file.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): win 10 - mobile device (e.g.
@summary.py:93] summarizing collection 'summaries ' size 78.
documentation in cloud-amazon-ec2.md section set-up-an-aws-user phofhyperlink suggests it is possible to use a mfa enabled iam user.
- have i written custom code: tflite converter code is straight from an example script - os platform and distribution: mac os 10.14.5 - tensorflow installed from: `pip install tf-nightly` and `pip install tensorflow` - tensorflow version: tested on and 1.13.1` python version: 3.6.5
large logs and files should be attached.
large logs and files should be attached.
first 43 operations will run on the gpu, and the remaining 7 on the cpu.tflitegpudelegate prepare: tensor ref has unsupported number of dimensions: 5node number 50 (tflitegpudelegate) failed to prepare.
running the following at the end of the notebook will reproduce the error.
my expectation was that if i pass a tf.variable as the learning_rate argument to and later assign a new value to the variable, that would affect the optimization.
in case i set shell to `bash.exe` no commands are found and the terminal shows a message "bash: cut: command not found" when starting.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): nvidia container based on tf 1.12 tensorflow version (use command below): 1.12 python version: 3.5.2 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: v100 32gb you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
large logs and files should be attached.
in my opinion this is a huge waste of resources, especially when image logging is being done and transformations are required.
# try d=5, n=9 vs d=7, n=8.
example: mobilenet-unet (converted from keras(h5) to tflite) average model runtime of mobilenet-unet(poco): 60ms average model runtime of mobilenet-unet(honor): 180ms simple - unet (converted from tensorflow(pb) to tflite) average model runtime of unet(poco): 50ms average of unet(honor): 60ms
... steps to reproduce the behavior: 1. point swagger-ui at an api spec that uses a path-relative url for the server, not a host-relative one 2. try to use swagger-ui 3. observe it sends requests to incorrect urls
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: _several, e.g.
2. the gpu running result on tx2 should be same as cpu running result on tx2.
- os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - tensorflow installed from (source or binary): virtual env, pip - tensorflow version (use command below):
if should actually build either completely custom dataset or custom operator that could take stream of examples and build the batches with some sort of internal buffer let me know.
it isn 't just hypothetical.
setup according to phofurl phofcode
1. go to admin panel 2. click on roles & permissions 3. select a particular role to assign permission 4. you will get unexpected error happened i.e 500
w0624 deprecation.py:506] from calling (from with constraint is deprecated and will be removed in a future version.
however, now, thanks `win_execute()`, `:resize` can run in context of a * non * -focused window which * * squashed; so `0` argument does make sense, and `:resize` should not alter when it 's run in context a non-focused window.
i followed nmt_with_attention.ipynb phofhyperlink on tensorflow tutorial.
i should be able to run python code on the cpu as part of my input pipeline when training on tpus.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): arch linux - tensorflow installed from (source or binary): pip - tensorflow version (use command below): | | python version: 3.6.8 cuda/cudnn version: 10.0, gpu model and memory: geforce rtx 2018 ti 11gb phofcode
the evaluate will happen after every checkpoint.
1. set `.vimrc` with the following contents: phofcode 2. leave `~/.vim/` folder empty; 3. edit any file long enough to allow for multiple full screen scrolls;
no real difference in was seen.
do you wish build tensorflow with rocm support?
this was working correctly in the tf2.0 alpha release but is an issue in the tf2.0 beta1 release.
even for the single thread case.
more specifically, be able to hyper-parameter tuning without restarting the jupyter kernel.
that is, if my server is ` phofurl the authorization url should be ` phofurl
n/a phofcode - os: ubuntu 18.04.2 lts - terminal: xterm(330) n/
` code that errors: aasset *asset = aassetmanager_open(mgr, tflite_path.c_str(), o_rdonly); off_t start = 0; off_t length = aasset_getlength(asset); int mmap_fd_ = &start, &length); if (mmap_fd_ < 0) { loge("failed to open file: %s", tflite_path.c_str()); aasset_close(asset); return false; } // mmap tflite binary struct stat sb; fstat(mmap_fd_, &sb); char *mmapped_buffer_ (char *) mmap(nullptr, sb.st_size, prot_read, map_shared, mmap_fd_, 0); if (mmapped_buffer_ == map_failed) { loge("mmap failed on file: %s", tflite_path.c_str()); aasset_close(asset); return false; } // build the model using namespace tflite; model if (!model) { loge("failed to create flatbuffermodel with file: %s", tflite_path.c_str()); // my code errors here.
masking should be supported: phofurl if a layer supports masking, it should be a kwarg to __call__.
when i try to run the .fit function on the model, i get an error: nodedef mentions attr 'explicit_paddings ' not in op<name=conv2d; signature=input:t, filter:t -> output:t; dt_bfloat16, dt_float, dt_double]; attr=strides:list(int); "valid"]; "nchw"]; 1, 1, 1]>; nodedef: {{node conv2d/conv2d}}.
during the converting the quantize aware trained mobilentnetv2 model from tf.keras, it will raise the follow error massage phofcode
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): tf 1.12, python version: 3.6.7 cuda/cudnn version: not applicable gpu model and memory: not applicable include any logs or source code that would be helpful to diagnose the problem.
phofcode typeerror traceback (most recent call last) in <module> ----> 1 m.fit(tf.constant(0), tf.constant(1)) in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing,
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - tensorflow installed from (source or binary): - tensorflow version (use command below): phofcode - python version: 3.7.1 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: not relevent gpu model and memory: not relevent include any logs or source code that would be helpful to diagnose the problem.
that is, if i swap in "for i in range(1):" rather than "for i in range(3):" above, the code executes cleanly.
(note that i use multiple prefetch statements; this significantly improves performance on both systems for me, but removing them does not change their relative performance.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): window 10 - mobile device (e.g.
in a popup window with filetype `diff`, adding a text property with a highlight breaks the diff syntax highlighting from the end of the text property to the end of line.
phofcode you can see there is bytes of memory leak occurred.
this should probably raise an exception.
a code that loops using tensorflow primitives (like `for i in tf.range(10)`) can be converted into its graph representation without any problem, while a code that creates a python iterator from a dataset object (using `iter(dataset)`) can't.
my model 's accuracy and loss are evaluating to 0.
should eager context behave similarly?
an error is thrown when saving a keras model that has been compiled in using a mirrored strategy using more than one gpu.
large logs and files should be attached.
i have built a tensorflow keras sequential model (see predrnn.py phofhyperlink ) which consists of several keras layers including my custom layer (see phofhyperlink ).
1. run vim with vim-airline plugin and edit a help filetype.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
bias->type != input_type (10 != 1)node number 3 (conv_2d) failed to prepare.
1. the graph that is created by estimator extremely large (graph.pbtxt 2.5gb)
11. table content announced accurately and in structured manner for benefits screen reader users irrespective screen reader used.
large logs and files should be attached.
iterating a shuffled dataset returns elements in the same order each time it is iterated over, and the order is unaffected by the random seed.
case 1: phofcode error: phofcode case 2: phofcode error: phofcode
but could you find something in the traceback?~~ see below please.
something like: phofcode this currently throws the following error: phofcode
disabling the toolbar should not result in such a huge start up delay.
here is the log of errors i see when running above code.
we are trying to train a bert model on google colab in keras with tensorflow hub.
large logs and files should be attached.
there is a sudden increase execution time of _conv3d_transpose_ operation on gpu which goes from
this make this function unusable and useless.
granted it's not a great gpu, but i'd expect it to be substantially faster than the cpu.
i think rewrapping autograph wrapper line inside of does not wrap autograph's `converted_call`
keras layers should use sensible default behavior and not have this smearing issue.
the code to be converted correctly
when calling setreg under specific conditions, gvim crashes with the following output to stderr: phofcode
it should simply update the variable and should output the following in a deterministic manner: [array([19., 19., 19., 19., 19., 19.
also, the algorithm should avoid stack overflow if initial value has a long dependency chain
hence, specific combination of input_signature and multiple gpus causes problem (which i need for performance reasons in my work).
the official tensorflow lite segmentation model ' phofhyperlink ' cannot be run on andorid 9.0, using nnapi.
-iproto -dhave_pathdef -dwin32 -dfeat_cscope -dfeat_terminal -dfeat_netbeans_intg -dfeat_job_channel -dfeat_xpm_w32 -dwinver=0x0501 /mp -dhave_stdint_h /ox /gl -dndebug /zl /mt -dfeat_ole -dfeat_mbyte_ime -ddynamic_ime -dglobal_ime -dfeat_gui_mswin -dfeat_directx -ddynamic_directx -ddynamic_iconv -ddynamic_gettext -dfeat_tcl -ddynamic_tcl -dfeat_lua -ddynamic_lua -dfeat_python -ddynamic_python -dfeat_python3 -ddynamic_python3 -dfeat_perl -dperl_implicit_context -dperl_implicit_sys -ddynamic_perl -dfeat_big /zi linking: link /nologo /opt:ref /ltcg:status oldnames.lib kernel32.lib advapi32.lib shell32.lib gdi32.lib comdlg32.lib ole32.lib netapi32.lib uuid.lib /machine:i386 gdi32.lib version.lib winspool.lib comctl32.lib advapi32.lib shell32.lib netapi32.lib /machine:i386 /nodefaultlib libcmt.lib oleaut32.lib user32.lib /nodefaultlib:lua53.lib wsock32.lib /pdb:gvim.pdb -debug - os: windows 7 - terminal: gui none.
errors should also ideally be added to the console log.
`model.evaluate` should not raise this error after training.
launched from docker trailofbits: starting app ... phofurl waiting for server to be up... waiting for server to be up... play [ask user for the input]
i have provided a modified version of classifying structured data phofhyperlink which demonstrates this.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
gcc/compiler version (if compiling from source): 5.4.0 cuda/cudnn version: cuda cudnn 7.4.1.5 gpu model and memory: titan v 10956 mb
please instead explicitly shut down your bazel server using command "bazel shutdown".
it does seem to change the issue it still occurs.
save an image to the content type check db and see complete url.
error encountered while trying to run ./algo
gpu device creation fails when the cuda malloc allocator is selected, with the error "no allocator statistics".
detailed steps to reproduce the behavior: 1. run `vim --clean filename.rst` 2. set configuration: phofcode 3. write the above shown comment (longer than ``textwidth``)
cursor should go to the signature line of `void qux()`.
i don 't have a minimal reproduction for this unfortunately.
when attempting to place layers after this, it returns two placeholders, despite the output shape only defining one.
- have i written custom code: yes - os platform and distribution: ubuntu 18.04 - tensorflow installed from: binary - tensorflow version: 1.14.0rc0 python version: 3.6.6 this specifically breaks codes that use the v1 properties `output_*` within the `reduce_func` function
this were run on the official tensorflow docker images.
to track down the root cause of this behavior, i have used the `objgraph` phofhyperlink dependency together with the `iris premade estimator` phofhyperlink example.
using should destroy the graph/session when either the max steps has been reached, or the input_fn raises an `outofrangeerror`
provide a reproducible test case that is the bare minimum necessary to generate the problem.
good luck to you thanks
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): unknown, probably binary tensorflow version (use command below): 1.10.0 python version: 2.7.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: na gpu model and memory: na you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" the input data is in sparse tensor of tf.int64.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (pip install tensorflow-gpu): tensorflow version (1.12.0): python version:3.6.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: include any logs or source code that would be helpful to diagnose the problem.
of note, this deadlock only happens if there is currently an op running on tpu device (via xrtexecute, but potentially others).
training proceed normally when the `class_weight` parameter is included without running out of memory.
using preserver_aspect_ratio flag generates deprecation warnings.
i ran algo prior to this but targeting digitalocean with success, however the same image + params fail for gce.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): here is a sample output testing different configurations for the matmul inputs sparsity and data types.
on my tesla v100 for tensorflow 1.8.0-devel-gpu-py3 i got around with tensorflow 1.9.0-devel-gpu-py3 a maximum at with the current 1.12.0-devel-gpu-py3 a maximum at
the expected behavior is that there is no memory leak even without
phofcode can run here: seed in dataset#map.ipynb phofhyperlink
the expected behavior is a successful epoch ending.
when i want to have a look at the weights of conv2d filters in tensorboard, only their biases get logged (see attached image).
3. installed with `left click` -> `visual select` but actually is `select` 1 and 2 are really annoying but not happen every time.
when that is not true, any nodes whose depth is equal to or greater than the inputs does not get evaluated, producing an exception.
the popup-filter function `popup_filter_menu()` and the option `#{close: 'button '}` do not work together.
2. if both mirroredstrategy and are used, one step is by passing when input checkpoint restored.
i will try to reproduce this issue with the next nightly package
no exception, no need to resize the tensor to a 0-dimensional array.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
detailed steps to reproduce the behavior: 1. run `vim --clean` (or `gvim --clean`, etc.
the compiled version prediction of a single image should be as fast as a non-compiled prediction.
phofcode if you changed from activation() to relu, it failed to serialize
- os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
and if an error would occur, it should be logged without me having to print it myself.
when eager execution is enabled, gpu resources are consumed significantly with only single calculation.
here is an example which reproduces this error: phofcode
the problem remains the same for other values of weight and thus in general violates commutativity of gradient but case of a weight of zero illustrates problem best because zero weighted summand really should not have any influence on gradients.
python scripts.zip phofhyperlink the data is from kaggle dogs-vs-cats.zip (file too big to attach: extract all to same subdirectory) the crash seems occur most often during the 100 epochs (2nd) fit.
this only occurs when using resourcevariables (not refvariables).
run the following code multiple times and have a look at the tensorboard results by running `tensorboard --logdir=test_outputs`.
i called the script using: phofcode
i should be able to save the file.
w the tensorflow library wasn 't compiled to use sse4.2 instructions, but these are available on your machine could speed up cpu computations.
111 phofimage - vim version : - os: windows 7
if run_eagerly parameter will not be set explicitly to `true` before calling model.fit function, the modell will take this phofhyperlink path instead, which will make it impossible to step into custom loss function nor custom metric function.
when a masked input is fed to a the mask is ignored
- colab notebook phofhyperlink with one-hot encoding.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): centos 7 - mobile device (e.g.
phofcode specifically with the first approach i have phofcode however for the second, i have following; phofcode the first call to self.bottleneck worked but not second one suggesting that we need to instantiate a new custom bottleneck layer and cannot reuse self.bottleneck.
little to no deprecated warning.
then i compute the gradients and loss.
large logs and files should be attached.
doing this the predictions sometimes (always?)
- have i written custom code: yes - os platform and distribution: macos 10.14 - tensorflow installed from (source or binary): pip - tensorflow version (use command below): broken with 1.13.1, works with 1.12.0 python version: 3.6.3
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary by pip tensorflow version (use command below): 1.11 python version: 2.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 9.0.176 gpu model and memory: tesla v100, 32g you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
40.2kib in use in bin.
the difference on gpu is much bigger than zero.
name: noop method name is: # <= empty method name the given savedmodel signaturedef contains following input(s): inputs[ 'input_1 '] tensor_info: dtype: dt_float shape: (-1, 3) name: the given savedmodel signaturedef output(s): outputs[ 'output_1 '] tensor_info: dtype: dt_float shape: (-1, 1) name: method name is: # <= empty, should be ``
my issue regards a performance degradation induced by enabling eager execution, in a context when no eager tensor should be created, apart from the model 's weights (to which i do not need access).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04.5 lts - mobile device (e.g.
references to members of tensorflow can be found.
1. go to graphql playground 2. execute simple query 3. the error will come
instructions for updating: colocations handled automatically by placer.
if there was no ":substitute" done since starting vim, `:help expr-!~?` and `:help expr-=~?` do not work.
is expected to work fine in both cases.
info:tensorflow:start train and evaluate loop.
also tried tcmalloc and found the same issues: $$$ heapcheck=normal ./bin/main.out phofcode i compiled the tensorflow code and got the .so file using the following commands: $$ git clone phofurl $$ git checkout r1.12 $$ ./configure warning: an illegal reflective access operation has occurred warning: illegal reflective access by to field java.lang.string.value warning: please consider reporting this to maintainers of warning: use --illegal-access=warn to enable warnings of further illegal reflective access operations all illegal access operations will be denied in a future release --batch mode is deprecated.
phofcode the error that i get from running the above code is as follows: traceback (most recent call last): file "example_error_file.py", line 44, in <module> kmeans_out = n_init=5)(conv_1) file line 634, in __call__ outputs = call_fn(inputs, *args,
--template gvim.desktop.in -o gvim.desktop msgfmt --desktop -d .
- model: class def __init__(self, conv_filters, pool_size,
first changes computed variables to constants, then converts readvariableops to identity node.
i was expecting the keras training logging to be printed post model.fit().
since the `nontext` highlight group usually does not have a `termbg` or `guibg` specified, the 'showbreak ' character should use the popup window background colour.
if including tracebacks, please include full traceback.
on my machine, having `set guioptions-=t` in my `.vimrc` causes start up to hang for 25 seconds.
furthermore, error is also eliminated when using tf.concat, so following code also executes cleanly.
the steps /epochs counters should be restored to allow continuation of training.
5. expect `add item` button to re-appear as array is no longer full, but it does not.
now imagine you instantiate a dense layer `dense` (w/ no bias) and initialize two independent mylayer instances `dense1` and `dense2` using `dense`... now, we extend the model class to take two mylayer instances on init.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): pip tensorflow version (use command below): 1.10 python version: 3.6.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: 9.0/7.2.1 gpu model and memory: 3x nvidia rtx 2080 ti you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" .
- os & version: [e.g.
instead, should check the first non-`identity` node when determining whether the last node is a softmax node.
all postgres (can be on same db-instance).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os x 10.13.6 - mobile device (e.g.
there should be no error.
108, 56) should always give an output of 108 by 56
if it's always been like that, then think perhaps it's best left as-is to avoid breaking plugins and other people's expectations.
misses to flatten some atrous_convs.
why does it only affect `bufread` events?
- os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - tensorflow installed from (source or binary): conda - tensorflow version (use command below): 1.10.1 - python version:python 3.6.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): gcc 7.2.0 cuda/cudnn version: cuda compilation tools, release 9.0, v9.0.176 gpu model and memory: rtx 2080ti it seems that there is a version mismatching between the cuda and the rtx support, but i am not sure
2. run ./algo 3. follow prompts.
"active" : "not active"} \' endfu fu setstlflag(nr) return get(extend(w:, { 'is_active ': (winnr() == a:nr)}), ' ', ' ') endfu or even simpler: set stl=%!getstl() fu getstl() return g:statusline_winid == win_getid() ?
detailed steps to reproduce the behavior: 1. install vim-fugitive (this commit phofurl (meanwhile there was workaround introduced which fails on paths with spaces) 2. install vim-dispatch plugin ( phofurl 3. run `vim --clean` 4. add vim-fugitive and vim-dispatch `:set packpath=~/vimfiles`, `:packadd vim-fugitive`, `:packadd vim-dispatch` 5. open directory with pushable git repo make changes and open `:gstatus` window 6. stage and commit changes 7. run `gpush`
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): centos 7 - mobile device (e.g.
if including tracebacks, please include full traceback.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - linux ubuntu 18.04: - mobile device (e.g.
different behavior while using tfbdg responding to catching outofrangeerror required for iteratively looping on dataset iterators from the tf.data api.
1. run `vim --clean` 2. type `:terminal` 3. type `printf " 033]52;;$(printf "%s" "blabla" | base64) a"` 4. observe that the text "blabla" is _not_ in the system clipboard.
i'm expecting the iterator release the memory each time it reinitialized.
so doubt there may be a bug in tensorflow's implementation of dilated convolution with data format nhwc.
the following `feature_column` creates the issue when subclassing `keras.models.model`, which works alright with `sequential` api.
when using supersimple network consisting of nhwc input, single tf.layers.conv2d() layer (constant initialized) and reduce_sum output, output for c=3 is the same for cpu (host tf and tflite and android tflite) and android gpudelegate are kind of the same (less than 2% difference).
windows 10 is shipped with a `scp` by default, and this version of scp is ready to parse the standard windows file path format.
either handle the case where is/contains `none`, or explicitly throw `valueerror` if it shouldn 't be allowed.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes.
in plots below all is same apart from learn rates which are 0.001 and 0.1 my question is if this a bug or this behavior expected and if so what underlying mechanism driving this?
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: not sure.
the tensorflow.keras function `load_weights` to have the same syntax as keras's.
importing tensorflow should not print errors about hadoop.
w0219 training_utils.py:1249] expected a shuffled dataset but input dataset `x` is not shuffled.
training vgg gives good enough result; however, training yolo gives poor outcome.
1. go to '...' 2. click on '....' 3. scroll down to '....' 4. see error
provide a reproducible test case that is the bare minimum necessary to generate the problem.
), but certainly, crashing (and actually spinning at 100% cpu until killed on macos) is not expected.
after running the following code it throws assertion error.
code and log: phofurl i tested with this machine on gce: * n1-standard-4 * 1 x nvidia tesla v100 * image: (google deep learning vm, common-dl-gpu) * installed anaconda3, cudnn 7.6.4.38 (got some errors with the original 7.4 cudnn in the image), tensorflow-gpu 2.0
succesful training with distribute strategy when tf.py_function called in dataset.map.
using tf.summary.scalar in keras loss/metrics function, then use model_to_estimator to convert keras model to estimator, tensorboard get duplicate scalars with totally same value.
i connect to the vpn.
this is a standard do deployment from macos.
if including tracebacks, please include the full traceback.
a clear and concise description of what the bug is.
i expect the function to be called concurrently by pair: phofcode
provide a reproducible test case that is the bare minimum necessary to generate the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
please find all the code i used in the notebooks that i have put in this dedicated repository phofhyperlink .
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): tensorflow-gpu-1.13.1 python version: 3.6.4 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: cuda 10.1/cudnn7.5 gpu model and memory: 930mx 2gb phofcode
it reproduces at an epoch numeber bigger than 150( sometimes is around 200, sometimes 500)
tf.name_scope does not effect the name of weights created by keras.layers output in tf 2.0.0-alpha0 phofcode
trace shows complete function names (at least) and signatures (optional) or even better: layers to which those activities/events belong.
by monitoring `nvidia-smi`, part of the execution seems to run sequentially on each gpu.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac - mobile device (e.g.
when i tested the same code on the terminal, it works.
was unable to figure out what it does to increase required memory.
why do we observe 2-3x higher latency ?
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): linux fedora 26 - mobile device (e.g.
files creaded by opening a terminsl, `cd`-ing into and `touch`-ing `file2.txt` don 't have this problem.
model fails to train, raising a `runtimeerror: replica-local variables may only be assigned in a replica context.` i was able to reproduce this issue just by using the official tutorial, so that 's the code given below rather than mine.
operations in compat.v1 for rnns for tf2.0 should be supported for eager execution.
if including tracebacks, please include the full traceback.
the easiest way to reproduce the problem is using the official notebook : phofurl and re-compiling and fitting a second time phofcode
during training it has 2 inputs - one is actual input and second one is ground truth mask, i am applying using `lambda` layer (see code example).
import tensorflow as tf import numpy as np import string import random from index_table_from_file from tensorflow.python debug as tf_debug lookup_table_filename = "./lookup_table.csv" data_filename = def generate_data(): """creates the required datafiles""" letters = [i for i in string.ascii_letters] vocab = set([ i + i + j + j + k k l l for i in letters for j in letters for k in letters l ]) positive_vocab set(random.sample(vocab, with 'w ') as f: f.write( ' f.write( ' ') f.write( ' '.join(map(str, range(10
if including tracebacks, please include the full traceback.
i 'm trying to run crossshardoptimizer with tf.optimizers.adam, however, tf.optimizer.adam is updated to v2 (with no global_step_parameter), and crossshardoptimizer is still in version 1 and still have that global_step parameter.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.14.0-rc0 python version: 3.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory:
ecc fan temp perf pwr:usage/cap| memory-usage gpu-util compute m. 0 geforce gtx 106... off on n/a 3% 49c p8 9w / 120w 275mib / 6070mib 0% default processes: gpu memory gpu pid type process name usage 0 1007 g /usr/lib/xorg/xorg 152mib 0 1485 g /usr/bin/kwin_x11 36mib 0 1489 g /usr/bin/krunner 1mib 1491 g /usr/bin/plasmashell 30mib 2729 51mib ``
phofcode fails with error phofcode this is because the lines: phofurl phofcode make a copy of the tuple instead of converting it to a list and later the lines: phofurl phofcode do not take care of the case when axis is a tuple.
the model loads and runs fine if with cpu, i.e.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): use plugin - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
for example: when i run following command (after completing rest of requisite steps): _python3 transformer_main.py --data_dir=$data_dir --model_dir=$model_dir --vocab_file=$vocab_file --param_set=$param_set i get error ( see attached/below for traceback)
... 1. open the swagger-ui page.
after hitting `s` for 2-3 times, file listing gets corrupted and it's no longer usable.
upgrading from tensorflow 1.12.0 to 1.13.1 shows a large performance regression.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): - mobile device (e.g.
i have tweaked his implementation for psenet ran into same problems with `tf.estimator`, so it seems that `tf.estimator` is indeed culprit.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04):mac - mobile device (e.g.
phofcode this example demonstrates passing a numpy array does not have the same issue.
file `foo.py`: phofcode command that produces the error: `tf_upgrade_v2 --infile foo.py --outfile foo_tf20.py`
the bug seems to appear from 1.10 version of tensorflow onwards e.g.
i wrote a toy example to reproduce the issue, it might be clearer than the description above.
it seams that either `tf.data` or `tf.keras.metrics` have a memory leak that starts showing up after several epochs of training and evaluation.
large logs and files should be attached.
phofurl i don 't know why there is not "-0.5f" like vanilla halfpixelscaler.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:iphone x - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: 3.7.3 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: na gpu model and memory: na
quantized is 4x slower than un-qualtized version (both testing on tflite)
w0611 23064 ag_logging.py:145] entity <bound method fe.call of <__main__.fe object at could not be transformed and will be executed as-is.
now, i send the froze .pb file to tensorflow lite.
ubuntu 18.04, windows 10 1809, macos 10.14] - terminal: [e.g.
model fits without any problems, like the same model without embedding layer (no _dest_input_).
precisely, i get a slowdown from 0.13s/it to 7.7s/it on gpu and 2s/it to 9s/it on cpu.
it defeats the purpose of it when i can combine an if statement with to produce better results.
so `for x in dataset` would be fine, but `for x tqdm(dataset)` wouldn 't.
there should not be a memory leak.
the training parameter should be passed as true when the model is training.
pressing `<c-l>` clears the highlighting.
gpu memory should be at most parameters+activations
the following steps will show the issue with the converted graph: phofcode
no error and the result should be 4
- os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
the minimally reproducible example below starts at ~0.04s per loop iteration and within about a minute of running is near 0.5s per loop iteration.
... steps to reproduce the behavior: 1. go to 'generate client ' > 'java ' 2. go to 'generate server ' > 'spring ' 3. check that java client generates `petapi.java` and `storeapi.java` (separated by tag) and spring server generates only one `v1apicontroller.java` containing all endpoints.
training should proceed without problems, the data is valid.
so i am trying to get checkpoint path, but i am unable to.
since cudnn force to behave determinisically = 'true '), and all the data/parameter/loss are the same, grad is expected to be same.
'vim' and 'i' and try to paste it.
- please implement or suggest a way to release gpu memory being used by unneeded models in google colab/jupter notebooks.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0.0alpha0 - python version: 3.6.5 `typeerror: tensor objects are only iterable when eager execution is enabled.
when running mobiletnet int8 model, starting from the third tflite's output mismatches with another framework.
without tfl converter optimization, prediction on
when run freeze.py, it should success according to the tutorials.
raises: typeerror: if `image` is an invalid type. """
i used the example code given here phofhyperlink .
if including tracebacks, please include the full traceback.
expected behavior is latency when i parse the feature before hand and feed in per feature tensor , there is lesser work for tensorflow to do , so it should be faster or at worse the same ?
sparse tensor operation inside a custom keras layer should not affect outside behavior if returning the expected type
- vim version (8.1, download binaray from office website vim.org) - os: windows10
my objective is to measure the amount of gpu memory needed by bert during the inference process.
a function decorated with `tf.function` cannot return a variable by reference.
tried to create do droplet with algo, everything defaults except selected ad blocking.
this would be really handy when running multiple runs on the machine and if processes might error or finish before the cache has been complete.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
7 return elt 8 assertionerror: random seed not set.
phofcode and the result of dir(tf) is : phofcode it seems that the "__init__.py" under tensorflow has been cleaned.
but it raises an exception internalerror with type: uint16, 32, 64. test with cpu and gpu.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 & 18.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12.0 & 1.13.1 python version: 3.6.8 cuda/cudnn version: n/a tpu gpu model and memory: n/a tpu here are the outputs of `capture_tpu_profile` for the base case & regression case respectively.
please see this gist with the bare minimum python script to reproduce this issue: phofurl
w0731 11136 deprecation.py:323] from <stdin>:1: read_data_sets (from is deprecated and will be removed in a future version.` `instructions for updating: please use alternatives such as from tensorflow/models.
3. create 2-3 entries for type.
far more seriously though, it is possible for function to sporadically generate false hits, since object ids are only guaranteed be unique for objects existing at same time.
2. run `vim.exe --clean` (or `gvim.exe --clean`, etc.)
large logs and files should be attached.
the first time the array is passed it somehow gets transformed to a uint8 array and for the following encodings it works as expected.
tflite model should be able to load and execute.
`e474: invalid argument: - vim version : _see below_ - os: fedora 30 - terminal: gnome terminal # vim --version vim - vi improved 8.1 (2018 may 18, compiled aug 2 2019 included patches: 1-1790 modified by <bugzilla@redhat.com> compiled by <bugzilla@redhat.com> huge version without gui.
the same is true when moving from lte to wifi, no traffic is passed such as dns or even reaching 1.1.1.1
when i print out tensors of tfilte file i get: phofcode some of tensors aren 't being converted into uint8 and i 'm not exactly sure how that is supposed to happen (i assume that is reason won 't compile.)
i've tried to boil it down as much as i could and cover any edge cases i could imagine.
currently the growing array (`garray_t`) uses a constant growing size which is specified by `ga_init2()`.
if the terminal uses 24-bit color before starting vim, the colors are not restored correctly.
last commit: update changelog.md python 2.7.16 runtime variables: algo_provider "ec2" algo_ondemand_cellular "true" algo_ondemand_wifi "true" "x251bgw=" algo_dns_adblocking "false" algo_ssh_tunneling "false" wireguard_enabled "false" dns_encryption "true"
however, if i create a layer and call the variable names are different.
save the following lines as `my_module.f90` and open it.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i know i could turn off verbosity, but i would expect sane defaults for the progress bars printed by tf/keras.
i know the distribution strategies may be kind of in flux right now, and i'm mainly just posting to make more knowledgeable people away of the issue; i'm just going to make all my tensors floats in the meantime :).
i am trying to use the create specifically constructed batches from a large number of datasets, ~7300.
osc-52 escape codes (used to send text to the host clipboard) aren 't working from inside a vim `:terminal` window.
i am running a toy matrix multiplication example ( code here phofhyperlink and timeline can be found here phofhyperlink ).
as there are only one image, i got
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: pixel 3 - tensorflow installed from (source or binary):tensorflow lite from aar tensorflow version (use command
phofcode if you remove any of the tf ops above, error doesn 't reproduce
as documented function assert_equal phofhyperlink accept any dtype.
`dict` objects wrapped around `_dictwrapper` by `autotrackable` are not serializable by pickle.
tf_upgrade_v2 should work in in jupyter and google colab
the problem is when i recreate the model and reload the weights.
run cifar10_main.py on a macbook without an nvidia gpu, i imagine this would also happen on linux if no gpu was detected.
files generated by `tf.data.dataset::cache` consume considerably more disk space than the original tfrecord files.
if i load this buffer in a second window, the fold options are correctly set again (copied from the first window).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes which i have added as an example - windows 7 enterprise - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0.0-alpha0 python version: 3.5 cuda/cudnn version: 10.0 gpu model and memory: geforce gtx 1050, 4gb with mode= 1 the erros occures.
provided value: 0.0 requested dtype: int3
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 2.00-beta python version: 3.7 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: na gpu model and memory: na
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 ` python version: 3.6.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: `release 10.0, cudnn: 7.4.2 gpu model and memory: geforce gtx 1070 you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
expected it to import so i could use it.
the complete code also covers other files in repo phofhyperlink .
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: pixel 2 - tensorflow installed from (source or binary): source tensorflow version (use command below): recent master python version: n/a bazel version (if compiling from source): 0.24.0 gcc/compiler version (if compiling from source): gcc 6 on coral dev board, clang in android ndk r17b cuda/cudnn version: n/a gpu model and memory: n/a include any logs or source code that would be helpful to diagnose the problem.
please see attached picture for my benchmarking result.
importantly, i see frequent crashes in vimspector caused by `curbuf` being invalid.
can anyone link me to some?
302.0kib 273.0kib 264.0kib (32768): 14, 13.
check your worker job for the reason why it was restarted.
phofurl ( !pip install tensorflow==2.0.0-alpha0 ) import tensorflow as tf import numpy as np from tensorflow import keras tf.version model = input_shape=[1])]) xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float) ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float) model.fit(xs, ys, epochs=500)
i get the following error message only under
the towering code i am using is based on the example: phofurl
the estimator should learn the mnist dataset at least as good from the model features as from the raw data.
so it seems to be a redraw issue; if you press any key, e.g.
just launch tensorflow initialization to show the info of the devices.
i would expect that the convolutional weights are visualized.
], use_resource=true) c = tf.constant(0.)
[default is /usr/bin/python]: found possible python library paths: please input desired python library path to use.
the cursor should go to the line containing the text "ab*cd".
=> {"changed": false, "content": "", "msg": "status code was -1 and not [200]: request failed: <urlopen error [ssl: certificate verify failed (_ssl.c:727)>", "redirected": false, "status": -1, "url": " phofurl included: for localhost
run_metadata is empty when a session run fails.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the issue disappears: - after the cursor is moved on another line - if `'conceallevel'` is set to a non-zero value - if some syntax item installed (i.e.
i 'll let you know if some pattern emerges, but though it 'd throw it up here in case people are having similar issues or have come across it before.
(could not find resource: when trying to run the graph with a feed dict to get variable value
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): source tensorflow version (use command below): master branch, commit 0c1eb886 onward python version: 2.7 bazel version (if compiling from source): 0.15.0 gcc/compiler version (if compiling from source): 6.3.0 cuda/cudnn version: na gpu model and memory: na
see this (gpu) colab notebook example with mnist data: phofurl see this (gpu) colab notebook example with numpy random for data: phofurl see this (gpu) colab notebook example using standard conv2d (not depthwiseconv2d): phofurl
the code for the layer is as follows.
see phofurl and change the sample function as described.
following example from the docs can be used to reproduce the problem.
phofcode then: `vi -u repro.vim + 'execute "normal gives the output: phofcode
... 1. open phofurl 2. click "try it out" 3. click "excute"
# therefore there one-to-one mapping between int value object-id in this range cache works as intended.
i 'm sure this a bug; maybe events are intentionally triggered, because user did create windows from a session but from shell command-line.
should not be difficult to create a sample code.
3. edit any `.vim` file.
when i run my model (which is a partial implementation of transformer) using xla, i get an error message.
import tensorflow as tf from tensorflow.keras.layers import (dense, input, lambda) from tensorflow.keras.models import model, sequential from scipy import sparse numpy as np def layer_lambda(input_x): sparse = input_x[0] dense = input_x[1] dense = tf.transpose(dense) y = dense) return tf.transpose(y) dense_mat np.eye(30, 30, dtype=np.float32) sparse_mat sparse_indices np.mat([sparse_mat.row, sparse_tensor sparse_mat.data, sparse_mat.shape) model sequential() model_input input(shape=(20,)) x dense(20)(model_input) x dense(30)(x) x lambda(layer_lambda, output_shape=(none, 30, 30))([sparse_tensor, x]) model model(model_input, x) model.save("model.h5") ``
it seems that one way to fix it is remove `#include from headers and add them back in `.cc` files.
i expect it to start/be responsive.
the 'showbreak ' character `>` is displayed at the start of each except the first line of wrapped text, but it does not have background colour of popup window, it has terminal background colour.
log of code that reproduces issue: python attributeerror traceback (most recent call last) in () 9 # y = tf.concat([y,y], axis=0) 10 ---> 11 g.jacobian(y, x) in jacobian(self, target, sources, unconnected_gradients, parallel_iterations, experimental_use_pfor) 1021 try: 1022 output = pfor_ops.pfor(loop_fn, target_size, -> 1023 1024 except valueerror as err: 1025 six.reraise( in pfor(loop_fn, iters, parallel_iterations) 149 if 150 f = function.defun(f) --> 151 return f() 152 153 call(self, *args,
when provided, any value of `max_steps` raises an `iteratorgetnext: end of sequence error`.
fyi, i have tried the graph_transforms tool under flatten atrous conv.
steps to reproduce the behavior: 1. go to '...' 2. click on '....' 3. scroll down to '....' 4. see error
i optimize a tensorflow graph with precision_mode = 'fp32 ' # "fp32","fp16" or "int8" graph_def =
should save records even out side of by default.
standalone script required to reproduce.
gradients to be computed correctly
if you limit the number of threads with and tensorflow creates a threadpool with more threads than num_threads and runs maximal num_threads causing high amount of context switches, which is delaying execution.
this happens at inference time, using a frozen model.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):
outputlocationbuffer outputlocationtype); outputclassbuffer outputclasstype); outputscorebuffer outputscoretype); numdetectionbuffer numdetectiontype); creates the post processor for the output probability.
when is specified, cuda contexts should only be created on the devices listed.
should convert frozen pb file to tflite with int8 quantization
this behavior is not seen when above message is not displayed.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
')[-1] == 'png' or _.split('.
phofurl a workaround/fix for problem can be achieved by adding another check to if clause making sure that op_input is not an indexedslices: `and not isinstance(op_input, indexedslices)` phofcode
python typeerror traceback (most recent call last) in <module>() 61 # fails 62 model.run_eagerly = false ---> 63 in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch) 1513 shuffle=shuffle, 1514 -> 1515 1516 1517 def evaluate_generator(self, in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name,
when running the example of creating a custom op from phofurl on a mac the compilation stage (see ( phofurl fails with the following error message: phofcode this appears to be because the specified linker option (returned by is not valid for `ld` on mac.
- tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.11 and 1.12 tested python version: 3.6.4 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: (tested on cpu) gpu model and memory: (tested on cpu) you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)"
], shape=(1,), dtype=float32) w op_requires failed at strided_slice_op.cc:106 : invalid argument: slice index 3 of dimension 0 out of bounds.
phofcode output tf 2.0: phofcode output tf 1.14 phofcode
run this shell command: $ vim -nu none --cmd 'au bufread,bufnewfile *.xx setf xx' --cmd 'filetype on | au filetype xx setl cul' +'wincmd w' -o xx.xx y in the left window, `'cursorline'` is set (as confirmed by the output of `:echo getwinvar(1, '&cul')` which is 1), but the text line of the cursor is not highlighted by `cursorline`.
`tflite_file = print(" want to save tflite_file" + tflite_file) # convert the model.
simpleperchanneltest appear to contain the wrong expected values.
3. console output: root-injects.js:95 typeerror: cannot read property 'get' of undefined at t.value (parameter-row.jsx:145) at t.render (root-injects.js:93) at at s.performinitialmount s.mountcomponent object.mountcomponent (reactreconciler.js:43) x.mountchildren (reactmultichild.js:234) x._createinitialchildren x.mountcomponent
during saving and loading model i face memory leakage.
this used to work in 1.12.0 but broke with 1.13.1. without metrics it 's not an issue.
iterating works perfectly well, but a runtimeerror is raised once the end of the dataset is reached (cf logs below).
function composition should be working properly.
i expect the interpreter to be able to resize all the tensors.
phofcode i think this happens because the dataset of 1825 gets split into sets of `batch_size=16`.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.12 python version: 3.5.2 bazel version (if compiling from source): 0.21.0 gcc/compiler version (if compiling from source): 5.4.0 cuda/cudnn version: 10.0/7.3 (drivers 410.72) gpu model and memory: gtx 1080ti 11178 mib runtimewarning: numpy.dtype size changed, may indicate binary incompatibility.
and when i try to convert it to a tensorrt module it fails
), but the network still trains, and if using a large learning rate, loss goes to nan.
i am not entirely sure if this is a bug or "feature" because this function is indeed called in the conversion: phofurl however, this is blocking using integer-inputs in the converted estimator.
this is what i did to trigger it: 1. install coc.nvim phofhyperlink 2. install coc extensions by running: `:cocinstall coc-tsserver coc-tabnine` 3. open the extensions list by running `:coclist extensions` 4. toggle an extension by pressting `tab` and then `t`.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
instead, it returns an `e486: pattern not found: ab*cd`, which is an error i would expect if `magic` was turned on.
when i try to save a model, i obtain the following errror: phofcode
secondly the document must include additional instructions for creating aws permissions policy phofhyperlink if mfa enforcement is desired.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): y - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu linux 19.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.13.0-rc2 python version: 3.7 cuda/cudnn version: 7.0 gpu model and memory: titan x gti 12gb the following warning is printed: warning: the tensorflow contrib module will not be included in tensorflow 2.0. for more information, please see: * phofurl * phofurl if you depend on functionality not listed there, please file an issue.
since this is run on gpu it kills the process.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): linux rhel 7.6 - tensorflow installed from (source or binary): binary - tensorflow version (use command below):1.13.1 python version: python 2.7.16 :: anaconda, inc. cuda/cudnn version: cuda 10.1, cudnn 7.6 gpu model and memory: tesla k80, 12g
today i was experimenting keras funcional api by building model using two `sequencefeatures` layers along both and `embedding_column`.
gpu runs are in blue while cpu runs are in red.
graph mode should always be faster than eager mode.
# we must start from 257 because, when creating ints between -5 and 256 python returns references singletons.
when dividing with this mapper: phofcode i get `typeerror: x and y must have the same dtype, got tf.int64 != tf.int32`.
however, when multiple models are generated within the same program life cycle, the specification of an input shape seems to produce a memory leak.
results in having the exact same content of src file in the dest file.
after 186 attempts make it work on ai platform and $400 of gsoc credits, i realized that problem was deeper than implementation details, decided overfit on a single sample, using `tf.keras` implementation of fpn from `segmentation_models` phofhyperlink by @qubvel.
i couldn 't open cuda library cupti64_100.dll f attempting to fetch value instead of handling error failed precondition: could not dlopen dso: cupti64_100.dll; dlerror: cupti64_100.dll not found
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): archlinux - tensorflow installed from (source or binary): repository - tensorflow version (use command below): 1.11 python version: 3.7 cuda/cudnn version: cuda 10, cudnn 7 gpu model and memory: nvidia 1080ti
freeze phofhyperlink from this model zoo phofhyperlink using: phofcode resulting `.pb` is 157mb.
`>>> import tensorflow as tf`
output names to be restored when loading the model and dictionaries for losses to be working when loading the model.
it seems that these adam applications and intermediate operations are completely serial with no parallelism.
when running with xla on, the code given below fails with the following exception: i cpu frequency: hz i xla service 0x4df19b0 executing computations on platform host.
when running multiple processes sharing the same gpu can cause one process to have out of memory exception.
when installing algo on a local server that has ipv6 support disabled, installation fails because it cannot set ipv6 settings in sysctl.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 (i 've also tried and results are the same) python version: 3.5.2 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version:10.0 gpu model and memory: gtx 1080ti, rtx 2080
provide a reproducible test case that is the bare minimum necessary to generate the problem.
possibility to save models with `tf.saved_model.save` as they would be saveable with `model.save`.
in the `stack` op, the allowed range for `axis` is `[ -(r+1),(r+1) )`.
corresponding values should then be shown in the "example value" box also.
comment out line 503 in env.cc: << 20, 512ll << 20); rebuild and everything works as expected.
5) then the output at [:,0,0,:] is zero (because the input array is zero there), but when batch size is large (e.g.
the specified savedmodel has no variables; no checkpoints were restored.
- os platform and distribution: ubuntu 18.04 - mobile device if the issue happens on mobile device: asus zenphone m2, snapdragon 820, 605 dev-kits.
- using google 's cloud tpu with tensorflow 1.12 system info: phofcode the issue only exists on the new 1.13. i would use 1.12 but my situation requires me to use 'channels_first ' for my model, xla enabled, and 1.13. here are also images of me running the script ( first on 1.12 which is working then on 1.13 which doesn 't work ) : screen shot at 10 20 31 am phofimage screen shot at 10 00 27 am phofimage
errors should remain at the top of the page until and unless manually dismissed.
then i 've tried casting `window_size` to `tf.int64`: `window_size = tf.constant(3, dtype=tf.int64)` and that fixed the issue in all cases.
in the following screenshot, i had put my cursor after "party" in "partylocalchatcontrol" and hit ctrl-p a few times.
test.vim phofcode 1. so %
with alpha0 and the same code the history looks as follows: alpha 0 phofimage an upgrade of the tensorflow version should not affect the resulting accuracy in such a manner.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary):binary tensorflow version (use command below):1.12 python version:3.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version:9.0/7.0 gpu model and memory:zr390 you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
the matmul operation in general does work, whith the example from: phofurl phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version: r1.12 python version: python 3.6.3 bazel version (if compiling from source): 0.18.1 gcc/compiler version (if compiling from source): apple llvm version 10.0.1 cuda/cudnn version: n/a gpu model and memory: n/a n/
instead i saw my isp's dns, indicating a leak.
the model works fine if the call function is decorated with `@tf.function` decorator.
1 feels 10% and 2 feels is 40%.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
and additionally, the order is the same between epochs, which you wouldn 't necessarily want in your training loop.
the script runs slow but fine.
several other methods of `tf.summary` are lost as well.
i deployed those models to my phone.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
setup ec2 t2.micro, setup a new strapi project, try to build.
i have tried additional padding both for % dilation_rate == 0 and % 2 == 0 without luck.
the exception information shows that the dependent resource file (libtensorflow_jni.so : is missing.
the first command raises no errors, and outputs: [' x'] the second command raises no errors, and outputs: ['xx', ' x', 'xx']
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): 2.0 alpha python version: 3.6.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: warning: logging before flag parsing goes to stderr.
set tf_cpp_min_log_level=2 should prevent showing information/warning logs including libpng warnings.
when the optimizer is specified with non-default parameters, like: phofcode the application crashes with following error: phofcode this
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.0rc1 python version: 3.6 bazel version (if compiling from source): no gcc/compiler version (if compiling from source): no cuda/cudnn version: no gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
from tensorflow.keras.models import sequential from tensorflow.keras import layers import numpy as np y_train = np.random.rand(1) epochs = 1 x_train = np.random.rand(1,20,16) model = sequential() 3,batch_input_shape (1,20,16))) activation='softmax')) optimizer='adam', metrics=['accuracy']) model.summary() history model.fit(x_train, y_train, batch_size=1, epochs=epochs) this reproduces the error in my usecase edit: using the solution proposed in phofurl did not work for me ( requirement already satisfied: gast==0.2.2 )
i would like this code to work without needing to enable persistence mode.
there is a stepped increased in memory usage after each epoch.
i printed out smallest and largest from sampe and i got these should be 2!
1. go to 'admin panel content builder' 2. add new content type `categories` 3. go to `user` collections and add relational field name `categoryid` to categories 4. go in advanced settings tab and check the `unique field` 5. create new user from front end using strapi sdk or axios with any valid mongodb 24 digit id and it will created without any error
provide a reproducible test case that is the bare minimum necessary to generate the problem.
large logs and files should be attached.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.6.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
if including tracebacks, please include the full traceback.
phofcode the github gist to reproduce the problem is: phofurl
this does not happen when passing in a numpy array to the problem is also less severe when passing in `tf.data.iterator` rather than `tf.data.dataset`.
import tensorflow as tf import numpy as np with open('same.pb', 'rb') as f: graph_def = tf.graphdef() nodes = graph_def.node for node in nodes: if node.op == 'conv2d': ``
func() return "*/ <plug>(up) " endfu eof ) +"put = 'test test \'" 3. press `*`.
running the model completes successfully.
i shouldn 't need to add an `if global_step % n == 0:` outside of the `with global_step):` function.
i will come up later with the code.
i use this function for moving pixels in an image.
info:tensorflow:saving checkpoints for 0 into info:tensorflow:loss for final step: none.
the gradients should exist for these layers.
when i want to train my own dataset (without url).
export python classify.py --image_file someimage.jpg phofcode
tensorflow lite convertion tutorial phofhyperlink and used android source code from phofhyperlink , changed classifier to phofcode for quantized model and phofcode for float model.
i want to convert a model to tflite with quantized uint8 as inference type.
- uses a basic cnn mnist keras example - centos7 - tensorflow installed from: anaconda - tensorflow version: both 1.12-gpu and 2.0.0-alpha0-cpu python version: 3.6 * this was originally posted under #26490 but @timudk helped clarify my problem so i reposted as a separate issue
saving and restoring the model should allow to resume training as if there was no interruption in the first place.
the model containing sequencefeatures layer and any corresponding feature_columns is loaded by without errors.
doing a simple list comprehension with tensor.numpy() shouldn 't be faster.
when using dataset.map to parse tfrecord, the tensor is not eagertensor
i am trying to use the pre-made estimator to use on the mnist dataset.
algo fails at `[wireguard : generate public keys]` task on aws with the following message: `failed!
so the behavior change would fail the jobs.
compile the lstmcell code correctly
though the example below is intentionally mundane, i 've found the dataset api functions like `padded_batch` to be extremely nice and was disappointed when i found i couldn 't immediately use 1.13.1 saved models as i had been in previous versions.
i have got the correct prediction results.
both torch and tensorflow can be imported in any order without gpu issues
- vim version phofcode - os: ubuntu 19.04 - terminal: xterm(330) add any other context about the problem here.
if applicable, copy/paste the text or add screenshots to help explain your problem.
the model should converge to a much higher accuracy and the moving mean and variance operations should be visible in the update ops.
is it not implemented yet?
in python, calling leads to a segmentation-fault.
this "threshold" value different in my original project where first faced the problem - there it happened when dataset array length was (100k+1) or higher, while working fine with (100k) long dataset.
i am trying to run the faster rcnn inception v2 model in a docker container.
if error in happening elsewhere third-party code ( this case , may be nvidia tensorrt closed source ) - making that explicit.
if i do not set parallel in map method, it goes well and augmentation yields same random numbers.
warning:tensorflow:tried colocate an op tower0/cond/prod_3 had different /device:cpu:0 vs /device:gpu:0. postponing error-checking until all devices are assigned.
i would expect a secondary algorithm that does not require any workspace to kick in should the scratch allocation for the primary algorithm fail even when i am not using autotune.
- os platform and distribution (e.g., linux ubuntu 16.04): google colab - mobile device (e.g.
unfortunately this triggers a casting / copy operation: phofurl because syntax of `tpucontext` only requires one positional argument phofurl a new `tpucontext` is initialized, where `_internal_ctx` is set to `tpucontext` instance, which itself actually contains instance of `_internaltpucontext`.
should work the same as all other weighted metrics.
the biases should have min/max nodes to be used by tfliteconverter after calling
it would be expected that both keras 's should be operational.
- my plugin test suite picked up the problem with vim version - os: ubuntu 16.04 64-bit - terminal: gnome terminal, gui
i was trying to match information between graph definition and timeline but they didn 't match.
while using the subclassing api for a subclassed layer and model, i was unable to use the model.save_model() function for h5 or hdf5 file types, but i could successfully save and load the model if it was saved as h5py file type.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
our library is a port of close to 1-1 from python to c# with couple of changes to allow multithreading work.
i 'm playing around with tf2.0 while mostly still working on tf1.14.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04.5 lts (xenial xerus) - tensorflow installed from (source or binary): pip version 1.12 tensorflow-gpu - tensorflow version (use command below): 1.12.0 python version: 3.5 cuda/cudnn version: 9.0 gpu model and memory: tesla v100 16g
i 'd expect `slice` to have shape `tensorshape([none, 10, 5])`.
it must display atleast the first two attributes which has a valid type.
the resulting highlight region of `v_gn` and `v_gn` with a search pattern that generates a match of length 1 doesn't include the matched string itself if it is on the left end.
import tensorflow as tf print( 'using tensorflow version {} (git version tf.version.git_version)) from import meanmetricwrapper from import accuracy def custom_accuracy(y_true, y_pred): return accuracy(y_true, y_pred) class def __init__(self,
provide a reproducible test case that is the bare minimum necessary to generate the problem.
running simple classification example with keras interface
expected to see 2 array(s), but instead got following list of 1 arrays: [<tf.tensor 'iteratorgetnext:0' shape=(?, 28, 28, 1) dtype=float32>]...``
i would expect at least 10x better performance on fake data generation.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux fedora 22 - mobile device (e.g.
train the keras estimator without errors
are graph-compatible, wrap call using original error: get w0611 23064 ag_logging.py:145] entity grl.call <__main__.grl transformed will executed as-is.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: / - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.6.7 bazel version (if compiling from source): / gcc/compiler version (if compiling from source): / cuda/cudnn version: gpu model and memory: tesla k80 include any logs or source code that would be helpful to diagnose the problem.
3. check out the value of the variable `lispwords` in the `vim` instance.
or does nightly solve the problem?
`prop_clear(1)` does not work as described in doc as per the doc, phofurl `prop_clear(1)` should remove all text properties from line 1, but it does not work.
tensorflow with mkl-dnn (intel-tensorflow) throws exception "could not initialize a memory descriptor, in file
for a minimal example, run phofcode and observe that loss and poisson values are different, and loss values vary: phofcode - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - tensorflow installed from (source or binary): `pip install tensorflow` - tensorflow version (use command below): 1.13.0-rc2 python version: 3.7.2 x64 cuda/cudnn version: n/a gpu model memory: n/a more code examples investigations at phofurl
(estimator: first epoch~92%, final~96%, sess+graph: first epoch~96%, final>98%)
the rank must be statically known (the shape is not `tensorshape(none)`.
why is it not processing all examples in the dataset?
i 've just started to migrate from tf 1.x to tf 2.0 and to use keras apis, so i might be wrong when using keras api.
when i run `result = model.evaluate(x_train, y_train), my jupyter notebook crashed.
the value became an array instead of scalar float after 8000+ steps phofurl
evaluating tf 2.0 keras model allocates twice as much memory as tf 1.x or cntk.
phofcode and the resulting error: phofcode
furthermore, there is no information of this behavior on the documentation.
both lines are highlighted with cursorline.
no sigabrt happens, echoes `{ 'x ': { 'y ': 2}}`.
i saved the resulting pb file with this code: from tensorflow.python.tools import freeze_graph phofcode
translate test function return same results but on training model, i got phofcode on restored model, i got phofcode looks almost random.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
steps to reproduce the behavior: 1. go to ' 2. scroll down to 'custom value view formatters'
this implementation causes three problems which are not expected.
); sess = tf.session(); sess.run( tf.pow(z, 0) )`
errors are: phofcode i test without plugins and clean config and don 't happens.
tensorflow-gpu can be used properly.
1. create a new content type "book".
i have two models that i have trained using keras.
i simply customized the formatting and added a metric to track the mean squared error in addition to the total loss.
2. open a new window with buffer `foo`, and run a job that will scroll the window displaying the buffer `foo`: phofcode 3. delete all lines in the buffer `foo`: phofcode * notice, there's no topline in buffer `foo`.
the code snippets works fine on colab but gives the following error on windows: overflowerror: python int too large to convert to c long the above exception was the direct cause of the following exception: traceback (most recent call last): file line 3296, in run_code exec(code_obj, self.user_global_ns, self.user_ns) file line 44, in <module> file line 36, in demo file line 660, in __call__ outputs = self.call(inputs, *args,
here is respective trt output: phofcode since trt model only has 3 nodes, one of which is trt engine node, does it make sense to convert this via uff?
when training a keras model containing a custom layer whose behavior depends on the training phase, `training` is not set by the `fit()` method, and `k.learning_phase()` is `0`.
i am getting a failure at the task [common : include_tasks] phase.
heres a minimal example: phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 python version: 3.5.2 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: 9.0.176 / 7.3.0.29 gpu model and memory: geforce gtx 1080 ti
please refer to my project at phofurl after train with train_mnist.py, executing start_serving.sh will reproduce the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
(the modification is that the final layer was replaced with a dense layer and sigmoid to use the for regression.)
if including tracebacks, please include the full traceback.
however, this isn't working with `'noesckeys'` either.
decorate `def call` with `@tf.function` in the second cell of the notebook: phofcode
------ jensen @ubu16 alexnet_imagenet$ python myalexnet_forward.py warning:tensorflow:from initialize_all_variables (from is deprecated and will be removed after instructions for updating: use instead.
result since data fits into memory, use entire per layer.
large logs and files should be attached.
i suggest treating exceptions in `find_help_tags()` as literal search strings and not invoke any regex search so that there is no need for a previous substitute string (`reg_prev_sub`).
tflite outputs don 't match with tensorflow outputs
write this code in `/tmp/vim.vim`: syn enable set cole=3 cocu=n lines=24 columns=80 nno cd :call func()<cr> fu func() call search( '| ', 's ') echom col( '.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): phofcode - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04.5 lts - mobile device (e.g.
i've printed a batch size for each call model and i see something like: phofcode which causes `check failed: work_element_count > 0 (0 vs. 0)` inside model call because empty batch.
i expect that using the --use_nnapi=true flag will also display op profiling information
this works fine everywhere except in terminal mode, where commands like `<c-w>j` ignore the mapping and execute their old command.
following code is a snippet from dataset.window documentation phofhyperlink phofcode calling `iterator.get_next()` throws following error: phofcode
getcurpos()[4] (curswant) returns after $ motion
#!/usr/bin/env python3 import numpy as np import tensorflow as tf from tensorflow.keras import layers # uncomment this line and you will get an error # def tf_dataset(): def data_gen(): while true: yield np.random.rand(72, 96, 1), np.random.rand(18) types = (tf.float64, tf.float64) shapes = (tf.tensorshape([none, none, none]), tf.tensorshape([none])) dataset = types, output_shapes=shapes) dataset = dataset.batch(16) return dataset dataset tf_dataset() model tf.keras.sequential() # notice how the input dimensions are mismatched 72, 1))) kernel_size=4, strides=4, activation='relu')) kernel_size=4, strides=4, activation='relu')) activation='softmax')) print(model.summary()) model.fit(dataset, steps_per_epoch=1, epochs=1) ``
"test-value" would be set in the testheader input field.
when i study the source code in i think it should returns [[[0.
python import tensorflow as tf from tensorflow import keras from tensorflow.keras import backend as k # import keras # from keras backend as k numpy as np class crf(keras.layers.layer): def __init__(self, num_tags,
on the cpu, `relu` consistently returns `nan`.
23. buttons must have discernable text.
1. compile vim (current git master): `./configure --without-x --enable-gui=no` 2. create `~/.vim/vimrc.segfault` (or some such) with contents: phofcode 3. run `src/vim --clean -u ~/.vim/vimrc.segfault` 4. focus should be on the terminal window of the second tab page.
`tf.nn.relu` + `tf.keras.model` + `@tf.function` (this is the only case that produce `none` gradient) python import tensorflow as tf z = tf.keras.input(()) h = tf.nn.relu(z) m = tf.keras.model(z, h) @tf.function def f(x): # with @tf.function with tf.gradienttape() as t: t.watch(x) z = m(x
` in the html indent plugin < phofurl i don 't know much about indent plugins, but the next command seems to have the same effect as `normal!
large logs files should be attached.
- we have made a specific adaptation of a inference script, that reads in tensorflow records, pushes the data through the network and then stores the result.
- reading a file with lines (and repeat 100 times): identical performance this suggests to me problem is with tfrecorddataset, but if there is any other benchmarking that makes sense, please let me know.
32 is number of filters/outputs/feature maps.
1, open a term buf 2, set cursorlineopt=number 3, set cursorline 4, hi cursorline ...sth...
i want to compute the gradients of a loss function with respect to a model (in order to do meta-learning) and use those gradients to define the same model with new weights.
xiaomi redmi 6: - os | android 8.1 (oreo), planned upgrade to android 9.0 (pie) - chipset | mediatek mt6762 helio p22 (12 nm) - cpu | octa-core 2.0 ghz cortex-a53 - gpu | powervr ge8320
i want to store them in a gcs, so i am using functions from the `tf.io.gfile` package inside a `tf.function`.
large logs and files should be attached.
if this would be the case then that 's the error i would expect.
my models use manually defined metrics for recall and f1 score: phofcode
a model is compiled and trained using `script a` on the google compute engine.
- do have do something different conversion from h5 tflite?
when we change save_format from `tf` to `h5`, the expection(oserror) should not happen.
phofcode batch_size = 750 history = y_train,batch_size), y_test, batch_size), # validation_data=none, verbose=1, epochs=100, ) warning:tensorflow:using a generator with and multiple workers may duplicate your data.
that code should produce an array `[0j, 1j, 2j, ...]` no matter how many times it's run.
training on mac os, with cpu only, with and without distributed scope for mnist example.
tf.tensor(1, shape=(), dtype=int64) tf.tensor(8, shape=(), dtype=int64) tf.tensor(8, shape=(), dtype=int64) tf.tensor(14, shape=(), dtype=int64) tf.tensor(3, tf.tensor(12, tf.tensor(6, tf.tensor(16, tf.tensor(0, tf.tensor(18, tf.tensor(2, tf.tensor(2, tf.tensor(4, tf.tensor(10, tf.tensor(7, tf.tensor(4, tf.tensor(9, tf.tensor(0, tf.tensor(5, tf.tensor(6, ``
1. go to strapi cli 2. run strapi generate:model category --api product to generate new model inside existing product api
i 've seen this line ( phofurl is it related to this issue?
expected to see only date in example phofcode
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): binary tensorflow version (use command below): python version: 3.6 bazel version (if compiling from source): na gcc/compiler version (if compiling from source): na cuda/cudnn version: gpu model and memory: geforce gtx 1050 ti, 4096 mb warning: logging before flag parsing goes to stderr.
this is the tf code that i modified for gpu tracing in a distributed mode.
in other cases fit() and fit_generator() work well.
large logs and files should be attached.
phofurl ~~~ assets/ assets.extra/ variables/ variables.index ~~~ my final target is to optimizing tensorflow serving performance with nvidia tensorrt phofhyperlink , and the model that i want to launch is a pre-trained mask-rcnn downloaded from tensorflow_model_zoo phofhyperlink any help would be grateful
the problem is not about converting, but about the interpreter, especially in the allocation.
could you please help to figure it out?
just mentioning it because @njia figured out his issue was an interaction between the mouse and `syntax on`.
- vim version: 8.1 (2018 may 18, compiled aug 3 2019 - os: macos 10.14 - terminal: iterm2
the above patch (that supposedly should correct this exact issue) does not test ` 'modifiable '`, and only deals with error cases (normal error and `:throw`) inside `function()`, but not the normal, successful execution.
- os: centos 7 - terminal: gnome terminal
in particular, number of steps per epoch gets set to number of steps in validation set.
there should not be any delay or at least no perceivable.
`tf.saved_model.save` does not work on keras model when the output layer produce multiple outputs.
if including tracebacks, please include the full traceback.
there should not be such a big gap.
i have now set up 2 cloud service to try and see if that might gave been the problem, but still can seem to get pass a certain stage.
trying to read a large cached entry (3141 laravel* models) with rdm, but it segfaults.
if including tracebacks, please include the full traceback.
s:do(event) abort if exists( '# '.a:event) exe 'do '.a:event. '
python import tensorflow as tf @tf.function def f(x): print(" tracing with shape", x.shape) return x
tensorflow version (use command below): 2.0.0-alpha0 python version: 3.7 bazel version (if compiling from source): 0.23.0 gcc/compiler version (if compiling from source): gcc 4.8 cuda/cudnn version: cuda 10.0, cudnn 7 gpu model and memory: a simple model can reproduce.
the network is an unrolled lstm-based model.
building a tf.keras sequence model when eager mode is enabled vs not results in different output nodes.
`:let g:netrw_list_hide = netrw_gitignore#hide()` 5.
it doesn't make sense to me that the possibly buggy inference of a shape influences whether an op is supported or not.
the betainc_op test (within fails because one of the gradient values is nan.
6 input features implies that `shifted_time` should be a 6-dimensional vector)
phofurl in the repl, 1. is an example of this bug, 2. is an example showing how if the component is transitioned out completely then the slot is updated properly.
the following code worked fine with tensorflow-cpu 1.12.0 but not with the gpu version.
phofcode can you confirm the strange behavior and whether tf.random.normal is implemented or not
i train a model on windows (gen_test_train_data.py, then train_model.py), using the same tf version that 's available on rpi.
`let nr = bufadd('')` 3.
i: ...; k.set_learning_phase(0); model.compile(...); model.fit(...)") k.set_learning_phase(0) optimizer="sgd") history model.fit(x, x, epochs=2) print("this test does not make much sense, why would you call fit with learning phase 0?
in the source code, i think they are marked as nodes to preserve in in but i could not find out the exact reason.
4. observe that vim doesn 't open the url in a web browser.
large logs and files should be attached.
`gvim` should detach from the terminal.
phofcode output: phofcode exact command to reproduce: phofcode (to please tensorflowbuttler
basically in code below i create a td.data.dataset using new package tensorflow_dataset.
e0625 ag_logging.py:133] error converting traceback (most recent call last): file line 78, in parse_entity return parse_str(source, source file line 140, in parse_str module_node = gast.parse(src) file line 240, in parse return
... steps to reproduce the behavior: 1. go to '... ' 2. click on '.... ' 3. scroll down to '.... ' 4. see error
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): macos - mobile device (e.g.
the basic code should work.
unknownerror traceback (most recent call last) in <module> 1 import tensorflow as tf ----> 2 3)(tf.ones((1, 5, 5, 3))) in __call__(self, inputs, *args,
detailed steps to reproduce the behavior: 1. run `vim --clean` (or `gvim --clean`, etc.)
`#print out model summary so can see it for bug report model.summary() #from tensorflow import lite tflite_file = print(" want to save tflite_file" + tflite_file) # convert the model.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - linux ubuntu 18.04 - tensorflow installed from: pip - tensorflow version (use command below): 1.13.1 python version: 3.6.7 cuda/cudnn version: cuda 10.1, cudnn 7.6 gpu model and memory: 2080ti include any logs or source code that would be helpful to diagnose the problem.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
gpu float32 implementations of `tf.matmul`, `tf.matvec` and `tf.einsum` all fail when applied to tensors with a `dimension >= 2
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): _pip_ package manager tensorflow version (use command below): 1.12.0 python version: python 3.6.7 64-bit bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: no gpu gpu model and memory: no gpu you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" yeah...... maybe no logs here.
i expect the custom model to be built automatically the first time it is called (e.g., by the `fit()` method).
expect the content pasted ok/there.
see logs bellow... on model.fit(), nothing happens... the gpu is at 3% utilization and one cpu core at 100%.
the json should just be: phofcode
i performed transfer learning on pretrained model with tf custom training loop and keras fit.
runs and detects objects smoothly
the second and third assertion (the `not` assertions) would be expected to fail.
here 's a gif illustrating the issue: issue phofimage - processor: 2.2 ghz intel core i7 - memory: 16 gb 1600 mhz ddr3 - graphics: intel iris pro 1536 m
currently, i trained tensorforest on a relatively large train data (1.5m examples, 1024 features) with 500 trees, max_nodes.
note : the inference is only running on cpu as i turn off the visibility for cuda devices, so tensorflow code runs only runs on cpu.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
for my purposes, it does not matter much if use a dynamically or a statically linked library.
in particular, it seems the autograph magic doesn't get applied to functions that are only called within the parameter list of other functions.
we have an image classification model trained from scratch in frozen graph format and: 1.
1383 """ -> 1384 iterator = get_iterator(dataset) 1385 inputs, targets, sample_weight 1386 return inputs, targets, sample_weight get_iterator(dataset) 1362 def get_iterator(dataset): 1363 """create initialize an iterator from a dataset."""
for example, 1. build version (just `make`) and run `src/vim` 2. insert 3. search `1` 4. enter visual mode and highlight `567` from right to left.
large logs and files should be attached.
unable to train a model using mirroredstrategy with cudnnlstm.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes.
if including tracebacks, please include the full traceback.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): 0.0.1-gpu-experimental python version: 3.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory:
`:setlocal cursorline` , cursorline is displayed.
1. download the android ndk.
image_placeholder = shape=[1, none, none, 1]) src_pts = tf.constant([10, 10]) dest_pts = tf.constant([100, 100]) = src_pts, dest_pts, interpolation_order 2, regularization_weight 0, num_boundary_points 1 )
this seems to occur when the `tf.app.flags` abseil wrapper attempts to use implicit parsing.
when using tf.dataset (tfrecorddataset) api with new tf.keras api, i am passing the data iterator made from the dataset, however, before the first epoch finished, i got an `when using data tensors as input to a model, you should specify the `steps_per_epoch` argument.` exception, even though i 've set this attribute in fit method.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): centos linux release (core) - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.13.1 python version: 3.6.7 (anaconda) cuda/cudnn version: 9.2/7.3.1 gpu model and memory: gtx 1070 ti
if i used instead it would work, i expect using momentum optimizer to work as well.
so it seems, that there is something wrong with k.sqrt(
only cache the dataset once and not throw an error.
`bufwinleave` is included, but it 's not triggered when vim starting up; it 's triggered when vim quitting (here via `:qa`).
feeding the mask into the call of a recurrent layer is the only workaround to combine convolution, lstm and masking, and it would be great if it worked the same way for batchnorm: phofcode .
`metrics` should be computed in any case to give insights about the training as it is based on model output and true value given by the data loader/sequence.
cpu usage shouldn't noticeably change when a mouse click is made.
3. edit a file, any file.
phofcode also see colab notebook here: phofurl
- os platform and distribution (e.g., linux ubuntu 16.04): windows 10 - tensorflow installed from (source or binary):binary - tensorflow version (use command below): 2.0.0 - python version: 3.6.5 include any logs or source code that would be helpful to diagnose the problem.
however, the script fails with the following error: traceback (most recent call last): file line 558, in make_tensor_proto str_values = [compat.as_bytes(x) for x in proto_values] file line 558, in <listcomp> str_values = [compat.as_bytes(x) for x in proto_values] file line 61, as_bytes (bytes_or_text,)) typeerror: expected binary or unicode string, got 1 during handling of the above exception, another exception occurred: traceback (most recent call last): file "try.py", line 82, <module> out = netvladlayer( num_clusters=16 )(input_img) 538, __call__ 1603, _maybe_build self.build(input_shapes) "try.py", 21, build trainable=true ) 349, add_weight aggregation=aggregation) 607,
i have an `gcp instance` with t4 gpu with same `tensorflow` `keras` `cuda` version as `gcolab`.
the diff is that `k.__name__` is in the first case and `tensorflow.keras` in second.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
i expect it to save a png image of the model.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow version (use command below): python version: 3.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
how absolute error can be 76?
steps to reproduce the behavior: 1. installed profile onto a relevant device (windows 10 or ios 12 with on demand) 2. activated vpn profile 3. began browsing and observed response + user experience
if including tracebacks, please include the full traceback.
i thought this problem is caused when distribute strategy trying to slice the inputs equally to all replica.
error: instance of 'checkpointstate ' has no member
when on `a`, only `ba` is selected (the `r` is missed).
provide a reproducible test case that is the bare minimum necessary to generate the problem.
if i run: python from tensorflow.python.client import device_lib i get an output with tf2.0: [name: "/device:cpu:0" device_type: "cpu" memory_limit: locality { } incarnation: name: "/device:xla_cpu:0" device_type: "xla_cpu" memory_limit: locality { } incarnation: physical_device_desc: "device: xla_cpu device", name: "/device:xla_gpu:0" device_type: "xla_gpu" memory_limit: locality { } incarnation: physical_device_desc: "device: xla_gpu device"] and import tensorflow as tf gives false.
the value estimate should be a scalar float
... steps to reproduce the behavior: 1. go to phofurl 1. authorize via `api_key` with some gibberish 1. close autorization window 1. open url in topbar of the same petstore browser window: phofcode (this is just a minimal spec to reproduce this issue i created on gist: phofurl on the test spec open the authorization window again you will now see that you are still logged in with `api_key` but for wrong api
phofcode replacing `x = tf.random_uniform((d,))` with `x = tf.ones((d,))` eliminates the problem, as does replacing `x = x + x` with `x = 2*x`.
should work with tfx pipeline such as tensorflow_transform and
will update when i have written a minimal example.
- vim version - os: arch linux - terminal: gtk 3 gui with gtk 2 gui it works as expected.
saving a keras model with `tf.saved_model.save()` works, but the `method_name` is empty, so deploying the model to tensorflow serving fails.
from tensorflow.train import adamoptimizer from import lossscaleoptimizer, ... optimizer = adamoptimizer() loss_scale_manager =
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): centos - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):
detailed steps to reproduce the behavior: 0. make sure window is small enough and search stats is shown: set columns=80 shortmess-=s 1. copy&paste this text to a file.
it should stop the training process as soon as rises.
in the following test, phofurl phofcode it appears that `let t_f2` should be `let &t_f2`, a simple typo.
(env) $ ./algo (master) play [ask user for the input]
the "focus" not to flicker as shown above.
cloudpickle cannot unpickle `tf.keras` because the `deprecation_wrapper` introduced in 1.14. in one python session: ~~~python import cloudpickle import tensorflow.keras as k with open("/tmp/k.pkl", "wb") as f: cloudpickle.dump(k, f) ~~~ then start another python session: ~~~python import cloudpickle with open("/tmp/k.pkl", "rb") as f: cloudpickle.load(f) ~~~ error: ~~~ traceback (most recent call last): file "<stdin>", line 2, in <module> file line 148, in __setstate__ keyerror: 'tensorflow.keras ' ~~~
you can try this here: phofurl
large logs and files should be attached.
it seems that i succeeded by assigning constant values see the function * assign_op * , while if i use symbolic values of tensor to modify same values of tensor cannot understand what is wrong.
there may be more, but i can see two explanations.
i'm running some experiments using tensorflow 2.0 nightly.
saving the pb does not error, creates valid pb file but it lacks the final graph output, tools like tf_coreml netron complain of missing output.
import os = '0' import numpy as np import scipy as sp import scipy.fftpack tensorflow as tf matplotlib.pyplot as plt # generate data slc = slc = (slc + 1j*slc)[0:256, 0:256] tilesize = slc.shape[0] freqimg np.fft.fft2(slc) # now do this in keras/tf input1 tilesize, 1), dtype='complex128') out model out) freqimg_tf plt.figure(); plt.title('sp') plt.figure(); plt.title('keras'); plt.show() # plots print('done') ``
for example in merge.java: phofcode
the tf in the docker container should be updated and build
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux - mobile device (e.g.
w_embedding_vocab = dtype=tf.string, w_embedding_vocab_table = default_value=0, name="word_embidx_tbl") w_embeddings = embdic.dim], dtype=tf.float32, trainable=false)
if i had to guess wildly, it could be another regex recursion issue similar to #3175, since the regex for the autocmd during visual selection might interfere with the regexes used for syntax highlighting.
python code: phofcode tf_config environment variable: phofcode
so made an experiment where replaced sparse categorical accuracy function by a function simply computing rank (this function `ndim_y_true` attached notebooks) found even more confusing results.
working keras model should always be convertable to estimator.
- if the input is a variable tensor (like `nan` wrapped into `tf.variable` or multiplied by a random tensor), it returns zeros.
... steps to reproduce the behavior: 1. go to phofurl 2. paste in the config above 3. notice the incorrectly rendered code blocks
intuitively, `wildignore` should not affect whether or not vim uses the files defined in `tags`.
phofcode fails with log: phofcode
since the pb file includes tf.contrib.resampler, it fails loading the model saying resampler op is not registered.
when multiplying (`*` operator) a sparse tensor `s` with rank `n` with a dense tensor `d` with rank `m`, where `n > m` and `s` and `d` have different but broadcast-compatible shapes, then tensorflow attempts to perform the multiplication producing and incorrect sparse tensor as a result with the shape of `s`.
see example here phofhyperlink in the above link, it works as expected.
as we all know that the commnuication is hidden in the back-propagation progress; and that 's true in my monitoring, like the yellow line in figure below, that is with xla disabled.
when passing `steps_per_epoch` to `model.fit` as a parameter when training on data fed via a instance the model should be updated exactly `steps_per_epoch` times in every epoch.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no i used the detectoractivity of android mobile demo app ( phofurl and the tensorflow lite demo app ( phofurl - os platform and distribution (e.g., linux ubuntu 16.04): linux 18.04.1 lts and lineageos android 8.1 - mobile device (e.g.
phofcode this results in the error phofcode
fake_quant should inherently be supported.
i could run the same command without issues ubuntu 16.04. this problem seems be related linking of dependencies.
not found: resource does exist.
(as of commit cd701ec in r1.14 branch).
the file update however is not managed well since modifying files training should not be affected.
i tried using `autochdir` when i started using vim during the 7.x days but it didn 't work for me; i think it interfered with things like `find` and `vimgrep`.
there should be a way to delete algo profiles(delete button or such
after that, the iterator is used as the second input of as i run the output of `experimental_run`, an error occurs: phofcode
`gpu delegate does not support but this option is necessary to convert an ssd model successfully (with 4 outputs).
num runs: [50] inter-run delay (seconds): [-1] num threads: [1] benchmark name: [] output prefix: [] warmup runs: [1] graph: input layers: [] input shapes: [] use nnapi : [0] nnapi error: unable to open library libneuralnetworks.so loaded model resolved reporter didn't find op for builtin opcode 'depthwise_conv_2d' version '2' registration failed.
phofcode this fails with the following error: phofcode not exactly sure if this is a bug or a feature.
as per the doc, phofurl if the guibg color is set as `none`, it indicates "no color (transparent)", but in popup window, the color is the same as `normal`.
however, if run exactly same code on aws p3.16 (64 vcpu, 8 gpus), throughput * decreases * significantly, even while cpu use is higher.
i built the network using the following: phofcode i then load and run the model on android using bytebuffers to hold input/outputs.
result of `nvidia-smi` phofcode here is my current workaround phofcode for now, i 'm creating a child process to see if it crushes
- os: macos 10.14 (haven 't tested others yet) bug was introduced in originally i got this bug filed against macvim at phofurl so cross-referencing here.
i got the `magic` version working flawlessly and encountered this bug when attempting to make my plugin work for `nomagic` version.
when i pass one-hot encoded labels as train and validation data into tensorflow keras ' model.fit() function, the metric (and tn, fn, fp) return wrong values.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: mi 5 - tensorflow installed from (source or binary): tensorflow version (use command below): python version: bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: gpu: phofcode cpu: phofcode
i expect vim to exit cleanly.
- terminal: gui, console, urxvt-unicode-256color.
` phofurl 1. the validator badge displays as a broken image with a broken link
*) ')|set cole=3 cocu=n nowrap" +"pu=repeat( \'x \', &columns/2).. \')z \'" --- if you execute `:norm!
devices: i streamexecutor device (0): <undefined>, <undefined> traceback (most recent call last): file line 87, in <module> main() file line 81, in main loss = train_one_step(params, data) file line 426, in __call__ self._initialize(args, kwds, file line 370, in _initialize *args,
while majority guarded by a check that won 't do actual work, there are somethings in method will run waste cycles on every iteration.
steps to reproduce the behavior: 1. download algo.
- os platform and distribution (e.g., linux ubuntu 16.04): phofcode - mobile device (e.g.
it looks like `learning_phase` is "hard-coded" into call() method 's graph when model is built?
i got the following error when running the inference: initialized tensorflow lite runtime.
no error, the definition is valid.
% output_types) ---> 91 output_types = [_execute.make_type(_t, "output_types") for _t output_types] 92 if not (list, tuple)): 93 raise typeerror( make_type(v, arg_name) 124 except typeerror: 125 raise typeerror("expected datatype for argument '%s ' not %s."
many seemingly trivial changes make `e488` disappear: * there has to be function invocation (e.g.
i expect this operation to be quicker, as (afaik) it is needed for any complex operations involving tf.scatter_nd and other element gathering ops.
i do not believe such a huge gap comes from bn/dropout or the batch difference, i doubt there is a bug in computing the loss in keras.model.fit()?
it should not throw any type of error during training.
5. add a new entry to your content type that contains said field.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - tensorflow installed from (source or binary): pip wheel - tensorflow version (use command below): python version: 3.7
i believe this is a iptables rule possibly but i have tried placing a "-a forward -p tcp --dport 443 -j accept" but it has not made any difference.
- with `a:` being mapped to ` wsl$ arch` via `subst`: 20c23 seconds.
if including tracebacks, please include the full traceback.
i follow the example classify structred data phofhyperlink as below: phofcode and it shows an error: `valueerror: passing a dictionary input to a sequential model which doesn 't have featurelayer as the first layer is an error.` however, i found the reason that `sequencefeatures` doesn 't have a property `is_feature_layer` as below: ## # todo(rohanj): this is a hack to get around not depending on feature_column and # create cyclical dependency.
i created a small colab notebook to demonstrate the issue: phofurl i recommend downloading the .py file and running it in command line (so colab logging doesn't interfere), because after the epoch is done, the correct batch number is found.
model saving should not fail.
i think `-0.5f` is more reasonable
should be able to save and reload models that have been saved under mirror strategy.
note: i still haven't gotten this script (despite some manual upgrading) to work on tf2, but it (pre-manual upgrade) works fine on tf 1.13.1, although it gives me a bunch a bunch of deprecation warnings :)
it equals to `-19` when `drop_remainder=false` and to `-16` when `drop_remainder=true`.
i think the phofcode function should continue training without crashing.
long starttimeforreference object[] inputarray map<integer, object> outputmap hashmap<>(); outputmap.put(0, outputmap.put(1, outputmap.put(2, outputmap.put(3, outputmap); long endtimeforreference trace.endsection(); logger.v("timecost to run model inference: " + (endtimeforreference - starttimeforreference)); map<string, float> labeledprobability tensorlabel(labels, .getmapwithfloatvalue(); trace.endsection(); gets top-k results.
this code succesfully run with tf 1.13 but failed on tf 1.14.
better wrapping and space distribution.
- os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 on wsl, also colab - tensorflow installed from (source or binary): binary (pip, no gpu) - tensorflow version (use command below): 1.13.1 python version: 3.6.8 (anaconda)
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04.5 lts (gnu/linux x86_64) - mobile device (e.g.
full log can be found phofurl
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no - os platform and distribution (e.g., linux ubuntu 16.04): windows 10 home: centos 7 - tensorflow installed from (source or binary): source - tensorflow version (use command below): 1.11 python version: 2.7 bazel version (if compiling from source): 0.18 gcc/compiler version (if compiling from source): gcc 5.4 cuda/cudnn version: cuda-10.0, cudnn 7.3, tensorrt 5.0 gpu model and memory: nvidia p40 crashed when session process two task in the same time.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
after installing the package globally, it worked.
i 'm migrating my codebase to a tensorflow 2.0 compatible version, thus i 'm replacing all the `tf.layers` call with their `tf.keras.layers` counterpart.
steps to reproduce behavior with chrome dev tools: 1. clone linked repo and run app 2. take a memory snapshot before clicking anything on app page 3. click 'toggle ui' button a couple of times 4. take another memory snapshot 5. note increase in memory use
i was expecting that since the batch size is unknown until runtime, that tensorflow would be able to handle this error, similar to how tensorflow can accept an unknown dimension and generate a matrix/tensor with the unknown shape.
2. type `:pyx import vim` 3. type `:pyx 4. instead of the expected output of `{0: 0}`, the result was `{ '0 ': '0 '}`.
phofcode this makes it harder to understand behavior of function phofcode in case phofcode is a matrix.
however, when i add some post-processing with c++(jni), the inference time (only the function `interpreter.run(input, output)` run time)increases to 47ms(1 thread), 34ms(2 threads) , 30ms(3 threads), respectively, and i got same inference time even though i didn 't use post-processing (just put jni code in my project).
if including tracebacks, please include the full traceback.
- vim version: both and (the backtrace is from - os: arch linux - terminal: kitt
import tensorflow as tf input = tf.keras.input([none]) spec = tf.signal.stft(input, 400, 160) ``
no distribution strategy used evaluation.
attached is a python notebook that reproduces the issue.
you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
creating a `mean` instance should be harmless, especially if it is unused.
the conversion code is taken straight from the document: phofcode the model has extra operators in the beginning (sub, div, gather) just because i did not have time to rebuild the bare minimal test case but i think it is already simple enough
test.vim phofcode 1. so % 2. press `<esc>` 3. the two popup windows don 't close at once, expected behavior is both are closed at once.
based on my observation, when training data is not balanced distributed among workers and chief, more specific, when chief has fewer data.
looking at comment line `# if shape y_true is (num_samples, 1), squeeze (num_samples,)` in code, it is clear that something is not working as intended.
if you are certain is graph-compatible, wrap call using original error: could get w0611 23064 ag_logging.py:145] entity dc.call <__main__.dc transformed will executed as-is.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): google deep learning vm version: m10 based on: debian gnu/linux 9.5 (stretch) (gnu/linux x86_64 ) - mobile device (e.g.
gif phofimage the same issue applies when `:put!` is run, or `[p` is pressed.
this could mean that the variable was uninitialized.
the error should not be raised because there is no loop in the computation graph.
the two calls to test in code below should have same output.
the vim - try to paste it at bash/cli -> paste ok. 5.
since i have many networks served on the same gpu, i would like them to take as low vram as possible.
2. if mape about the same as mae - how percentage can be equal to absolute value of error?
the following code will be able to reprorduce this problem: phofcode however, if we comment out the then it will work as expected:
large logs and files should be attached.
- have i written custom code:
bazel run -c opt -- --inference_type=float --max_detections=500 --allow_custom_ops
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macosx 10.13.6 - mobile device (e.g.
like other layers, conv3dtranspose layer should automatically infer the dimension size as well.
using tf.py_function which has tf.string type input generates warning like this: > w0429 13252 backprop.py:820] the dtype of the watched tensor must be floating (e.g.
from the result, you can see the inference time of the quantized models is always a little bit longer than the float point model.
images0, images1 = self.load_batches() self.images0 = shape=(self.batch_size, self.c_dim, self.output_size, self.output_size)) self.images1 = shape=(self.batch_size, self.c_dim, self.output_size, self.output_size)) for i in with if i == 0: self.set_tower_loss('', self.images0, generator, discriminator) else: self.set_tower_loss('', self.images1, generator, discriminator) _, g_grads= , feed_dict={self.images0: images0, self.images1: images1} )
phofcode therefore, i do have a working alternative, but there does appear to be an issue with keras concatenate functio
if you don't manage to reproduce, i will give a sample code.
i have now stumbled on the following problem: the function `load_weights` for a keras model does support the argument `skip_mismatch` in keras, but not in tensorflow.keras.
(was not able to bisect).
i 'm trying to run a c++ demo of tflite+opengles on my aarch64 android 8.1 board.
the code running correctly in tf 1.x.
i wrote a custom estimator to implement fm algorithm and used train_and_evaluate to perform distributed training (4 ps and 100 workers).
... steps to reproduce the behavior: 1. navigate to phofurl -> tab key over to "try it out" button -> enter key -> tab key down to execute -> enter key tab over clear enter 2. navigate phofurl tab through the entire page 3. navigate phofurl "try it out" button 4. navigate phofurl lock icon enter 5. schemes combo box authorize button 6. pet link 7. over "try it out" button 8. down "parameter content type" combo box 9.
phofcode below is the code to load, update inputs, perform inference, and freeze the model.
[[1, 2, 3, 0, [{ 'lnum ': 2, 'col ': 1, 'added ': 1, 'end ': 2}, { 'lnum ': 2, 'col ': 1, 'added ': 0, 'end ': 3}]], [1, 2, 3, 0, [{ 'lnum ': 'col ': 3, 'added ': 0, 'end ': 3}, { 'lnum ': 'col ': 'added ': 'end ': 3}, { 'lnum ': 1, 3}, { 'lnum ': 3}]]]
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device:na - tensorflow installed from (source or binary):binary tensorflow version (use command below):1.12.0 python version:3.5 bazel version (if compiling from source):na gcc/compiler version (if compiling from source):na cuda/cudnn version:na gpu model and memory:na you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
on my initial example (which i do not include as it relies on custom data and model layers), run time went from two minutes to five.
data should render properly like it did with swagger 2.0 ui
calling tf.function from tf.py_function does not hang the program.
it is expected that the above script produces pbtxt file similar to the one provided by google for its mobilenet ssd model.
... steps to reproduce the behavior: 1. go to api 2. choose any method 3. click try it out 4. click execute 5. error: image phofimage
tensorflow requires that this dll be " "installed in a directory that is named in your %%path%% " "environment variable.
doing a training with tf.keras results in out of memory after some time when including the `class_weight` parameter.
the model can be converted only when global step variable and custom cross entropy implementation are used (but it cannot be run on android).
i am using a keras model with tf 2.0. i am converting the model to estimator: ` then it is crashing when running when using (after migrating the code to tf 2.0 of course).
with tensorflow graph without eager execution everything works fine.
tflite gpu supported ops, must run in gpu.
no node-device colocations were active during creation.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): docker image, macos 10.14 - mobile device (e.g.
then i used this option to create a session and load a model.
if including tracebacks, please include the full traceback.
i am not sure if this is intended, a bug, or a reasonable failure.
this leads me to suspect that the issue is windows-only
(env) ./algo play [ask user for the input]
2. edit a few lines and save it 4-5 times.
1. wrapping a 1-layer convolutional nn with phofcode error: phofcode 2. wrapping a pre-trained `tf.keras.applications` model (closer to my actual use case): phofcode error: phofcode 3. saving as an hdf5 file instead: this works without errors.
i use the code below to generate graph.pb and ckpy file `g = tf.get_default_graph() graph_def = g.as_graph_def() "./model", 'graph.pb ', as_text=false) saver = tf.train.saver() saver.save(sess, and then i freezed the graph.pb successfullly python freeze_graph.py --input_binary=true
without one of them or both the converting process crashes.
1. make a copy and rename it to qa.
the updated netrw would instead try to curl it, which seems wrong.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): n/a (gcp) tensorflow version (use command below): 1.13.0-rc0 python version: python 3.5.3 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: gpu model and memory:
however, the current transformer example is not working anymore.
the custom op makes training ~3x faster, compared to using something like subwordtextencoder phofhyperlink .
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): official site tensorflow version (use command below): 1.13.1 python version: 3.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.1 gpu model and memory:
the first code block on: phofurl gives an error when it tries to execute: phofcode `syntaxerror: invalid syntax`
these ops are not constant folded.
phofcode if i change the `dataset_evens = tf.int64)` to the place after `dataset_range = tf.int64)`, it also will be ok.
detailed steps to reproduce the behavior: 1. create a new vimscript with following content: phofcode 2. save and source the vimscript with `:w test.vim | source %` 3. as indicated by the output is the return value of filereadable _true_ for first and _false_ for second given path.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
in 2.0.0-rc0, while using model subclassing of tf,keras.model and passing `dynamic=true`.
import tensorflow as tf a = b = minval=-1, maxval=1, dtype=tf.dtypes.int32)) value = print(a.numpy) print(b.numpy) @tf.function def run(): for j in tf.range(10): var = tf.gather(a, j) tf.print(var) var_int tf.gather(b, j) tf.print(var_int) run()
gpudelegate should load correctly and run inference on my android devices.
it will switch the to-be-imported component from `child1` to `child2` (both of which dynamically import `subchild`)
i used to quantized trainning a cnn network, , and used freeze_graph.py to generate frozen pb file successfully but when i used toco to convert the pb to tflite ,,error occured shown below array slice, which is an input the conv operator producing the output array conv1/relu, is lacking min/max data, which is necessary for quantization.
scroll to top, select first character.
with and ``comments=fb:..`` for ``rst`` text (which seems to be correct ``comments`` settings imho) both vim and neovim doesnt indent the comments, so instead of phofcode i get phofcode moreover, vim is rather weird state, where i cannot insert space before the arrives word (just nothing happens), and even when i force there space with ``ctrl-v<space>`` it gets removed on next ``gqap``.
a clear and concise description of what the bug is.
tfprof unable to parse the profile generated by profilecontext.
`:call appendbufline( ' ', 0, [1, 2])` <img width="1680" alt=" 59 38" src=" phofurl line 3 is the unwanted blank lines.
if applicable, copy/paste the text or add screenshots to help explain your problem.
i am wondering if the model is not being put onto gpu and is kept on cpu when trained this way.
i want to continue using static graphs and monitoredsession to train my models (since i do have all the input pipelines defined with tf.data.dataset and all the training script defined to work in this way).
1499 runtimeerror: is not supported when eager execution is enabled
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: pixel 2 - tensorflow installed from (source or binary): binary tensorflow version (use command below): r.12 python version: 3.5 bazel version (if compiling from source): 0.21 gcc/compiler version (if compiling from source): apple llvm version 9.1.0 cuda/cudnn version: 10.1 gpu model and memory: nvidia 2080ti error logs: e/tflite: model provided has model identifier ' x14 e/audio-app: failed to create flatbuffermodel with file:
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- repl showing the issue phofhyperlink against `3.13.0.-alpha.2` - same repl working phofhyperlink against `3.13.0.aplha.1` - same repl working phofhyperlink against `3.12.1`.
4. try to add code from
the data is read earlier in the notebook from with tensorflow 2.0.0 and tensorflow.keras i get a loss of and val_loss: 1.1059 after first epoch and similar figures after subsequent epochs.
if an unhandled exception occurs during the first call the second call is still made.
the matrix multiplication should complete successfully.
i have written a custom estimator and wanted to train it on 2 gpus using submitting this job phofhyperlink to the ai platform.
the following leaves the command line window open.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): colab - tensorflow installed from (source or binary): source using `pip` - tensorflow version (use command below): 2.0.0-alpha0 python version: 3.6 include any logs or source code that would be helpful to diagnose the problem.
tensorflow should always detect all attached gpus, even on subsequent runs.
it is supported by underlying code and mentioned by the depthwiseconv2d documentation, but is missing from depthwiseconv2d.
`tf_upgrade_v2` does not fails if the file contains f-strings
all cpu core 's usage is below 30% and gpus usage are around 100% equally for all gpus while training.
create a `subcom.svelte` and add this code: phofcode now in `app.svelte` add this code: phofcode you should see the error.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - tensorflow version: 2.0.0-alpha0 - python version: 3.6.7
when collecting batch-level metrics, epochs should be numbered with the epoch number
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - mobile device (e.g.
a custom, deep convolutional model, with batch normalization, does not converge.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): source tensorflow version (use command below): python version: 2.7 bazel version (if compiling from source): 0.24.1 gcc/compiler version (if compiling from source): 5.4 cuda/cudnn version: 10/7 gpu model and memory: 8 v100s n/a /cc @azaks2 @tfboyd @nluehr @benbarsdell @mattconley
use the tool saved_model_cli to covert a severable model with tf-trt: ~~~ saved_model_cli convert --dir --output_dir "/path/to/trt-mask-rcnn" --tag_set serve tensorrt --precision_mode fp32 --max_batch_size 32 --is_dynamic_op true ~~~ ## while i got a frozen_graph model and the variables fold is empty, which is not severable!
whether default or reverse control dependency is used between layers should not have any implications on the gradients seen in dummy c++ op for a given layer, since we are running a predictable model, with same datasets.
the man page for `ld` says following: phofcode i believe that correct format for this flag is (since linker will prepend `lib` and append `.dylib`).
but it 's working incorrectly in tf 2.0. phofurl python value = 1, activation=none) self.value = tf.identity(value, name="value_estimate") ``
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): preinstalled in docker image tensorflow version (use command below): 1.14.0 python version: 3.6.8 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: i 've tested this using official tf docker image and also using python docker image (python:3.6) with tensorflow installed with pip
i called the operator from the following python code.
i am using tensorflow 2 on a cluster which limits the gpu hours per job.
phofcode traceback warning:tensorflow:from test.py:33: dynamic_rnn (from is deprecated and will be removed in a future version.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
`mkdir()` should throw an exception if the path exists, like the python built in `os.mkdir()` phofhyperlink does.
instead, the optimizer seems to "cache" value of variable at time when optimizer was constructed.
3. look a the rendered swagger-ui on the right.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): bug was discovered when writing a custom script that used `tensorforestestimator`.
the weights should restore and not run into an error.
wget phofurl ---- phofurl resolving github.com (github.com)... connecting to github.com connected.
3. do `:set formatoptions+=al` 4. enter insert mode anywhere on the line entered in step 2 and type anything.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 python version: 3.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 9.0 gpu model and memory: geforce gtx 1070 with max-q design you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)"
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary(pip) tensorflow version (use command below): 1.13.1 python version: 3.5.2 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: cuda 10, cudnn 7.5 gpu model and memory: gtx 1050ti, 4gb this only happens in tf 1.13 and not in 1.12. i also tried 2.0 alpha and the bug is still presen
in my case, it would be <=2021.5 mb.
parameters from [*] load success testing... an exception has occurred, use %tb to see the full traceback.
`go test` fails with following error mesg: phofcode
when passed to different function, tf.variable dtype doesn't cast to _ref type.
it possible have graph tensors leak out of function building context by including tf.init_scope your function building code.
tried very simple transfer learning on inception v3.
phofcode you can try removing the `call()` method, it will fail.
moreover, there 's another strange behavour: when i define a model output passing a new input (hence i invoke `model.call` method) update operation have no idea of this new input.
but that can 't be right, because you can invoke `win_execute()` to execute an arbitrary command in a squashed window, without height of latter being reset to `1`: vim -nu none + 'set wmh=0|sp|wincmd _|call "echo 123") \' notice how bottom window is still squashed, even though it has been temporarily focused by `win_execute()`.
args: images: a tensor of shape (num_images, num_rows, num_columns, num_channels) (nhwc), (num_rows, num_columns, num_channels) (hwc), or (num_rows, num_columns) (hw).
if including tracebacks, please include the full traceback.
see code checked here for a benchmark: phofurl
phofcode the base random seed is non-zero, it's supposed to produce the same result.
---> 65 functions = 66 signature = none) 67 if signature is not none: list_functions(self, obj) 109 obj_functions self._functions.get(obj, none) 110 if obj_functions is none: --> 111 obj_functions # pylint: disable=protected-access 112 self._functions[obj] obj_functions 113 return 1793 def 1794 return { -> 1795 1796 } 1797 trace_model_call(model, input_signature) 76 for input_tensor, input_name zip(inputs, input_names): 77 ---> 78 79 name=input_name)) 80 # the input signature of call function a list with one element, since attributeerror: 'str ' object has no attribute 'shape ' ``
this is the case when both tensors have the same rank or the rank of the sparse tensor is less than rank of dense tensor, but not when rank of sparse tensor is greater.
detailed steps to reproduce the behavior: 1. run `vim --clean` (or `gvim --clean`, etc.)
- os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - tensorflow installed from (source or binary): pip - tensorflow version (use command below): 2.0.0-alpha0 python version: python 3.7.3 cuda/cudnn version: cuda10.0 not relevant.
i expected get_available_gpus() to return the empty list when none of them is available.
i got this "todo" error when i wrote something like `export let x = a?.b`.
code to save the hub module to a savedmodel: phofcode bash command to convert tensorrt model: phofcode
nvidia-smi shows a usage of ~2500 mb for a batch size of 227.
take any saved_model that contains a function returning a dict.
on a 10 core machine $ bazel test --config=opt --cache_test_results=no -- $ cat
reduced to the minimum: phofcode
large logs and files should be attached.
gpu memory should be controled
numpy operations on a list of tensor is considerably slow.
where for atomic assignments mean single value tensor assignments.
eventually i start getting warning about gpu memory usage and start getting oom errors.
phofcode given that interpreter_ contains resizeops with -1 specified.
it indents like this: string[] defs = finddefs( new string[] { "postprocessing.editor", unwanted indentation "recorder", }); the current indentation is controlled by the cino-jn (anonymous java functions) option.
searches in such small help window are affected by this.
i have tried multiple tensorflow versions and results are same.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
however, have not yet investigated reproducing issue on other types estimators.
algo running on: linux mint 19.2 (virtualized: oracle) zip file created: +0800 python 2.7.15+ runtime variables: algo_provider "vultr" algo_ondemand_cellular "false" algo_ondemand_wifi "false" "x251bgw=" algo_dns_adblocking "true" algo_ssh_tunneling "false" wireguard_enabled "true" dns_encryption "true" phofcode play [localhost]
according to phofurl this would appear to be the way to log scalar values in tensorboard 2.0. am i incorrect?
after searching the historical records, i found this pr #14251 which has introduced this test case.
this could mean that the was uninitialized.
- have i written custom code (as opposed to using a stock example script provided in tensorflow):no - os platform and distribution (e.g., linux ubuntu 16.04):unknown - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code: yes - os platform and distribution: archlinux - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12.0 python version: 3.7.1 cuda/cudnn version: gpu model and memory: 11gb gtx 1080 ti log output (abbreviated) ~~~ # run 1 ... info:tensorflow:calling model_fn.
dimension of final output for vgg is [?, 3] dimension of final output for yolo is [?, 1, 1, 3] `softmax = labels=y, dim=-1)` print(softmax) run 1 vgg output [ 0. after optimization [ 0.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
phofurl a green "tap here!"
just add some sleep in the python code or the rdma.cc file
because of the identical implementations, besides the difference in the way the model is built (subclassing and functional api), i would expect results to be same.
(or paste the result of `vim --version`.)
please see the short example test case here: phofurl in particular, see: cell 4 (random noise image, 4800 unique values) vs. cell 6 (image after conversion to pil and back, 256 unique values)
image phofimage - vim version [e.g.
- os platform and distribution (e.g., linux ubuntu 16.04): mac os mojave - tensorflow installed from (source or binary): pip - tensorflow version (use command below): 2.0.0 alpha-0 - python version: 3.7.2
when using the metric with an estimator, a `typeerror` is raised when calling the `evaluate` method on the estimator if the evaluation is configured to be distributed with the evaluation is successful if estimator is not configured for distributed evaluation.
2. edit `filename` 3. type '....' 4. describe the error
open repl phofhyperlink i created the simplest custom element that do `console.log` of attribute `prefix` when connected.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
for this sample image sample image phofimage one of labels sample label phofimage after 300 epochs, same loss optimizer, predicted labels - with a pure `tf.keras` implementation: phofimage - with `tf.keras` model converted using phofimage - `tf.keras` model used in `tf.estimator` model function: have tried `tf.keras`-in-`model_fn` setup on 10 000 images for 30-50 epochs, results are much worse than this, which is itself not perfect.
vim outputs `['not from heredoc']`.
for i in range(num_inputs): # convolve input tensors to output tensors of same channel depth (# of channels) conv_layer = kernel_size, strides=strides, padding=padding) setattr(self, f'conv_{i}', conv_layer) # deconv input tensors.
algo fails installing in debian on scaleway.
i messed with it once and i got 49k true positive (which makes total sense).
if i create a layer and call it on some input, everything works as expected.
to confirm that xla is active, pass (as a proper command-line flag, not via tf_xla_flags) or set the envvar w0603 deprecation.py:323] from checkpoint_exists (from is deprecated and will be removed in a future version.
is inconsistent with its documentation and `tf.gradients` when computing gradients with respect to tensors.
- vim version: - os: linux - terminal: i find the value of `popup_getoptions(id)` is `{ 'highlight ': ' ', 'mousemoved ': [0, 0, 0], 'line ': 0, 'close ': 'none ', 'time ': 0, 'drag ': 0, 'cursorline ': 'minheight ': 'wrap ': 1, 'pos ': 'topleft ', 'minwidth ': 'fixed ': 'col ': 'maxwidth ': 'zindex ': 50, 'firstline ': 'title ': ' ', 'scrollbar ': 1, 'moved ': [0, 0], 'mapping ': 1, 'tabpage ': 'resize ': 'maxheight ': 20}`, if remove ` 'mousemoved ': [0, 0]` and ` 'moved ': [0, 0]`, the errors disappear.
17 # pylint: disable=wildcard-import 18 from tensorflow_transform import coders ---> 19 from import * 20 from tensorflow_transform.api import apply_function 21 from import * in <module> 42 43 ---> 44 gen_quantile_ops 45 quantile_ops 46 tensorflow.python.ops resources modulenotfounderror: no module named 'tensorflow.contrib '
my graph has many nodes that are supported by tf-trt phofhyperlink yet none are simplified into a trtengineop.
i can reproduce the ticket with for example with terminal that has 58 lines & 203 columns.
there 're still some crash cases with nested map/filter functions.
if including tracebacks, please include the full traceback.
this is my uname -a from the droplet i 'm running the scripts on (env) algo git:(master) uname -a linux gdangus #167-ubuntu smp wed dec 5 utc 2018 x86_64 x86_64 x86_64 gnu/linux (env) algo git:(master) ./algo play [ask user for the input]
the code below should not raise the `assertionerror`: python import tensorflow as tf class def __init__(self, cell): super(cellwrapper, self).__init__() self.cell = cell @property def state_size(self): return self.cell.state_size @property def output_size(self): return self.cell.output_size def get_initial_state(self, inputs=none, batch_size=none, dtype=none): return inputs=inputs, batch_size=batch_size, dtype=dtype) call(self, inputs, states, training=none,
running on gpu should be much faster than on cpu
my problem will only occur with runoptions.
in windows, with general protection error.
i've tried to create a trt model using both of the following functions: phofcode and phofcode in both cases, the tf-trt model is about 35x slower (20ms vs 700ms inference).
on tensorboard queuedequeuev2 operation's device is gpu:1. image phofimage but on timeline, it is cpu:0 image phofimage
phofcode algo running on: ubuntu 16.04.5 lts created from git fork.
1. if a user provides `params`, then the `tpuestimator` adds the `tpucontext` to `params` by calling phofurl this triggers the `add_param` or `set_param` methods.
phofcode for me, the output is: python version 3.6.8 tensorflow version 1.13.1 numpy version 1.16.2 i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2
1. run `vim --clean -s reproduce.vim` phofcode
i define a hook class which is inherited from sessionrunhook, and add an assign operation in the begin() function.
because the only non-zero entry is outside the window used to compute the value, then this is most likely a buffer overflow (or some other memory access issue).
if there is no explicit usage of the particular op, it should not be listed in file(.pbtxt
the example code should work without modifications.
however, compiling a model with an instance of results in the following shape error when calling `model.fit()` afterwards: phofcode
labels are converted to 1 and 0 as below: phofcode the data are split into train_df and test_df then created an input_fn for training.
model should not corrupt inputs shape.
at step 8. above, i expected the text properties to be retained for the buffer, after navigating away and back again.
all is as to be expected (save slight difference in output tensor name).
229 model_config = --> 230 = 231 232 # set weights custom_objects) 308 309 import deserialize # pylint: --> 310 return deserialize(config, 311 312 deserialize(config, custom_objects) 62 module_objects=globs, 63 ---> 64 module_objects, custom_objects, printable_module_name) 171 custom_objects=dict( 172 + --> 173 174 with 175 return etwork.py from_config(cls, config, custom_objects) 1300 if layer unprocessed_nodes: 1301 for node_data -> 1302 process_node(layer, node_data) 1303 1304 name config.get( 'name ') etwork.py process_node(layer, node_data) 1258 if input_tensors: 1259 len(input_tensors) == 1: -> 1260 layer(input_tensors[0],
the updates should be placed on the default graph.
relevant part of my traceback is below file line 61, in <lambda> predictions = file line 289, in from_tensor_slices return file line 1565, in __init__ for i, t in file line 1565, <listcomp> for i, t 1050, convert_to_tensor as_ref=false) 1106, raise runtimeerror("attempting to capture an eagertensor without " runtimeerror: attempting to capture an eagertensor without building a function.
i 've created repo phofhyperlink where you can find * pipeline.config - one step training, the same tfrecord for training and evaluation * classes.pbtxt - two sample classes * t.tfrecord - containg two white images: one with 100 bounding boxes and second with 200 bounding boxes * notebook-images-view.png - view from notebook when i 've tried visualise bboxes tensorboard images view
replacing mirroredstrategy by parameterserverstrategy launches a training with cpu acting as ps and 2 gpu workers.
large logs and files should be attached.
tpu training should run similarly to gpu training (without errors in this case)
i expect to get an array of labeled probabilities, but the app crashes instead.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
phofcode outputs: phofcode (ideally it should always say `number of uncollected graphs: 0`)
it seems setup time increases more than evaluation - reproduction example below has very simple data model, you can still see this behavior that.
instructions for updating: use traceback (most recent call last): file line 58, <module> tflite_model = converter.convert() file line 455, convert
<img width="428" alt="screenshot at 11 22 36" src=" phofurl <img width="1680" alt="screenshot at 11 22 21" src=" phofurl stack trace: phofcode i got asan output too, but it 's hard to capture due to resizing of the terimal windows.
it should raise an exception for incorrect `class_weight` keys.
it throws this error i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 warning: logging before flag parsing goes to stderr.
any modification should be explicitly documented
when fitting, the metric is always greater than the loss, whereas loss is computed as mean squared error plus a custom error defined as a kl divergence in model.
if ec2 mfa is to be implemented, a new feature request needs to be created.
playbook for updating user should work behind strict firewall.
according to the docs/examples, i would expect that to be `[(none, 2)]`.
any suggestions to work around?
in the self-contained test below, i am expecting to initialize `my_dense/dense/kernel` and `my_dense/dense/bias`, which should be checkpointable, but it does not, as can be seen from the error message.
the same problem doesn 't happen if the number is not inside a dictionary
rnn layers have poor performance and low gpu usage when used with `mirroredstrategy`.
if all the outputs of that lambda layer are stored into a single variable, passing that variable as output for model fails with `assertionerror: could not compute output tensor...` using index[0] gives me behavior i would have expected when receiving with no index specified.
the model does not run on localhost and the code returns an error with
phofcode phofcode code using c++ interpreter also reports duplicated outputs(2 2), even though outout of add(builtin code 0) shows one output.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
1. create `bug.vim` with the content `autocmd bufunload * redraws`.
the error seems to point a mismatch in the way embedding_lookup was defined in tensorflow serving.
run `vim --clean foo.vim -c "source %"` with phofcode output: phofcode
i 'm working on image segmentation, with 5 classes.
the same model works well when integrated with `tf.estimator` as i 'm trying to move from estimator to v2 custom loops
provide a reproducible test case that is the bare minimum necessary to generate the problem.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): arch linux - mobile device (e.g.
large logs and files should be attached.
appearance of selection should not be there
1. run `gvim -u none -u none` 2. set the following options.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 python version: 2.7 bazel version (if compiling from source): no gcc/compiler version (if compiling from source): no cuda/cudnn version: gpu model and memory: typeerror traceback (most recent call last) in <module>() 7 dataset = 8 ----> 9 it = in 178 """ 179 if --> 180 return 181 # note(mrry): we capture by value here to ensure that `_make_dataset()` is 182 # a 0-argument function.
phofcode running this code gives the result: uint32 uint32 [0 0 0 0 0 0] [0 0] 1.14.0-rc1 ``
during handling the above exception, another exception occurred: valueerror traceback (most recent call last) in _maybecompile(scope, op, func, grad_fn) 414 try: --> 415 xla_compile = 416 = op.get_attr( in get_attr(self, name) 2412 # convert to valueerror for backwards compatibility.
and the same situation happens when run tensorflow neural machine translation tutorial phofhyperlink , which is the reason why tested this simple code.
the code is massive and i cannot tell where the error is.
the `x` character is inserted in the buffer.
my instantiation looks like: phofcode
however, it does not work - in fact, there seems be little no difference comparing a version with without a final `.prefetch(x)` in data pipeline.
this causes critical problems for applications that require stdout to transfer data.
it should print "geforce rtx 2080 with max-q design" (as pytorch does).
`tf.einsum` returns buggy values compared to `np.einsum` for identical parameters.
def tgt_h, tgt_w): # get the image name list input_dir = # a path to some image dataset if (input_dir == 'none'): raise valueerror('input directory is not provided') if not raise valueerror('input directory not found') image_list_lr_temp = os.listdir(input_dir) image_list_lr = [os.path.join(input_dir, _) for _ in image_list_lr_temp if _.split('.
graph = tf.graph() with with graph.as_default(): sign_in = name='signal_in') conv = tf.keras.layers.conv2d( 10, (10,2), padding='valid', name='conv_linear', use_bias=true, )(sign_in) data_tensor = feed_dict { } op_value feed_dict=feed_dict)
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary from pypi tensorflow version (use command below): 2.0 alpha0 python version: 3.6.7 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.0 gpu model and memory: gtx1070 8gb
either don 't produce a sigsegv but a more meaningful error message or simply don 't fail at all.
929, run run_metadata_ptr) 1152, _run feed_dict_tensor, options, run_metadata) 1328, _do_run run_metadata) 1348, _do_call raise type(e)(node_def, op, message) error while reading resource variable my_dense/dense/kernel container: localhost.
i 'm not printing anything because calling `tf.print` results in a syntax error on colab and i know that these snippets are being run on colab by you.
cuda [y/n]: n cuda download a fresh release clang?
keras model generates `model.output_names` here phofurl for outputs layers with multiple outputs, all outputs will have the same name!
however, programs run as expected.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): source tensorflow version (use command below): 2.0.0-alpha0 python version: 3.7 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory:
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os 10.13.6 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): both 1.13.0rc2 and tf-nightly python version: 3.6.8 cuda/cudnn version: no gpu gpu model and memory: no gpu
i am running the sample rnnlm code phofhyperlink .
both should be consistent (and accepting a string would provide the simplest api).
provide a reproducible test case that is the bare minimum necessary to generate the problem.
no cpu memory leak when training models on gpu device using 'tf.function '.
this should be able to reproduce my issue: phofcode
large logs and files should be attached.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no, just enabled xla jit compilation.
function get_operations() from class graph() is not returning the operations from a namescope enclosing the tensorflow operations from respective namescope but is returning the operations for all the graph
#### log on my gcp instance phofcode #### log on gcolab phofcode
)`) when no l1/l2 is set (while otherwise summing weights and applying l1/l2)
4. now try to request directly with rest: phofurl it is lighting fast while returning everything.
detailed steps to reproduce the behavior: 1. run vim in putty terminal 2. use :term to open a terminal 3. int t mode , input up or down key 4. can 't get last of next shell cmd in shell use message cmd in vim cmd line and get the following log e21: cannot make changes, 'modifiable ' is off
it appears that the presentation width of a tab character is not used in the width calculation of a popup, hence popup contents appear clipped: clipped phofimage
hi i 'm trying to detect more than 10 objects in the image ( which is default ) i 'm usin the following commands: bazel run -c opt -- --inference_type=float --max_detections=500 --allow_custom_ops i also modified 500 <--- instead of 10, 'maximum number of detections (boxes) to show. ')
the following values should be updated for the now hidden buffer: - `'hidden'` should be `1` - `'windows'` should be an empty list
i expect tensorflow to find that my gpu (or at least my cpu) is a valid device to execute a batchnorm operation on.
', loss) 356 return self in _train_model(self, input_fn, hooks, saving_listeners) 1181 return hooks, saving_listeners) 1182 else: -> 1183 return 1184 1185 def input_fn, saving_listeners): input_fn, 1211 1212 estimator_spec = self._call_model_fn( -> 1213 features, labels, self.config) 1214 global_step_tensor = 1215 return worker_hooks, _call_model_fn(self, features, labels, mode, config) 1169 1170 logging.info( 'calling model_fn. ')
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
note: you might have to run the code multiple times.
like first line, because it 's more readable.
only keep_checkpoint_max checkpoints should be saved
i suspect that tf2.0 alpha 's gru 's call() is changed in tf2.0 beta version.
release compiled model when i don 't need it anymore
` protected void runinference() { log.d(tag, "inference starts "); object[] inputarray = {imgdata}; map<integer, object> outputmap = new hashmap<>(); outputmap.put(0, outp1); outputmap.put(1, outp2); outputmap); log.d(tag, "inference ends "); }`
normal behavior as expected from the `train_and_evaluate` function
report the actual error, provide mechanism to catch the exception and handle it.
i have remapped my movement keys `hjkl` to `jkl` with langmap.
is this expected in tensorflow 2.0 or a bug?.
5. run the code with beta0 or beta1 choose version phofimage 6. make note of training history plot.
when the header field is empty no header should be sent since it is configured as optional
on windows 10 2. edit any existing file from a remote machine, with command such as phofcode 3. the underlining scp command that netrw is using appears to be something like: phofcode
a co-worker was able load standard (unmodified) with tf.keras.
large logs and files should be attached
#13 _pyeval_evalframedefault #14 #15 #16 #17 _pyeval_evalframedefault #18 #19 #20 #21 _pyeval_evalframedefault #22 #23 pyeval_evalcode #24 #25 #26 #27 pyrun_anyfileexflags #28 py_main #29 main
1. go to ' phofurl 2. click on '+ add new user' 3. create the user by adding username and password and checking confirmed '....' 4. add the role authenticated 5. wait for server to restart 6. then logout of the admin panel and try to log in with the credentials you created 7. receive an error screenshot from phofimage
steps to reproduce the behavior: 1. download master.zip per readme on 2. edit config.cfg to taste.
6. re-run with an alpha of `ffff` and observe that it resolves the issue 7. now try `vim --clean` (without clearing `t_rc`), it's back :(
when pressing `$` to move the cursor on the last character of the line, the cursor jumps beyond latter if some text is concealed on line.
written on `:h completion-function` too.
could you please show how * exactly * you generated mobilenet_v1 tflite model found in your tutorials, since it works like a charm with gpudelegate: ~80ms per single-image inference on gpu several hundreds seconds on cpu on sufficiently recent android.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.13.6 - mobile device (e.g.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): 16.04, docker hub tag: 1.12.0-gpu-py3 - mobile device (e.g.
using keras validation data and will fail with ` 'batchdataset ' object is not subscriptable` in tf 2.0 nightly.
17/100 4:00 w allocator (gpu_0_bfc) ran out trying allocate 219.70mib.
2. edit any file 3. type phofcode then motion `$` on a line with text.
i tried running algo installer script on raspberry pi running ubuntu.
instructions updating: use standard file apis to check files this prefix.
especially considering that `tf.exp()` is available.
in the above example `for i in y:print(i.numpy())` should produce result without attributeerror.
if including tracebacks, please include the full traceback.
num_examples len(y_train) def make_input_fn(x, y, n_epochs=none, shuffle=true): def input_fn(): dataset y)) if shuffle: dataset for training, cycle thru dataset many times need (n_epochs=none).
divide integer tensors by assigning everything one worker and giving zeros everyone else?
provide a reproducible test case that is the bare minimum necessary to generate the problem.
detailed steps to reproduce the behavior: * `vim -nu none --clean --noplugin` * `:call popup_create( 'test ', { 'minwidth ': 100 } )` * resize the windows to a very small width crash seems to be reallocating the screen buffers:
this happens only when a buffer number is passed to `popup_create()`.
i 've got a little `not included in path` log when installing, though.
1. register account at bithost or bitlaunch 2. setup algo on digital ocean or vultr server 3. do localhost provision 4. use
warning:tensorflow:tried colocate with an op tower0/cond/prod_3 that had a different device: /device:cpu:0 vs /device:gpu:0. postponing error-checking until all devices are assigned.
fails with the following error: phofcode
the only value that matters is the first one (when function gets traced).
python import os import sys import tensorflow as tf import time n = 8192 dtype = tf.float32 matrix1 = tf.variable(tf.ones((n, n), dtype=dtype)) matrix2 = tf.variable(tf.ones((n, n), dtype=dtype)) product tf.matmul(matrix1, matrix2) # avoid optimizing away redundant nodes config sess os sys tensorflow as tf time n 8192 dtype tf.float32 matrix1 tf.variable(tf.ones((n, n), dtype=dtype)) matrix2 tf.variable(tf.ones((n, n), dtype=dtype)) product tf.matmul(matrix1, matrix2) # avoid optimizing away redundant nodes config sess iters 10 # pre-warming sess.run(product.op) start time.time() for i in range(iters): sess.run(product.op) end time.time() ops n
the server url used in fetching an oauth2 token is the one shown in the `server` dropdown.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: 1+6t - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: 3.6 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: 10.0 gpu model and memory: gtx 1050 ti the facenet has it own loss function, so i set compile=false in the tflite conventor source code.
or run gen_test_train_data.py on windows, run train_model.py on windows, transfer the model the rpi, then model_visual_test.py it.
that code should print identical arrays (the latter is correct, the former is not).
so changed dilated convolution to a native implementation from tensorflow-wavenet phofhyperlink .
error message: in converted code: * return layer_call(inputs), test_signature.py:32 call * return self.inner(x, dummy=dummy), self.inner(x_small, dummy=dummy) __call__ outputs = call_fn(inputs, *args,
this used to work with smaller set of embeddings.
tensorflow raises a typeerror when creating a dynamic_rnn with tf.int32 type in its input and state.
i followed the guide: classify structured data phofhyperlink .
python import numpy as np import tensorflow as tf def input_fn(): x = np.random.random((1024, 10)) y = np.random.randint(2, size=(1024, 1)) x = tf.cast(x, tf.float32) dataset = y)) dataset dataset.shuffle(100) dataset dataset.batch(32) dataset dataset.repeat(10) def _extract_features(_x, _y): features { 'x': _x, 'z': tf.zeros_like(_x) } return features, _y return class def __init__(self): super(mymodel, self).__init__() self.features shape=(10,)) ]) self.dense1 activation='relu') self.dense2 tf.keras.layers.dense(1, activation='sigmoid') def call(self, inputs, training=none, mask=none): outputs self.features(inputs) outputs self.dense1(outputs) outputs self.dense2(outputs) return outputs model mymodel() # works model.run_eagerly true model.fit(input_fn()) # works model.run_eagerly false model.fit(input_fn()) # works model.run_eagerly true # fails model.run_eagerly false ``
provide a reproducible test case that is the bare minimum necessary to generate the problem.
however, size of string element is not fixed.
mobileconfig does not install on ios (iphone x); error that password is invalid despite using what was generated during the algo install.
also, i expected fp 16, to be faster than fp 32, (as is in normal conv).
in any case i would not expect the creation of the graph to slow down at each extra added layer.
if including tracebacks, please include the full traceback.
i created a tfkeras model and saved it to .h5 format using tf version 1.13.1. the model can be loaded and used for inference just fine in 1.13.1. after upgrading to tf 2.0 (nightly build), loading the model results in a valueerror (see traceback below).
node: {{node fusedbatchnorm}} = fusedbatchnorm t=dt_double, data_format="nhwc", epsilon=0.001, is_training=false phofhyperlink all kernels registered for op fusedbatchnorm : device= 'xla_gpu '; t in [dt_float] device= 'xla_cpu '; t in [dt_float] t [dt_float] t [dt_float] device= 'cpu '; device= 'gpu '; [op:fusedbatchnorm] $
- have i written custom code: yes - os platform and distribution: ubuntu 16.04 - mobile device: not tested - tensorflow installed from: binary tensorflow version: 1.13.1 python version: 3.7.3 bazel version: gcc/compiler version: cuda/cudnn gpu model and memory: nvidia, driver 418.56, 11178mib
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the "predict" signature definition is lost when running the `saved_model_cli convert` command against a resnet model.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): debian linux 9 (m26 deep learning image) running on google cloud - tensorflow installed from (source or binary): binary (using the one provided with the deep learning image - tensorflow version (use command below): 1.13.1 python version: 3.5.3 cuda/cudnn version: cuda 10.0 / cudnn 7.4.1 gpu model and memory: nvidia tesla v100
the pytorch is: torch==1.0.1.post2 from executing code above $ python test.py i your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 fma i successfully opened dynamic library libcuda.so.1 i xla service executing computations on platform cuda.
sometimes, during training process , i get this error: e error polling for event status: failed to query event: cuda_error_unknown: unknown error f unexpected event status: 1 this happens sometimes when i am not connected to that machine.
detailed steps to reproduce the behavior: 1. run `vim --clean` (or `gvim --clean`, etc.)
i loaded the model as below: model = 299, 3), include_top=true, weights=none, classes=category_size) optimizer = model.summary() epochs=200, callbacks=[], shuffle=true, validation_data test_generator) # !!!
from tensorflow.keras.models import sequential from import modelcheckpoint, reducelronplateau, earlystopping model= sequential() ... earlystopping = monitor= 'val_loss ', verbose=1, checkpoint = {epoch:02d} {val_loss:.3f} {val_acc:.3f} {loss:.3f} {acc:.3f}.h5 ', monitor= 'val_loss ', verbose=1, save_best_only=true) reduce_lr_loss = monitor= 'val_loss ', verbose=1) callbacks = [earlystopping, checkpoint, reduce_lr_loss] tmodel.fit(x_train, y_train, batch_size=32, epochs=1000, validation_data=(x_test, y_test), callbacks=callbacks, verbose=1)
- os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 2.0.0-alpha0 - python version: python 3.5 cuda/cudnn version: 10. gpu model and memory: titan
what i 've notice is that without the `concatenate` op allocation works correctly.
the expected behavior is that the model performs the inference without any need to be compiled and returns the result.
defining the same model using the sequential api gives no errors.
@registry.py:133] group0/block0/conv1 output: @registry.py:125] group0/block0/conv2 input: @batch_norm.py:164] wrn [batchnorm] using in training.
2. run this shell command: $ vim -nu <(cat << 'eof ' cno <plug>(up) <up> nmap <expr> * func() fu!
i 've tried converting to .tflite with tf-gpu versions 1.14, 1.13, nightly 1.14 and tf-2.0.0-beta1
getting the error: `valueerror: unrecognized `embedding` layer names passed to `embeddings_metadata` argument: when passing embeddings_metadata to
no distribution strategy will be used for evaluation.
tensorflow matrix multiplication on cpu does not reproduce same result as numpy matrix multiplication for float32.
to keep consistency with other activation functions, it should return `nan` in all cases.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): rhel6 - mobile device (e.g.
keras.model.fit converts sparsetensors to tensors.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
steps to reproduce the behavior: 1.
instructions for updating: use tf.identity instead.
i use tflite model (quantization-aware training and fully quantized with toco) and deploy on android for segmentation task.
first repl where you see the expected behavior: phofurl load the page, hit the button, see the error message second repl that shows how i actually want my code to be: phofurl load page, hit button, no error message.
the suggested flags are appropriate: kmp_blocktime=0 kmp_settings=1 omp_num_threads=32 any ideas how debug root cause and get maximum performance my models.
(1) the following code produces the same 'numpy_data0.pkl ', 'loss0.pkl ' all the times (which means same data, same parameter, same loss), but 'grad0.pkl ' changes.
- have i written custom code: yes - os platform and distribution: linux ubuntu 18.04 - mobile device: n/a - tensorflow installed from: binary tensorflow version: 2.0.0-alpha0, python version: 3.6.7 bazel version: n/a gcc/compiler version: n/a cuda/cudnn 10.1 / 7.4.2 gpu model and memory: geforce gtx 1080 ti (11 gb)
furthermore the error doesn 't specify anything regarding cuda 10.1 not being supported.
large logs and files should be attached.
-iproto -dhave_config_h -d_fortify_source=2 -march=x86-64 -mtune=generic -o2 -pipe -fno-plt -u_fortify_source -d_fortify_source=1 enlazado: gcc -l. -fstack-protector-strong -rdynamic -wl,-export-dynamic -wl,-e -l/usr/local/lib -wl,--as-needed -o vim -lm -ltinfo -lelf -lnsl -lacl -lattr -lgpm -ldl -wl,-e -fstack-protector-strong -l/usr/local/lib -lperl -lpthread -ldl -lm -lcrypt -l util -lc -l/usr/lib -ltclstub8.6 -ldl -lz -lpthread -lm - os: i test in archlinux, debian buster 10.1 and gvim 8.1 in win10.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 18.04 - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
the code should calculate gradient properly without error.
unfortunately, sum is used in the gradient of tf.add phofhyperlink , if non-static shapes are used.
the distributed training is conducted with one ps and three workers.
taking only the data pipeline part and skipping all shuffling, preprocessing, etcetera i am left with this as a minimal working example.
run predrnn.py in phofurl modify the "model_creation_device" to reproduce issues.
node trtengineop_0 added for segment consisting 486 nodes succeeded.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
requires go 1.12.2 phofhyperlink .
i modified the bert network to do fp16 math and compiled with xla.
3. now try to run `:!xxd`.
i 'd like to set keras model 's run_eagerly property to true so that i 'd be able to step into custom-defined loss functions when being in eager mode when using sgd as an optimiser.
1. create a json field 2. add json schema 3. save 4. get error
i only got to run the demo app on gpu with gpu experimental but not the object detection app.
if keep it then press c-s again, there 's no more it and save sccessfully.
notice how the cursor jumps to the end of input control.
when convert freezed pb to tflite, the following problem happened: valueerror: nodedef mentions attr 'half_pixel_centers ' not in op<name=resizebilinear; signature=images:t, size:int32 -> resized_images:float; dt_uint8, dt_int16, dt_uint16, dt_int32, dt_int64, dt_bfloat16, dt_half, dt_float, dt_double]; nodedef: {{node resizebilinear}}.
the goals of this quantization are: 1. speed up inference 2. decrease model size as quantization and conversion proceeds from i expect inference time to decrease (fps to increase), and model size to decrease.
vimsubst highlighting is selected when ` 's '` appears in an array in a function.
1. run `vim --clean -c "source popup.vim"` phofcode 2. try to close the popup window by clicking with the mouse on the `x`.
different runs of the test program produce different output.
successful config did not happen.
if including tracebacks, please include the full traceback.
i tried to regenerate it myself from to build my custom operators, but tf.load_op_library throws the exception and does not find the library given in argument.
reinitializing tpu can cause previously created variables on tpu to be lost.
attempt to install algo on macos 10.15.1
replacing `return tf.linalg.expm(x)` in the following code example with e.g.
original bug report: phofurl using tf 2.0.0-rc0 and constructing a simple keras model fails to train.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac os - mobile device (e.g.
fatal python error: initfsencoding: unable to load file system codec modulenotfounderror: no module named 'encodings ' but i input "py3 print( 'hello world \')" when i opened vim and delete "py3 import os;import ", it show "hello world " , i think problem is python3 is not init finish when vim open vimrc file .
if i download via this link, i get a corrupt file.
making predictions immediately after training works too.
if including tracebacks, please include the full traceback.
if including tracebacks, please include the full traceback.
the use of `feedkeys` with `<cursorhold>` to work reliably regardless of environment.
if including tracebacks, please include the full traceback.
if including tracebacks, please include the full traceback.
but note this is also a problem when training, so i don 't think using an untrained model here is related to the issue.
i used this simple cnn to detect the eye region: phofurl thanks.
i 've downloaded an inception model from tf-hub (specifically this one: phofurl i have added to it two keras layers (a dropout layer and a dense layer) and during the training, i 'm trying to save the model using the `modelcheckpoint` keras callback.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): tf.version.version: tf.version.git_version: python version: 3.6.8 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: here is the stacktrace: pycon traceback (most recent call last): file "<stdin>", line 1, in <module> file line 248, in load_model file line 456, in _method_wrapper method(self, *args,
code to reproduce this issue can be found on the related stackoverflow page: phofurl another good reproduction of this since i'm using `attention()` layers is found here: phofurl
i 'm trying to train my own deeplab model using this code phofhyperlink and convert it to tflite.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
large logs and files should be attached.
this only works when `wildignore` does not contain the folder.
i expect a different concrete function to be returned when i pass it [1.0], but this second concrete function should be the same as for the arguments [2.0], or tf.constant([3.0]), tf.constant([4.0]) np.array([5.0]), np.array([6.0])
if including tracebacks, please include the full traceback.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: i don't know - tensorflow installed from: source tensorflow version: 1.12 python version: not used bazel version (if compiling from source): i don't know gcc/compiler version (if compiling from source): g++ 5.4.0 cuda/cudnn version: 10.1/libcudnn.so.7.5 gpu model and memory: geforce gtx 1080 ti 11175mib
also checked: 'conda config --show ssl_verify ' it showed accert.pem file path
task [common : generate p12 export password]
provide a reproducible test case that is the bare minimum necessary to generate the problem.
if including tracebacks, please include full traceback.
i can test the model on windows just fine using model_visual_test.py.
i am attempting to train the neural network in this repo, sketch2normal phofhyperlink every time i attempt to train the model, it always gives me an error message `segmentation fault (core dumped)` at epoch 0 sometime between 10/449 - 90/449.
gvim does support `focusgained` and `focuslost`, and when you run an ex command from gvim, whose output contains multiple lines, you can focus a different window without the command-line being cleared.
phofcode and here is my vgg_pr model: phofcode
if applicable, add screenshots to help explain your problem.
get no warning or make sure that this situation would affect result or performance.
... steps to reproduce the behavior: 1. save the above yaml to a file (i used unix line endings) 2. serve up the file with something like python 's simplehttpserver 3. use firefox to open the online swagger ui ( phofurl or a local copy of it, adding url query param 4. see error
self-attention gan is one example that i cannot run successfully.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.13.1 python version: 2.7.5 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: gpu model and memory: you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" '1.13.1 ') include any logs or source code that would be helpful to diagnose the problem.
1. go to create entry with date field 2. do not fill date field 3. save 4. check created entry
- have i written custom code (as opposed to using a stock example script provided in tensorflow): no - os platform and distribution (e.g., linux ubuntu 16.04): ubuntu 16.04 - mobile device (e.g.
i decorated the train step with `@tf.function`.
what the interesting thing is that in tf2.0-alpha, the bug existed already, but later in the problem seems to be solved.
- os platform and distribution ubuntu 18.04 lts - tensorflow installed from source or binary): source - tensorflow version (use command below): 2.0.0-alpha0 - python version: 3.5.7 bazel version (if compiling from source): from docker image gcc/compiler version (if compiling from source): from docker image cuda/cudnn version: na gpu model and memory: na
my input layer is sub_7, output layer is resizebilinear_3.
it should be possible to use to scroll the popup buffer, just as you can when the buffer does not contain signs.
anyone knows how to debug only a small module.
if including tracebacks, please include the full traceback.
maybe i am missing on how to import the savedmodel in python.
because raspberry pi has limited memory.
the code linked at the bottom calculates a matrix exponential (`expm`) using a taylor series phofhyperlink .
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 16.04 - mobile device (e.g.
... try to use oauth2 with internet explorer 11
provide a reproducible test case that is the bare minimum necessary to generate the problem.
gradients can be determined without error.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): centos linux - mobile device (e.g.
i built a neural network with the following code: phofcode i loaded the weights from a .npy file and saved the resulting pb file.
gif phofimage i bisected the issue to phofurl when this commit was added, the preceding tab character was not fully highlighted though, only the second screen cell.
a code below works well on python 3.6 with tf 1.12. on python 3.7 with tf 1.13, however, the code crashes.
the info popup to be coloured the same as the completion candidates popup.
i don't have an android phone, or a wireguard client for that matter, and can't reproduce this.
... steps to reproduce the behavior: 1. use 'content' as stated in phofurl phofcode (complete yaml above) 2. see error when rendering in swagger ui: could not render parameterrow, see the console.
it works fine with tf2.0.0 and python 3.6 or python 3.7.
i would like to have the training metrics logged as well.
the buffer of `./tags` is focused.
multiple calls to `predict` should not degrade in performance over time when passing in a dataset.
- similar behavior happens when using tfds.load with data_dir setted
there is of course an easy workaround to provide own exampl
- have i written custom code (as opposed to using a stock example script provided in tensorflow): na - os platform and distribution (e.g., linux ubuntu 16.04): linux centos 7.5 - mobile device (e.g.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
` with latter, issue is fixed.
`docker run -it --rm tensorflow/tensorflow pip list 2>&1 | grep tensorflow` `docker run -it --rm pip list 2>&1 | grep tensorflow` (the issue persists in docker versions as well)
... api is not publicly available at the moment.
- vim version 8.1.1 - os: windows 10, 1909
if you want xla:cpu, either set that envvar, or use experimental_jit_scope enable xla:cpu.
the `worker_fn` will be used if an "evaluator" task exists in the cluster.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux - tensorflow installed from (source or binary): binary - tensorflow version (use command below): 1.12 python version: 3.5.3 cuda/cudnn version: 9.0 / 7.2.1 gpu model and memory: tesla m60 (also tested on geforce gtx titan black)
tf.keras.rnn should be equivalent to dynamic_rnn
you can get the model from phofurl after untar, you 'll see phofcode then you can review it in phofurl
detailed steps to reproduce the behavior: 1. save the below script as `test_preview_popup.vim` 1. run `tvim -nu none --clean --noplugin 2. run `source %` 3. type `ga<c-x><c-u>` to invoke user defined completion at the point after "invoke" in the right-hand vertical split 4. use `<c-n>` to move to 2nd completion.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: n/a - tensorflow installed from (source or binary): binary tensorflow version (use command below): `tf.version.version = `tf.version.git_version = python version: 3.6.8 bazel version (if compiling from source): n/a gcc/compiler version (if compiling from source): n/a cuda/cudnn version: n/a gpu model and memory: here is the full stacktrace: python traceback (most recent call last): file "<stdin>", line 1, in <module> file line 807, in fit shuffle=shuffle) file line 2417, in _standardize_user_data file line 2646, _set_inputs outputs = self(inputs,
3. lifecycle hooks `afterdestory` executed.
this does not happen when `scrollbar` is disabled.
environment variable: {u 'environment ': u 'cloud ', u 'cluster ': {u 'ps ': u 'chief ': u 'worker ': []}, u 'task ': {u 'index ': 0, u 'type ': u 'ps '}} phofcode error tracing, phofcode
tensorflow documentation for states that: "when eager execution is enabled, this function returns a function which in turn returns the decayed learning rate tensor"
using together with keras to train models as described in phofurl results in gpu out of memory errors appearing after several epochs of training.
everything works fine just i remove the scafflold and the multiple gpu training for estimator is referred this tutorial phofhyperlink .
i am able to ping the server and everything resolves correctly but i am unable to visit a website that is hosted on the server via https.
1. create question type in ui with string field "name" 2. generate questiontype api `strapi generate:api questiontype` or create it in ui with lowercase and then rename it to questiontype (because of the bug 4.)
it is impossible to keep the popup window at the same width when `wrap` and `scrollbar` are enabled, even when `minwidth` and `maxwidth` are set to the same value.
to help us help you, if you 've found a bug please consider the following: * phofurl when you type a value in the input, it should change the "enter your pin".
unfortunately, after one epoch and during the model saving i receive following error: phofcode highlighting error:
phofcode gunzip the following file in that same directory as `input`: input.gz phofhyperlink phofcode then issue the following commands: phofcode at this point you will have to press enter a few times to allow the output to continue, but then look at `/tmp/listener.log`: phofcode notice the third change as part of first callback: this does _not_ have leading `//`.
- tensorflow version (use command below): r1.12.0 python version: python 3.6.4 |anaconda bazel version (if compiling from source): 0.18.1 gcc/compiler version (if compiling from source): 5.4.0 cuda/cudnn version: cuda 9.0, cudnn 7.4.2 gpu model and memory: 3*nvidia 1080ti,
so i'm teaching a course and decide to use `tensorflow.keras` instead of keras.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: null - tensorflow installed from (source or binary): `pip install tensorflow-gpu` tensorflow version (use command below): 1.10.0 python version: 3.6.7 bazel version (if compiling from source): null gcc/compiler version (if compiling from source): null cuda/cudnn version: cudnn 7.4.1 gpu model and memory: nvidia gtx 1070 8gb
the color is not right if wincolor in popup window has `reverse` attribute.
when using the pre-canned `dnnclassifier` together with i noticed that a `tf.graph()` instance is left allocated in memory _after_ an `estimator.train()` call terminates.
- vim version: or later - os: debian sid - terminal: xterm 35
the _conv3d_transpose_ operation should not see such a sudden drop in performance when increasing input size.
t additional grpc error information: received from [64].
), these workers should be always under the same iteration.
i may be misinterpreting what exactly is going wrong, but certainly the behavior shown in the colab below is incorrect.
i see that my `libtensorflow.so` build works fine on arm7 but fails on arm6 with the following error: phofcode
in reality, need to load different into memory.
a list of omni complete suggestions should have appeared.
- os platform and distribution : android, - mobile device : samsung sm-a730f, android 9 , api 28 - tensorflow: 1.13 the model gives error when we run it using tensorflow lite interpreter on android.also it gives similar error on android benchmark tool, when we run it with 'use_nnapi=1' option.
with what i understand about tensorflow 2.0 is that this performance should be the same / similar for this code (see next item for the code).
there are some tests that are known to fail on these builds.
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: - tensorflow installed from (source or binary): tensorflow docker tensorflow version (use command below): python version: 3.5.2(in docker image) bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10.1 gpu model and memory: 4 geforce rtx 2080 you can collect some of this information using our environment capture script phofhyperlink you can also obtain the tensorflow version with python -c "import tensorflow as tf; print(tf.git_version, tf.version)" include any logs or source code that would be helpful to diagnose the problem.
servers should be displayed inline, similar to swagger 2.0 schemes: <img width="1058" alt="image" src=" phofurl
using `tf.function` when enumerating a dataset will cause an infinite loop.
it literally defeats the purpose of it, as i might as well just use `with inside of an `if global_step % n == 0:`
5541 5542 # monkey patch to get rid of an unnecessary conditional since the context is valueerror: must be called at program startup
i would expect no performance penalty or improved performance for upgrading a library.
i ran the same code with keras (not tf.keras) using a tensorflow 1 backend a while ago and it gave correct results.
it works for model built with tf.contrib.slim.
myvar = tf.get_variable('myvar', initializer=np_arr) ragged = with tf.name_scope('gather'): ragged_gather = tf.gather(myvar, ragged) with product ragged_gather, name='prod') with tf.name_scope('output'): output tf.reduce_sum(product
if including tracebacks, please include the full traceback.
trying to write any tensor as histogram summary: > in _buckets(data, bucket_count) 89 if bucket_count is none: 90 bucket_count = default_bucket_count ---> 91 with tf.name_scope('buckets', values=[data, bucket_count]): 92 93 tf.int32) typeerror: __init__() got an unexpected keyword argument 'values'
ctc) on a tpu, tensorflow throws an invalidargumenterror claiming that i didn 't feed a value for a placeholder tensor.
write this in `/tmp/vimrc`: set ls=2 ino <expr> <c-b> func() fu func() redraws return '' endfu then, start vim like this: vim -nu none -s /tmp/vimrc finally, press `c-b` in insert mode: gif phofimage the cursor is drawn somewhere in the middle of the screen where no text exists.
popup window has one line gap when call popup_create() at the first line.
therefore, i load the model with and run `fit` again.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
a mere inspection of the model.cc source code (in directory tensorflow/lite) will show that the function assigns a flatbuffers buffer pointer directly to the tf lite tensor's object data field.
the only modification i brought to this file, besides the new __main__ part at the bottom to run a simple fail case, can be found in the _nas_stem function.
the return value should be
this library needs to run and not choke on its own imports.
maybe convert to float instead of integer if input is a float dtype?
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: na - tensorflow installed from (source or binary): binary tensorflow version (use command below): 1.12.0 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 9/7 gpu model and memory: nvidia gtx1080, 8gb include any logs or source code that would be helpful to diagnose the problem.
add a new content type.
large logs and files should be attached.
448.83mib 406.00mib 385.49mib 7, 7.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
console strapi new my_api custom (manual setting) ?
i expect the model to be able to run and calculate loss on tpu using xla
phofcode 4. redo step 3, but before closing popup window, first make a mouse selection inside popup window.
python version: not using python.
code runs normally when xla is disabled.
no assertion error, or at least a better explanation as to the cause.
it should produce outputs as the masked timestamps should be filled zeros if zero_output_for_mask is true.
generating several models with no input shape specified results in normal memory occupancy: phofcode no_shape phofimage this is the behavior expected when the input shape is specified.
the error does not occur if `batch_size = 1`, or if omitting `sample_weight`.
this is probably because cudnn failed to initialize, try looking to see if a warning log message was printed above.
... steps to reproduce the behavior: 1. building project with ` ng build --prod --aot` 2. deploy project on nginx
iphone 8, pixel 2, samsung galaxy) if the issue happens on mobile device: no - tensorflow installed from (source or binary): source tensorflow version (use command below): 1.12.0 python version: 3.6 bazel version (if compiling from source): gcc/compiler version (if compiling from source): cuda/cudnn version: 10, 7.3 gpu model and memory: 4x 1080ti 11gb
tf.nn.conv2d with data format nhwc should convergence as expected, it should behave the same as data format nchw or naive dilated convolution implementation.
fix is simple, replace the lines: phofurl with: phofcode
the trained model was frozen to a .pb file using freeze_graph.
i wish to convert this model to pb.
new code: python import tensorflow as tf import tensorflow.keras as k import tensorflow.keras.layers as kl import tensorflow.keras.metrics as km from fashion_mnist tfcfg = tf.configproto() = true sess = tf.session(config=tfcfg) (x_train, y_train), (x_test, y_test) = x_train x_train.reshape((-1, 28, 28, 1)) x_test x_test.reshape((-1, 28, 28, 1)) y_train 10) y_test 10) model k.sequential([ kl.conv2d(32, 3, 1, input_shape=[28, 1]), kl.batchnormalization(), kl.leakyrelu(), kl.conv2d(64, 3, 1), kl.batchnormalization(), kl.leakyrelu(), kl.conv2d(128, 3, 1), kl.batchnormalization(), kl.leakyrelu(), kl.leakyrelu(), kl.flatten(), kl.dense(512), kl.batchnormalization(), kl.dense(10) ]) class def __init__(self, name=none, dtype=none,
somewhere between vim 7.4 and vim 8 (or at least the vim 8 that ubuntu ships), the ability to write to files created by nautilus in directories mounted over gnome 's virtual file system (gvfs) was lost.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
first, the heredoc needs to be inside a function.
global_step/sec essentially plateaued around 8 steps/sec within first 3k steps.
detailed steps to reproduce the behavior: 1. run `mkdir /tmp/toto@tata` 2. run `touch 3. run `vim --clean` 4. type `:set 5. vim doesn 't change spellfile option
it seems to me that years of refactoring have disabled fallback mechanism phofhyperlink when both primary and secondary algorithms are default ones.
it is very small so i am able to load it into memory, so i do not need the dataset api.
i run the transfer_learning.ipynb phofhyperlink tutorial on/r2 for tensroflow 2.0 on jupyter notebook in anaconda windows 10 and i get the error on step: phofcode
i understand that floating point approximation might play a role here but should output depend on shape like this?
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): mac - mobile device (e.g.
the following structure is expected.
following from #4876. if a popup is opened to display part of a buffer (for example, displaying quickfix errors in a preview popup as described in the linked issue), the popup cannot be programatically scrolled if there are signs in the buffer.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
visually selecting from the bottom up, while wordwrap is on, and then unselecting still leaves the appearance of selected text if the last displayed line extends beyond the length of window.
phofcode attached file is zipped logs directory to be viewed with tensorboard.
with this simple config it crash: phofcode i test in archlinux, debian buster 10.1 and gvim in win10.
then run `python3 and get an exception.
the mirroredvariable class phofhyperlink is a subclass of the distributeddelegate class phofhyperlink , which if i understand correctly, means that `mirroredvariables` are supposed to act like regular tensors that you can perform ops on.
this function is then submitted to phofcode is used to get the dataset and create iterator from it.
i've created a tensor set to zero, except a single entry at [:,-1,-1,0] that was set to a large number 1e10.
i think `sequencefeatures` `call` should accepts dense `tensor` objects and just don 't try to convert them from sparse to dense.
it seems i should simply be able to quit it upon `cmdwinenter`, but this does not appear to be the case.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
this occured while extending the training epochs by executing `model.fit()` subsequently.
this is a followup to the issue phofhyperlink raised in mit 's tensorflow labs.
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): linux ubuntu 18.04 - mobile device (e.g.
this may have performance implications.
- vim version : - os: centos 7 - terminal: gnome terminal it would be much appreciated if the pop up window can be focusable.
it should remain sessionrunhook 's interface in distribution mode.
t e aborting ringgather e aborting ringreduce ringreduce cancelled: [_derived_]cancelled cancelled: [_derived_]cancelled op_requires failed at collective_ops.cc:125 : op_requires failed at collective_ops.cc:234 : op_requires failed collective_ops.cc:125 : op_requires failed collective_ops.cc:234 : cancelled: [_derived_]cancelled collective_ops.cc:234 traceback (most recent call last): file line 71, in <module> file line 728, in fit file line 789, in fit *args,
in a tiny build, the sound feature is disabled, so i don 't think it 's necessary to link with libcanberra which deals with playing sounds and "sound themes" as far as i know.
however, it my understanding that model data parallel training not fully operational at this time for custom models (non models).
n/a - vim version phofcode - os: ubuntu 19.04 - terminal: xterm(330) n/a
in other words, we were able to embed the training into our tensorflow graph.
i have a dataset in csv `/tmp/foo.csv`: phofcode i can run something like with `header=true` phofcode which gives: phofcode when c should not be in the dataset.
... you can use this docker-compose file to launch the service with the same options i 'm using: phofcode steps to reproduce the behavior: once swagger ui is launched you will see a blank page is loaded.
if you uncomment the `:throw 'something '`, however, there 'll be an `e488: trailing characters: l:var`, even though that line 's execution is skipped (it 'll proceed with `:catch` block).
- have i written custom code (as opposed to using a stock example script provided in tensorflow): yes - os platform and distribution (e.g., linux ubuntu 16.04): macos 10.13.6 - tensorflow installed from (source or binary): from pip install - tensorflow version (use command below): 2.0.0-beta0 python version: oct 20 2018,
maybe it can manage in a particular way recursive blocks like fold.
gpu version is expected to be faster ( or at the least same ) than cpu version.
3.4gb of memory is used and 338k is free.
provide a reproducible test case that is the bare minimum necessary to generate the problem.
if applicable, copy/paste the text or add screenshots to help explain your problem.
